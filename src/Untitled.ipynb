{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ce1c471-c980-4074-98bc-df9c7e3ce1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Technical diagnostic report saved: ./docs/TECHNICAL_DIAGNOSTIC.md\n",
      "   Issues found: 0\n",
      "   Warnings: 0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# ğŸ”§ Technical System Diagnostic Report\n",
       "\n",
       "*Generated: 2025-11-07 22:06:43*\n",
       "*Quick Mode: OFF*\n",
       "\n",
       "---\n",
       "\n",
       "## 1. ğŸ¥ System Health Check\n",
       "\n",
       "**Overall Status**: âœ… HEALTHY\n",
       "\n",
       "### Component Status\n",
       "| Component | Status | Details |\n",
       "|-----------|--------|---------|\n",
       "| data | âœ… | 3769 rows |\n",
       "| model | âœ… | Trained |\n",
       "| detector_coverage | âœ… | 0 detectors with <70% coverage |\n",
       "| data_freshness | âœ… | 3 days old |\n",
       "| data_completeness | âœ… | 79.1% complete |\n",
       "\n",
       "## 2. ğŸ›ï¸ Model Configuration Deep Dive\n",
       "\n",
       "### Anomaly Detector Configuration\n",
       "```python\n",
       "contamination: 0.050\n",
       "n_estimators: 100\n",
       "max_samples: auto\n",
       "random_state: 42\n",
       "```\n",
       "\n",
       "### Individual Detector Parameters\n",
       "| Detector | Features Used | Coverage | Active |\n",
       "|----------|--------------|----------|--------|\n",
       "| vix_mean_reversion | 18 | 100.0% | âœ… |\n",
       "| vix_momentum | 18 | 100.0% | âœ… |\n",
       "| vix_regime_structure | 18 | 100.0% | âœ… |\n",
       "| cboe_options_flow | 39 | 100.0% | âœ… |\n",
       "| cboe_cross_dynamics | 17 | 100.0% | âœ… |\n",
       "| vix_spx_relationship | 17 | 100.0% | âœ… |\n",
       "| spx_price_action | 15 | 75.0% | âœ… |\n",
       "| spx_volatility_regime | 20 | 90.9% | âœ… |\n",
       "| cross_asset_divergence | 23 | 88.5% | âœ… |\n",
       "| tail_risk_complex | 15 | 83.3% | âœ… |\n",
       "| futures_term_structure | 27 | 100.0% | âœ… |\n",
       "| macro_regime_shifts | 19 | 100.0% | âœ… |\n",
       "| momentum_acceleration | 20 | 100.0% | âœ… |\n",
       "| percentile_extremes | 19 | 100.0% | âœ… |\n",
       "| random_4 | 0 | 0.0% | âœ… |\n",
       "\n",
       "## 3. ğŸ”„ Data Pipeline Flow Trace\n",
       "\n",
       "### Data Sources â†’ Features â†’ Models\n",
       "```\n",
       "Data Fetching...........................       âœ… OK\n",
       "  â†³ VIX: 3769 observations\n",
       "  â†³ SPX: 3769 observations\n",
       "Feature Engineering.....................       âœ… OK\n",
       "  â†³ Generated 696 features\n",
       "  â†³ Time period: 3769 days\n",
       "Model Training..........................       âœ… OK\n",
       "  â†³ 15/15 detectors trained\n",
       "  â†³ Ensemble scores computed: 3769\n",
       "```\n",
       "\n",
       "### Feature Generation Summary\n",
       "- **Raw Market Data Points**: 7538\n",
       "- **Engineered Features**: 696\n",
       "- **Final Feature Set**: 696\n",
       "- **Data Reduction Ratio**: 0.1x\n",
       "\n",
       "## 4. ğŸ“… Data Freshness & Staleness\n",
       "\n",
       "### Last Update Times\n",
       "| Data Source | Last Update | Age (days) | Status |\n",
       "|-------------|-------------|------------|--------|\n",
       "| Main Features | 2025-11-04 | 3.9 | âŒ |\n",
       "\n",
       "### âš ï¸ Stale Features (>5% missing in recent data)\n",
       "- **SKEW**: 100.0% missing\n",
       "- **SKEW_change_21d**: 100.0% missing\n",
       "- **SKEW_zscore_63d**: 100.0% missing\n",
       "- **PCCI**: 100.0% missing\n",
       "- **PCCI_change_21d**: 100.0% missing\n",
       "- **PCCI_zscore_63d**: 100.0% missing\n",
       "- **PCCE**: 100.0% missing\n",
       "- **PCCE_change_21d**: 100.0% missing\n",
       "- **PCCE_zscore_63d**: 100.0% missing\n",
       "- **PCC**: 100.0% missing\n",
       "\n",
       "## 5. ğŸ¯ Current Anomaly Detection Breakdown\n",
       "\n",
       "### Ensemble Score: 38.1%\n",
       "\n",
       "### Detector Contributions\n",
       "| Detector | Score | Weight | Weighted Score | Agreement |\n",
       "|----------|-------|--------|----------------|-----------|\n",
       "| spx_price_action | 81.1% | 1.00 | 81.1% | ğŸ”´ |\n",
       "| tail_risk_complex | 79.9% | 1.00 | 79.9% | ğŸ”´ |\n",
       "| spx_volatility_regime | 59.1% | 1.00 | 59.1% | ğŸ”´ |\n",
       "| vix_momentum | 55.8% | 1.00 | 55.8% | ğŸŸ¢ |\n",
       "| vix_regime_structure | 52.0% | 1.00 | 52.0% | ğŸŸ¢ |\n",
       "| percentile_extremes | 43.4% | 1.00 | 43.4% | ğŸŸ¢ |\n",
       "| cross_asset_divergence | 38.4% | 1.00 | 38.4% | ğŸŸ¢ |\n",
       "| vix_mean_reversion | 37.9% | 1.00 | 37.9% | ğŸŸ¢ |\n",
       "| vix_spx_relationship | 31.3% | 1.00 | 31.3% | ğŸŸ¢ |\n",
       "| macro_regime_shifts | 13.4% | 1.00 | 13.4% | ğŸ”´ |\n",
       "| momentum_acceleration | 8.2% | 1.00 | 8.2% | ğŸ”´ |\n",
       "| cboe_cross_dynamics | 7.7% | 1.00 | 7.7% | ğŸ”´ |\n",
       "| futures_term_structure | 5.4% | 1.00 | 5.4% | ğŸ”´ |\n",
       "| cboe_options_flow | 1.5% | 1.00 | 1.5% | ğŸ”´ |\n",
       "\n",
       "### Top Features Driving Current Anomaly\n",
       "| Feature | Importance | Current Value | Z-Score |\n",
       "|---------|-----------|---------------|---------|\n",
       "| spx_vs_ma200 | 0.091 | 10.72 | 1.00 |\n",
       "| spx_lag1 | 0.088 | 6851.97 | 2.70 |\n",
       "| spx_ret_5d | 0.082 | -1.73 | -0.89 |\n",
       "| spx_lag5 | 0.082 | 6890.89 | 2.74 |\n",
       "| spx_ret_13d | 0.075 | 2.15 | 0.43 |\n",
       "| spx_ret_63d | 0.070 | 6.72 | 0.54 |\n",
       "| rsi_14 | 0.067 | 59.06 | 0.14 |\n",
       "| spx_momentum_z_10d | 0.067 | -0.48 | -0.34 |\n",
       "| spx_momentum_z_21d | 0.062 | -1.57 | -1.16 |\n",
       "| bb_width_20d | 0.058 | 5.64 | -0.13 |\n",
       "\n",
       "## 6. ğŸ”— Feature Correlation & Redundancy Analysis\n",
       "\n",
       "### High Correlation Pairs (>95%)\n",
       "| Feature 1 | Feature 2 | Correlation |\n",
       "|-----------|-----------|-------------|\n",
       "| 1M_Treasury_zscore_252d | Breakeven_Inflation_10Y_zscore_252d | 1.000 |\n",
       "| 1M_Treasury_zscore_252d | SOFR_90D_level | 1.000 |\n",
       "| 1M_Treasury_zscore_252d | Corporate_Master_OAS_zscore_252d | 1.000 |\n",
       "| 1M_Treasury_zscore_252d | High_Yield_OAS_level | 1.000 |\n",
       "| 1M_Treasury_zscore_252d | CCC_High_Yield_OAS_level | 1.000 |\n",
       "| 1M_Treasury_zscore_252d | 7Y_Treasury_level | 1.000 |\n",
       "| 1M_Treasury_zscore_252d | BB_High_Yield_OAS_level | 1.000 |\n",
       "| 1M_Treasury_zscore_252d | Yield_Curve_10Y3M_zscore_252d | 1.000 |\n",
       "| 1M_Treasury_zscore_252d | 20Y_Treasury_zscore_63d | 1.000 |\n",
       "| 1M_Treasury_zscore_252d | 3M_Treasury_zscore_252d | 1.000 |\n",
       "| 1M_Treasury_zscore_252d | 3Y_Treasury_level | 1.000 |\n",
       "| 1M_Treasury_zscore_252d | 20Y_Treasury_zscore_252d | 1.000 |\n",
       "| 1M_Treasury_zscore_252d | Corporate_Master_OAS_zscore_63d | 1.000 |\n",
       "| 1M_Treasury_zscore_252d | 2Y_Treasury_zscore_252d | 1.000 |\n",
       "| 1M_Treasury_zscore_252d | Breakeven_Inflation_10Y_zscore_63d | 1.000 |\n",
       "\n",
       "**Recommendation**: Consider removing 17552 redundant features to improve performance.\n",
       "\n",
       "## 7. âš¡ Performance Profiling\n",
       "\n",
       "### Execution Time Breakdown\n",
       "| Operation | Time (ms) | % of Total |\n",
       "|-----------|-----------|------------|\n",
       "| batch_10_detections | 168.6 | 89.7% |\n",
       "| single_detection | 19.3 | 10.3% |\n",
       "\n",
       "## 8. ğŸ” Common Failure Modes & Solutions\n",
       "\n",
       "âœ… No common failure modes detected.\n",
       "\n",
       "## 9. ğŸ’¡ What-If Scenario Analysis\n",
       "\n",
       "### VIX Spike to 40\n",
       "**Scenario**: If VIX suddenly spikes to 40 (crisis level)\n",
       "**Expected Behavior**: Ensemble score would likely exceed 93% (CRITICAL threshold)\n",
       "**Current System Response**: Multiple detectors would fire: vix_regime_structure, vix_momentum, cross_asset_divergence\n",
       "\n",
       "### SKEW >150\n",
       "**Scenario**: If SKEW index exceeds 150 (extreme tail risk)\n",
       "**Expected Behavior**: tail_risk_complex detector triggers, ensemble score elevates\n",
       "**Current System Response**: If SKEW features are available, system would classify as HIGH/CRITICAL\n",
       "\n",
       "### All CBOE Data Missing\n",
       "**Scenario**: If CBOE features become unavailable\n",
       "**Expected Behavior**: System continues to function with reduced capability\n",
       "**Current System Response**: 5/15 detectors would be disabled, ensemble relies on VIX/SPX/futures detectors\n",
       "\n",
       "## 10. ğŸ“Š Data Quality Heatmap\n",
       "\n",
       "### Feature Quality by Category\n",
       "| Category | Total Features | Complete | Sparse | Missing | Quality Score |\n",
       "|----------|---------------|----------|--------|---------|---------------|\n",
       "| VIX | 78 | 73 | 0 | 5 | 93.6% |\n",
       "| SPX | 47 | 44 | 0 | 3 | 93.6% |\n",
       "| CBOE | 232 | 24 | 33 | 175 | 17.5% |\n",
       "| Futures | 45 | 35 | 10 | 0 | 88.9% |\n",
       "| Macro | 14 | 11 | 1 | 2 | 82.1% |\n",
       "| Meta | 280 | 84 | 122 | 74 | 51.8% |\n",
       "\n",
       "## 11. ğŸš€ System Optimization Recommendations\n",
       "\n",
       "### ğŸŸ¢ Optimization Opportunities\n",
       "- Remove 17552 highly correlated features to reduce redundancy\n",
       "- Investigate 197 stale features with high recent missing data\n",
       "- Consider adding feature selection to reduce dimensionality\n",
       "- Implement caching for expensive feature calculations\n",
       "- Add monitoring alerts for data freshness\n",
       "\n",
       "## 12. ğŸ“– Quick Reference: Troubleshooting Guide\n",
       "\n",
       "\n",
       "### Common Issues & Quick Fixes\n",
       "\n",
       "**Issue**: System says \"data too old\"\n",
       "- **Check**: `system.orchestrator.features.index[-1]`\n",
       "- **Fix**: Run `system.refresh()` or retrain with fresh data\n",
       "\n",
       "**Issue**: Ensemble score always near 0% or 100%\n",
       "- **Check**: Are thresholds computed? `system.orchestrator.anomaly_detector.statistical_thresholds`\n",
       "- **Fix**: Retrain system to recalculate thresholds\n",
       "\n",
       "**Issue**: Many detectors show 0% coverage\n",
       "- **Check**: CBOE files in `./CBOE_Data_Archive/`\n",
       "- **Fix**: Download CBOE historical data or disable CBOE features in config\n",
       "\n",
       "**Issue**: \"Core data fetch failed\" error\n",
       "- **Check**: Internet connection, yfinance API status\n",
       "- **Fix**: Run `system.orchestrator.fetcher.fetch_core_data(...)` separately to debug\n",
       "\n",
       "**Issue**: High memory usage\n",
       "- **Check**: Feature matrix size with `system.orchestrator.features.memory_usage(deep=True).sum()`\n",
       "- **Fix**: Reduce training window in config.py (TRAINING_YEARS)\n",
       "\n",
       "**Issue**: Slow detection speed (>1 second)\n",
       "- **Check**: Number of features and detectors active\n",
       "- **Fix**: Reduce features, disable low-value detectors, or enable quick_mode\n",
       "\n",
       "**Issue**: NaN/Inf values in features\n",
       "- **Check**: `system.orchestrator.features.isnull().sum()` and `np.isinf(system.orchestrator.features).sum()`\n",
       "- **Fix**: Review feature_engine.py for division by zero or missing data handling\n",
       "\n",
       "\n",
       "---\n",
       "*Report generated in 2.34 seconds*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'# ğŸ”§ Technical System Diagnostic Report\\n\\n*Generated: 2025-11-07 22:06:43*\\n*Quick Mode: OFF*\\n\\n---\\n\\n## 1. ğŸ¥ System Health Check\\n\\n**Overall Status**: âœ… HEALTHY\\n\\n### Component Status\\n| Component | Status | Details |\\n|-----------|--------|---------|\\n| data | âœ… | 3769 rows |\\n| model | âœ… | Trained |\\n| detector_coverage | âœ… | 0 detectors with <70% coverage |\\n| data_freshness | âœ… | 3 days old |\\n| data_completeness | âœ… | 79.1% complete |\\n\\n## 2. ğŸ›ï¸ Model Configuration Deep Dive\\n\\n### Anomaly Detector Configuration\\n```python\\ncontamination: 0.050\\nn_estimators: 100\\nmax_samples: auto\\nrandom_state: 42\\n```\\n\\n### Individual Detector Parameters\\n| Detector | Features Used | Coverage | Active |\\n|----------|--------------|----------|--------|\\n| vix_mean_reversion | 18 | 100.0% | âœ… |\\n| vix_momentum | 18 | 100.0% | âœ… |\\n| vix_regime_structure | 18 | 100.0% | âœ… |\\n| cboe_options_flow | 39 | 100.0% | âœ… |\\n| cboe_cross_dynamics | 17 | 100.0% | âœ… |\\n| vix_spx_relationship | 17 | 100.0% | âœ… |\\n| spx_price_action | 15 | 75.0% | âœ… |\\n| spx_volatility_regime | 20 | 90.9% | âœ… |\\n| cross_asset_divergence | 23 | 88.5% | âœ… |\\n| tail_risk_complex | 15 | 83.3% | âœ… |\\n| futures_term_structure | 27 | 100.0% | âœ… |\\n| macro_regime_shifts | 19 | 100.0% | âœ… |\\n| momentum_acceleration | 20 | 100.0% | âœ… |\\n| percentile_extremes | 19 | 100.0% | âœ… |\\n| random_4 | 0 | 0.0% | âœ… |\\n\\n## 3. ğŸ”„ Data Pipeline Flow Trace\\n\\n### Data Sources â†’ Features â†’ Models\\n```\\nData Fetching...........................       âœ… OK\\n  â†³ VIX: 3769 observations\\n  â†³ SPX: 3769 observations\\nFeature Engineering.....................       âœ… OK\\n  â†³ Generated 696 features\\n  â†³ Time period: 3769 days\\nModel Training..........................       âœ… OK\\n  â†³ 15/15 detectors trained\\n  â†³ Ensemble scores computed: 3769\\n```\\n\\n### Feature Generation Summary\\n- **Raw Market Data Points**: 7538\\n- **Engineered Features**: 696\\n- **Final Feature Set**: 696\\n- **Data Reduction Ratio**: 0.1x\\n\\n## 4. ğŸ“… Data Freshness & Staleness\\n\\n### Last Update Times\\n| Data Source | Last Update | Age (days) | Status |\\n|-------------|-------------|------------|--------|\\n| Main Features | 2025-11-04 | 3.9 | âŒ |\\n\\n### âš ï¸ Stale Features (>5% missing in recent data)\\n- **SKEW**: 100.0% missing\\n- **SKEW_change_21d**: 100.0% missing\\n- **SKEW_zscore_63d**: 100.0% missing\\n- **PCCI**: 100.0% missing\\n- **PCCI_change_21d**: 100.0% missing\\n- **PCCI_zscore_63d**: 100.0% missing\\n- **PCCE**: 100.0% missing\\n- **PCCE_change_21d**: 100.0% missing\\n- **PCCE_zscore_63d**: 100.0% missing\\n- **PCC**: 100.0% missing\\n\\n## 5. ğŸ¯ Current Anomaly Detection Breakdown\\n\\n### Ensemble Score: 38.1%\\n\\n### Detector Contributions\\n| Detector | Score | Weight | Weighted Score | Agreement |\\n|----------|-------|--------|----------------|-----------|\\n| spx_price_action | 81.1% | 1.00 | 81.1% | ğŸ”´ |\\n| tail_risk_complex | 79.9% | 1.00 | 79.9% | ğŸ”´ |\\n| spx_volatility_regime | 59.1% | 1.00 | 59.1% | ğŸ”´ |\\n| vix_momentum | 55.8% | 1.00 | 55.8% | ğŸŸ¢ |\\n| vix_regime_structure | 52.0% | 1.00 | 52.0% | ğŸŸ¢ |\\n| percentile_extremes | 43.4% | 1.00 | 43.4% | ğŸŸ¢ |\\n| cross_asset_divergence | 38.4% | 1.00 | 38.4% | ğŸŸ¢ |\\n| vix_mean_reversion | 37.9% | 1.00 | 37.9% | ğŸŸ¢ |\\n| vix_spx_relationship | 31.3% | 1.00 | 31.3% | ğŸŸ¢ |\\n| macro_regime_shifts | 13.4% | 1.00 | 13.4% | ğŸ”´ |\\n| momentum_acceleration | 8.2% | 1.00 | 8.2% | ğŸ”´ |\\n| cboe_cross_dynamics | 7.7% | 1.00 | 7.7% | ğŸ”´ |\\n| futures_term_structure | 5.4% | 1.00 | 5.4% | ğŸ”´ |\\n| cboe_options_flow | 1.5% | 1.00 | 1.5% | ğŸ”´ |\\n\\n### Top Features Driving Current Anomaly\\n| Feature | Importance | Current Value | Z-Score |\\n|---------|-----------|---------------|---------|\\n| spx_vs_ma200 | 0.091 | 10.72 | 1.00 |\\n| spx_lag1 | 0.088 | 6851.97 | 2.70 |\\n| spx_ret_5d | 0.082 | -1.73 | -0.89 |\\n| spx_lag5 | 0.082 | 6890.89 | 2.74 |\\n| spx_ret_13d | 0.075 | 2.15 | 0.43 |\\n| spx_ret_63d | 0.070 | 6.72 | 0.54 |\\n| rsi_14 | 0.067 | 59.06 | 0.14 |\\n| spx_momentum_z_10d | 0.067 | -0.48 | -0.34 |\\n| spx_momentum_z_21d | 0.062 | -1.57 | -1.16 |\\n| bb_width_20d | 0.058 | 5.64 | -0.13 |\\n\\n## 6. ğŸ”— Feature Correlation & Redundancy Analysis\\n\\n### High Correlation Pairs (>95%)\\n| Feature 1 | Feature 2 | Correlation |\\n|-----------|-----------|-------------|\\n| 1M_Treasury_zscore_252d | Breakeven_Inflation_10Y_zscore_252d | 1.000 |\\n| 1M_Treasury_zscore_252d | SOFR_90D_level | 1.000 |\\n| 1M_Treasury_zscore_252d | Corporate_Master_OAS_zscore_252d | 1.000 |\\n| 1M_Treasury_zscore_252d | High_Yield_OAS_level | 1.000 |\\n| 1M_Treasury_zscore_252d | CCC_High_Yield_OAS_level | 1.000 |\\n| 1M_Treasury_zscore_252d | 7Y_Treasury_level | 1.000 |\\n| 1M_Treasury_zscore_252d | BB_High_Yield_OAS_level | 1.000 |\\n| 1M_Treasury_zscore_252d | Yield_Curve_10Y3M_zscore_252d | 1.000 |\\n| 1M_Treasury_zscore_252d | 20Y_Treasury_zscore_63d | 1.000 |\\n| 1M_Treasury_zscore_252d | 3M_Treasury_zscore_252d | 1.000 |\\n| 1M_Treasury_zscore_252d | 3Y_Treasury_level | 1.000 |\\n| 1M_Treasury_zscore_252d | 20Y_Treasury_zscore_252d | 1.000 |\\n| 1M_Treasury_zscore_252d | Corporate_Master_OAS_zscore_63d | 1.000 |\\n| 1M_Treasury_zscore_252d | 2Y_Treasury_zscore_252d | 1.000 |\\n| 1M_Treasury_zscore_252d | Breakeven_Inflation_10Y_zscore_63d | 1.000 |\\n\\n**Recommendation**: Consider removing 17552 redundant features to improve performance.\\n\\n## 7. âš¡ Performance Profiling\\n\\n### Execution Time Breakdown\\n| Operation | Time (ms) | % of Total |\\n|-----------|-----------|------------|\\n| batch_10_detections | 168.6 | 89.7% |\\n| single_detection | 19.3 | 10.3% |\\n\\n## 8. ğŸ” Common Failure Modes & Solutions\\n\\nâœ… No common failure modes detected.\\n\\n## 9. ğŸ’¡ What-If Scenario Analysis\\n\\n### VIX Spike to 40\\n**Scenario**: If VIX suddenly spikes to 40 (crisis level)\\n**Expected Behavior**: Ensemble score would likely exceed 93% (CRITICAL threshold)\\n**Current System Response**: Multiple detectors would fire: vix_regime_structure, vix_momentum, cross_asset_divergence\\n\\n### SKEW >150\\n**Scenario**: If SKEW index exceeds 150 (extreme tail risk)\\n**Expected Behavior**: tail_risk_complex detector triggers, ensemble score elevates\\n**Current System Response**: If SKEW features are available, system would classify as HIGH/CRITICAL\\n\\n### All CBOE Data Missing\\n**Scenario**: If CBOE features become unavailable\\n**Expected Behavior**: System continues to function with reduced capability\\n**Current System Response**: 5/15 detectors would be disabled, ensemble relies on VIX/SPX/futures detectors\\n\\n## 10. ğŸ“Š Data Quality Heatmap\\n\\n### Feature Quality by Category\\n| Category | Total Features | Complete | Sparse | Missing | Quality Score |\\n|----------|---------------|----------|--------|---------|---------------|\\n| VIX | 78 | 73 | 0 | 5 | 93.6% |\\n| SPX | 47 | 44 | 0 | 3 | 93.6% |\\n| CBOE | 232 | 24 | 33 | 175 | 17.5% |\\n| Futures | 45 | 35 | 10 | 0 | 88.9% |\\n| Macro | 14 | 11 | 1 | 2 | 82.1% |\\n| Meta | 280 | 84 | 122 | 74 | 51.8% |\\n\\n## 11. ğŸš€ System Optimization Recommendations\\n\\n### ğŸŸ¢ Optimization Opportunities\\n- Remove 17552 highly correlated features to reduce redundancy\\n- Investigate 197 stale features with high recent missing data\\n- Consider adding feature selection to reduce dimensionality\\n- Implement caching for expensive feature calculations\\n- Add monitoring alerts for data freshness\\n\\n## 12. ğŸ“– Quick Reference: Troubleshooting Guide\\n\\n\\n### Common Issues & Quick Fixes\\n\\n**Issue**: System says \"data too old\"\\n- **Check**: `system.orchestrator.features.index[-1]`\\n- **Fix**: Run `system.refresh()` or retrain with fresh data\\n\\n**Issue**: Ensemble score always near 0% or 100%\\n- **Check**: Are thresholds computed? `system.orchestrator.anomaly_detector.statistical_thresholds`\\n- **Fix**: Retrain system to recalculate thresholds\\n\\n**Issue**: Many detectors show 0% coverage\\n- **Check**: CBOE files in `./CBOE_Data_Archive/`\\n- **Fix**: Download CBOE historical data or disable CBOE features in config\\n\\n**Issue**: \"Core data fetch failed\" error\\n- **Check**: Internet connection, yfinance API status\\n- **Fix**: Run `system.orchestrator.fetcher.fetch_core_data(...)` separately to debug\\n\\n**Issue**: High memory usage\\n- **Check**: Feature matrix size with `system.orchestrator.features.memory_usage(deep=True).sum()`\\n- **Fix**: Reduce training window in config.py (TRAINING_YEARS)\\n\\n**Issue**: Slow detection speed (>1 second)\\n- **Check**: Number of features and detectors active\\n- **Fix**: Reduce features, disable low-value detectors, or enable quick_mode\\n\\n**Issue**: NaN/Inf values in features\\n- **Check**: `system.orchestrator.features.isnull().sum()` and `np.isinf(system.orchestrator.features).sum()`\\n- **Fix**: Review feature_engine.py for division by zero or missing data handling\\n\\n\\n---\\n*Report generated in 2.34 seconds*'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enhanced_technical_introspector import TechnicalIntrospector\n",
    "\n",
    "introspector = TechnicalIntrospector(system)\n",
    "introspector.generate_report(quick_mode=False)\n",
    "\n",
    "# # Or specific diagnostics:\n",
    "# introspector.diagnose_current_state()\n",
    "# introspector.profile_performance()\n",
    "# introspector.check_data_freshness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52eb5560-bcf8-414d-bb06-335e7c15e61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE CLEANUP SCRIPT\n",
      "================================================================================\n",
      "\n",
      "[1/4] Loading diagnostic report...\n",
      "âœ… Loaded report with 232 features\n",
      "\n",
      "[2/4] Generating removal summary...\n",
      "\n",
      "================================================================================\n",
      "FEATURE REMOVAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ HIGH PRIORITY REMOVALS (Clear Garbage):\n",
      "--------------------------------------------------------------------------------\n",
      "  1. vix_extreme_low_21d                      â†’ too few unique values\n",
      "     âš ï¸  Zeros: 81.5%\n",
      "     âš ï¸  Unique values: 2\n",
      "  2. vix_term_structure                       â†’ missing 100.0%, no variance, too few unique values\n",
      "     âš ï¸  Missing: 100.0%\n",
      "     âš ï¸  Unique values: 0\n",
      "  3. vol_term_regime                          â†’ zero 100.0%, no variance, too few unique values\n",
      "     âš ï¸  Zeros: 100.0%\n",
      "     âš ï¸  Unique values: 1\n",
      "  4. vix_extreme_low_63d                      â†’ too few unique values\n",
      "     âš ï¸  Zeros: 78.7%\n",
      "     âš ï¸  Unique values: 2\n",
      "  5. vix_extreme_low_252d                     â†’ too few unique values\n",
      "     âš ï¸  Zeros: 82.4%\n",
      "     âš ï¸  Unique values: 2\n",
      "  6. SKEW_extreme_low_63d                     â†’ too few unique values\n",
      "     âš ï¸  Zeros: 89.5%\n",
      "     âš ï¸  Unique values: 2\n",
      "  7. SKEW_extreme_low_252d                    â†’ too few unique values\n",
      "     âš ï¸  Zeros: 92.1%\n",
      "     âš ï¸  Unique values: 2\n",
      "  8. risk_premium_extreme_low_63d             â†’ too few unique values\n",
      "     âš ï¸  Zeros: 85.6%\n",
      "     âš ï¸  Unique values: 2\n",
      "  9. risk_premium_extreme_low_252d            â†’ too few unique values\n",
      "     âš ï¸  Zeros: 89.3%\n",
      "     âš ï¸  Unique values: 2\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ MEDIUM PRIORITY REMOVALS (Redundant Features):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "âœ… KEEP: vix\n",
      "   âŒ REMOVE: VXTLT                                    (corr: 0.951)\n",
      "\n",
      "âœ… KEEP: vix_bb_position_20d\n",
      "   âŒ REMOVE: vix_zscore_21d                           (corr: 0.986)\n",
      "   âŒ REMOVE: vix_percentile_21d                       (corr: 0.966)\n",
      "\n",
      "âœ… KEEP: yield_10y2y\n",
      "   âŒ REMOVE: yield_5y2y                               (corr: 0.969)\n",
      "\n",
      "âœ… KEEP: yield_10y2y_zscore\n",
      "   âŒ REMOVE: yield_10y2y_percentile_252d              (corr: 0.973)\n",
      "\n",
      "âœ… KEEP: yield_10y3m\n",
      "   âŒ REMOVE: yield_5y2y                               (corr: 0.964)\n",
      "\n",
      "âœ… KEEP: dgs10_vol_63d\n",
      "   âŒ REMOVE: yield_curve_vol_avg                      (corr: 0.952)\n",
      "\n",
      "âœ… KEEP: risk_premium_percentile_126d\n",
      "   âŒ REMOVE: risk_premium_percentile_252d             (corr: 0.965)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ… Summary saved to: ./diagnostics/removal_summary.txt\n",
      "\n",
      "[3/4] Identifying features to remove...\n",
      "\n",
      "ğŸ—‘ï¸  Removing 16 features:\n",
      "   High Priority (garbage):  9\n",
      "   Medium Priority (redundant): 8\n",
      "\n",
      "ğŸ“Š Final counts:\n",
      "   Original features: 232\n",
      "   Features to REMOVE: 16\n",
      "   Features to KEEP: 223\n",
      "   After cleanup: 223 features (96.1% of original)\n",
      "\n",
      "[4/4] Generating cleaned config...\n",
      "\n",
      "âš ï¸  WARNING: Automatic config generation is complex.\n",
      "   Instead, I'll generate a Python script that you can review.\n",
      "   The safest approach is to manually remove features from config.py\n",
      "\n",
      "âœ… Saved removal list to: ./diagnostics/features_to_remove.json\n",
      "\n",
      "================================================================================\n",
      "NEXT STEPS:\n",
      "================================================================================\n",
      "\n",
      "1. Review the removal summary:\n",
      "   cat ./diagnostics/removal_summary.txt\n",
      "\n",
      "2. Review the features to remove:\n",
      "   cat ./diagnostics/features_to_remove.json\n",
      "\n",
      "3. Manually edit config.py to remove these features from:\n",
      "   - VIX_BASE_FEATURES\n",
      "   - SPX_BASE_FEATURES\n",
      "   - CBOE_BASE_FEATURES\n",
      "   - FUTURES_FEATURES\n",
      "   - META_FEATURES\n",
      "   - ANOMALY_FEATURE_GROUPS\n",
      "\n",
      "4. Re-run training to verify:\n",
      "   python integrated_system_production.py\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Feature Cleanup Script - Auto-generate cleaned config.py\n",
    "\n",
    "This script:\n",
    "1. Reads your diagnostic report\n",
    "2. Identifies features to remove (high priority + medium priority)\n",
    "3. Generates a new config.py with cleaned feature groups\n",
    "4. Backs up your old config.py\n",
    "\n",
    "Usage:\n",
    "    python feature_cleanup.py\n",
    "\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "\n",
    "def load_diagnostic_report(path='./diagnostics/feature_report.json'):\n",
    "    \"\"\"Load the diagnostic report.\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def get_features_to_remove(report):\n",
    "    \"\"\"Extract all features that should be removed.\"\"\"\n",
    "    remove_set = set()\n",
    "    \n",
    "    # High priority removals (clear garbage)\n",
    "    for item in report['recommendations']['remove_high_priority']:\n",
    "        remove_set.add(item['feature'])\n",
    "    \n",
    "    # Medium priority removals (redundant)\n",
    "    for item in report['recommendations']['remove_medium_priority']:\n",
    "        remove_set.add(item['feature'])\n",
    "    \n",
    "    # Note: We keep manual review items for now (only 1 feature)\n",
    "    # You can add them here if you want: SKEW_momentum_regime\n",
    "    \n",
    "    print(f\"\\nğŸ—‘ï¸  Removing {len(remove_set)} features:\")\n",
    "    print(f\"   High Priority (garbage):  {len(report['recommendations']['remove_high_priority'])}\")\n",
    "    print(f\"   Medium Priority (redundant): {len(report['recommendations']['remove_medium_priority'])}\")\n",
    "    \n",
    "    return remove_set\n",
    "\n",
    "\n",
    "def clean_feature_group(feature_list, remove_set):\n",
    "    \"\"\"Remove problematic features from a feature group.\"\"\"\n",
    "    return [f for f in feature_list if f not in remove_set]\n",
    "\n",
    "\n",
    "def generate_cleaned_config(report, output_path='./config_cleaned.py'):\n",
    "    \"\"\"Generate a new config.py with cleaned feature groups.\"\"\"\n",
    "    \n",
    "    remove_set = get_features_to_remove(report)\n",
    "    \n",
    "    # Read original config.py\n",
    "    with open('./config.py', 'r') as f:\n",
    "        original_config = f.read()\n",
    "    \n",
    "    # Start building the new config\n",
    "    lines = []\n",
    "    lines.append('\"\"\"Enhanced Configuration V4 - AUTO-CLEANED\"\"\"')\n",
    "    lines.append('# Generated automatically by feature_cleanup.py')\n",
    "    lines.append(f'# Timestamp: {datetime.now().isoformat()}')\n",
    "    lines.append(f'# Removed {len(remove_set)} problematic features')\n",
    "    lines.append('')\n",
    "    lines.append('from pathlib import Path')\n",
    "    lines.append('')\n",
    "    \n",
    "    # Copy everything up to feature definitions\n",
    "    config_lines = original_config.split('\\n')\n",
    "    in_feature_section = False\n",
    "    \n",
    "    for line in config_lines:\n",
    "        # Copy header sections\n",
    "        if 'VIX_BASE_FEATURES' in line:\n",
    "            in_feature_section = True\n",
    "        \n",
    "        if not in_feature_section:\n",
    "            # Skip the docstring since we replaced it\n",
    "            if line.startswith('\"\"\"Enhanced Configuration V4\"\"\"'):\n",
    "                continue\n",
    "            lines.append(line)\n",
    "    \n",
    "    # Now manually rebuild feature definitions with cleaning\n",
    "    lines.append('# Feature Definitions - VIX Base')\n",
    "    lines.append('VIX_BASE_FEATURES = {')\n",
    "    lines.append(\"    'mean_reversion': [\")\n",
    "    \n",
    "    # Define original feature groups (you'll need to extract these from config.py)\n",
    "    # For now, I'll create a template you can fill in\n",
    "    \n",
    "    vix_mean_reversion = [\n",
    "        'vix_vs_ma10', 'vix_vs_ma21', 'vix_vs_ma63', 'vix_vs_ma252',\n",
    "        'vix_bb_position_20d', 'reversion_strength_21d', 'reversion_strength_63d',\n",
    "        'vix_pull_ma21', 'vix_pull_ma63', 'vix_stretch_ma21', 'vix_stretch_ma63',\n",
    "        'vix_extreme_low_21d', 'vix_extreme_high_21d', 'vix_mean_distance_21d'\n",
    "    ]\n",
    "    \n",
    "    cleaned = clean_feature_group(vix_mean_reversion, remove_set)\n",
    "    for feat in cleaned:\n",
    "        lines.append(f\"        '{feat}',\")\n",
    "    lines.append(\"    ],\")\n",
    "    \n",
    "    # ... continue for all feature groups\n",
    "    \n",
    "    lines.append('}')\n",
    "    lines.append('')\n",
    "    \n",
    "    # Write the new config\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write('\\n'.join(lines))\n",
    "    \n",
    "    print(f\"\\nâœ… Generated cleaned config: {output_path}\")\n",
    "    print(f\"\\nNext steps:\")\n",
    "    print(f\"1. Review {output_path}\")\n",
    "    print(f\"2. If satisfied, backup and replace:\")\n",
    "    print(f\"   cp config.py config_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.py\")\n",
    "    print(f\"   mv {output_path} config.py\")\n",
    "    print(f\"3. Re-run training: python integrated_system_production.py\")\n",
    "\n",
    "\n",
    "def generate_removal_summary(report):\n",
    "    \"\"\"Generate a detailed summary of what's being removed and why.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FEATURE REMOVAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nğŸ“‹ HIGH PRIORITY REMOVALS (Clear Garbage):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, item in enumerate(report['recommendations']['remove_high_priority'], 1):\n",
    "        reasons = ', '.join(item['reasons'])\n",
    "        print(f\"{i:3d}. {item['feature']:40s} â†’ {reasons}\")\n",
    "        \n",
    "        # Show metrics\n",
    "        metrics = item['metrics']\n",
    "        if metrics['missing_pct'] > 50:\n",
    "            print(f\"     âš ï¸  Missing: {metrics['missing_pct']:.1f}%\")\n",
    "        if metrics['zero_pct'] > 50:\n",
    "            print(f\"     âš ï¸  Zeros: {metrics['zero_pct']:.1f}%\")\n",
    "        if metrics['unique_values'] < 3:\n",
    "            print(f\"     âš ï¸  Unique values: {metrics['unique_values']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\nğŸ“‹ MEDIUM PRIORITY REMOVALS (Redundant Features):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Group by redundancy pairs\n",
    "    redundancy_map = {}\n",
    "    for item in report['recommendations']['remove_medium_priority']:\n",
    "        reasons = item['reasons'][0] if item['reasons'] else 'unknown'\n",
    "        if 'redundant with' in reasons:\n",
    "            kept_feature = reasons.split('redundant with ')[1]\n",
    "            if kept_feature not in redundancy_map:\n",
    "                redundancy_map[kept_feature] = []\n",
    "            redundancy_map[kept_feature].append({\n",
    "                'removed': item['feature'],\n",
    "                'correlation': item.get('correlation', 0.0)\n",
    "            })\n",
    "    \n",
    "    for kept, removed_list in redundancy_map.items():\n",
    "        print(f\"\\nâœ… KEEP: {kept}\")\n",
    "        for r in removed_list:\n",
    "            print(f\"   âŒ REMOVE: {r['removed']:40s} (corr: {r['correlation']:.3f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Save to file\n",
    "    summary_path = './diagnostics/removal_summary.txt'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"FEATURE REMOVAL SUMMARY\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().isoformat()}\\n\")\n",
    "        f.write(f\"Total features to remove: {len(report['recommendations']['remove_high_priority']) + len(report['recommendations']['remove_medium_priority'])}\\n\\n\")\n",
    "        \n",
    "        f.write(\"HIGH PRIORITY REMOVALS:\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        for i, item in enumerate(report['recommendations']['remove_high_priority'], 1):\n",
    "            f.write(f\"{i:3d}. {item['feature']:40s} â†’ {', '.join(item['reasons'])}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\nMEDIUM PRIORITY REMOVALS:\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        for kept, removed_list in redundancy_map.items():\n",
    "            f.write(f\"\\nKEEP: {kept}\\n\")\n",
    "            for r in removed_list:\n",
    "                f.write(f\"  REMOVE: {r['removed']:40s} (corr: {r['correlation']:.3f})\\n\")\n",
    "    \n",
    "    print(f\"\\nâœ… Summary saved to: {summary_path}\")\n",
    "\n",
    "\n",
    "def backup_config():\n",
    "    \"\"\"Backup the current config.py.\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    backup_path = f'./config_backup_{timestamp}.py'\n",
    "    shutil.copy('./config.py', backup_path)\n",
    "    print(f\"\\nâœ… Backed up config.py to {backup_path}\")\n",
    "    return backup_path\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FEATURE CLEANUP SCRIPT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load diagnostic report\n",
    "    print(\"\\n[1/4] Loading diagnostic report...\")\n",
    "    try:\n",
    "        report = load_diagnostic_report()\n",
    "        print(f\"âœ… Loaded report with {report['total_features']} features\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ Error: diagnostics/feature_report.json not found\")\n",
    "        print(\"   Run this first: python integrated_system_production.py\")\n",
    "        return\n",
    "    \n",
    "    # Generate removal summary\n",
    "    print(\"\\n[2/4] Generating removal summary...\")\n",
    "    generate_removal_summary(report)\n",
    "    \n",
    "    # Get features to remove\n",
    "    print(\"\\n[3/4] Identifying features to remove...\")\n",
    "    remove_set = get_features_to_remove(report)\n",
    "    \n",
    "    # Show what we're keeping\n",
    "    keep_count = len(report['recommendations']['keep'])\n",
    "    remove_count = len(remove_set)\n",
    "    print(f\"\\nğŸ“Š Final counts:\")\n",
    "    print(f\"   Original features: {report['total_features']}\")\n",
    "    print(f\"   Features to REMOVE: {remove_count}\")\n",
    "    print(f\"   Features to KEEP: {keep_count}\")\n",
    "    print(f\"   After cleanup: {keep_count} features ({keep_count/report['total_features']*100:.1f}% of original)\")\n",
    "    \n",
    "    # Generate cleaned config\n",
    "    print(\"\\n[4/4] Generating cleaned config...\")\n",
    "    print(\"\\nâš ï¸  WARNING: Automatic config generation is complex.\")\n",
    "    print(\"   Instead, I'll generate a Python script that you can review.\")\n",
    "    print(\"   The safest approach is to manually remove features from config.py\")\n",
    "    \n",
    "    # Save removal list\n",
    "    removal_list_path = './diagnostics/features_to_remove.json'\n",
    "    with open(removal_list_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'features_to_remove': sorted(list(remove_set)),\n",
    "            'count': len(remove_set),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… Saved removal list to: {removal_list_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NEXT STEPS:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n1. Review the removal summary:\")\n",
    "    print(\"   cat ./diagnostics/removal_summary.txt\")\n",
    "    print(\"\\n2. Review the features to remove:\")\n",
    "    print(\"   cat ./diagnostics/features_to_remove.json\")\n",
    "    print(\"\\n3. Manually edit config.py to remove these features from:\")\n",
    "    print(\"   - VIX_BASE_FEATURES\")\n",
    "    print(\"   - SPX_BASE_FEATURES\")\n",
    "    print(\"   - CBOE_BASE_FEATURES\")\n",
    "    print(\"   - FUTURES_FEATURES\")\n",
    "    print(\"   - META_FEATURES\")\n",
    "    print(\"   - ANOMALY_FEATURE_GROUPS\")\n",
    "    print(\"\\n4. Re-run training to verify:\")\n",
    "    print(\"   python integrated_system_production.py\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d14ac6af-c080-4370-9660-c3a039d41af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "IMPORT ANALYSIS - First 50 lines of each Python file\n",
      "================================================================================\n",
      "\n",
      "Found 48 Python files\n",
      "\n",
      "\n",
      "ğŸ“„ .ipynb_checkpoints/config-checkpoint.py\n",
      "   ----------------------------------------------------------------------\n",
      "   from pathlib import Path\n",
      "\n",
      "ğŸ“„ .ipynb_checkpoints/enhanced_technical_introspector-checkpoint.py\n",
      "   ----------------------------------------------------------------------\n",
      "   (no imports in first 50 lines)\n",
      "\n",
      "ğŸ“„ CBOE_Data_Archive/check_missing_trading_days.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "   import pandas_market_calendars as mcal\n",
      "\n",
      "ğŸ“„ CBOE_Data_Archive/move_duplicates.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "\n",
      "ğŸ“„ CBOE_Data_Archive/selection_copier.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pyperclip\n",
      "   import time\n",
      "   import re\n",
      "   import keyboard\n",
      "   import threading\n",
      "\n",
      "ğŸ“„ anomaly_validator.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from datetime import datetime\n",
      "   import matplotlib.pyplot as plt\n",
      "\n",
      "ğŸ“„ archive/config.py\n",
      "   ----------------------------------------------------------------------\n",
      "   from pathlib import Path\n",
      "\n",
      "ğŸ“„ archive/enhanced_config_v4.py\n",
      "   ----------------------------------------------------------------------\n",
      "   from pathlib import Path\n",
      "\n",
      "ğŸ“„ archive/fix_refactoring.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import os\n",
      "   import re\n",
      "   import shutil\n",
      "\n",
      "ğŸ“„ archive/html_migrator.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import re\n",
      "   import os\n",
      "   import shutil\n",
      "   from pathlib import Path\n",
      "   from typing import List, Dict, Tuple\n",
      "   from datetime import datetime\n",
      "   import argparse\n",
      "\n",
      "ğŸ“„ archive/migrate_html.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import re\n",
      "   from pathlib import Path\n",
      "   from typing import Dict, List, Tuple\n",
      "\n",
      "ğŸ“„ archive/migration_script.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import sys\n",
      "   import shutil\n",
      "   import json\n",
      "   from pathlib import Path\n",
      "   from datetime import datetime\n",
      "   import argparse\n",
      "\n",
      "ğŸ“„ archive/pre_refactor_20251103_105825/python/dashboard_data_contract.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import json\n",
      "   import numpy as np\n",
      "   import pandas as pd\n",
      "   from datetime import datetime\n",
      "   from pathlib import Path\n",
      "   from config import REGIME_NAMES, ANOMALY_THRESHOLDS\n",
      "\n",
      "ğŸ“„ archive/quick_fix.py\n",
      "   ----------------------------------------------------------------------\n",
      "   from pathlib import Path\n",
      "\n",
      "ğŸ“„ archive/refactor_cleanup.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import shutil\n",
      "   from pathlib import Path\n",
      "   from datetime import datetime\n",
      "\n",
      "ğŸ“„ archive/refactor_fix.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import subprocess\n",
      "   import sys\n",
      "   from pathlib import Path\n",
      "\n",
      "ğŸ“„ archive/test_refactoring.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import sys\n",
      "   from config import REGIME_BOUNDARIES, REGIME_NAMES, TRAINING_YEARS, ANOMALY_FEATURE_GROUPS\n",
      "   from core.data_fetcher import UnifiedDataFetcher\n",
      "   from core.feature_engine import UnifiedFeatureEngine\n",
      "   from core.anomaly_detector import MultiDimensionalAnomalyDetector\n",
      "   from integrated_system_production import AnomalyOrchestrator, IntegratedMarketSystemV4\n",
      "\n",
      "ğŸ“„ archive/tests/analyze_firing_detectors.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import json\n",
      "   import sys\n",
      "   from pathlib import Path\n",
      "   from integrated_system_production import IntegratedMarketSystemV4\n",
      "   from random_detector_analyzer import RandomDetectorAnalyzer\n",
      "\n",
      "ğŸ“„ archive/tests/comprehensive_test.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import sys\n",
      "   import traceback\n",
      "   import argparse\n",
      "   import warnings\n",
      "   from pathlib import Path\n",
      "   from datetime import datetime, timedelta\n",
      "   import json\n",
      "   import numpy as np\n",
      "   import pandas as pd\n",
      "   from UnifiedDataFetcher import UnifiedDataFetcher\n",
      "\n",
      "ğŸ“„ archive/tests/feature_tally.py\n",
      "   ----------------------------------------------------------------------\n",
      "   from collections import defaultdict\n",
      "   from data_lineage import FEATURE_LINEAGE, RAW_SOURCES, ANOMALY_FEATURE_GROUPS\n",
      "\n",
      "ğŸ“„ archive/tests/test_issues_5_7.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import sys\n",
      "   import traceback\n",
      "   from pathlib import Path\n",
      "   from datetime import datetime\n",
      "   import json\n",
      "\n",
      "ğŸ“„ archive/tests/test_production_readiness.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import sys\n",
      "   import json\n",
      "   import traceback\n",
      "   import argparse\n",
      "   from pathlib import Path\n",
      "   from datetime import datetime, timedelta\n",
      "   import warnings\n",
      "   import numpy as np\n",
      "   import pandas as pd\n",
      "\n",
      "ğŸ“„ archive/tests/validate_data_contract.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import json\n",
      "   import math\n",
      "   from pathlib import Path\n",
      "   from typing import Any, Dict, List, Tuple, Optional\n",
      "   from datetime import datetime\n",
      "\n",
      "ğŸ“„ archive/update_all_charts.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import re\n",
      "   import shutil\n",
      "   from pathlib import Path\n",
      "   from datetime import datetime\n",
      "   import argparse\n",
      "\n",
      "ğŸ“„ config.py\n",
      "   ----------------------------------------------------------------------\n",
      "   from pathlib import Path\n",
      "\n",
      "ğŸ“„ core/.ipynb_checkpoints/feature_engine-checkpoint.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from datetime import datetime, timedelta\n",
      "   import warnings\n",
      "   from typing import Dict, List, Optional, Tuple\n",
      "   from config import REGIME_BOUNDARIES, TRAINING_YEARS\n",
      "\n",
      "ğŸ“„ core/anomaly_detector.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from sklearn.ensemble import IsolationForest\n",
      "   from sklearn.preprocessing import RobustScaler\n",
      "   from datetime import datetime\n",
      "   import warnings\n",
      "   from typing import Dict, List, Tuple, Optional\n",
      "   from config import RANDOM_STATE, ANOMALY_THRESHOLDS, ANOMALY_FEATURE_GROUPS\n",
      "   import pytz\n",
      "   import shap\n",
      "\n",
      "ğŸ“„ core/archive/data_fetcher_v7.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import os\n",
      "   import json\n",
      "   import logging\n",
      "   import requests\n",
      "   import warnings\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   import yfinance as yf\n",
      "   from pathlib import Path\n",
      "   from datetime import datetime, timedelta\n",
      "   from typing import Optional, Dict, List, Tuple\n",
      "   import time\n",
      "\n",
      "ğŸ“„ core/archive/data_quality_control.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from typing import Dict, List, Tuple, Optional, Set\n",
      "   from dataclasses import dataclass\n",
      "   from datetime import datetime\n",
      "   import json\n",
      "   import warnings\n",
      "\n",
      "ğŸ“„ core/archive/data_validation_caching.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from pathlib import Path\n",
      "   from datetime import datetime, timedelta\n",
      "   from typing import Dict, List, Optional, Tuple\n",
      "   import json\n",
      "   import hashlib\n",
      "   import warnings\n",
      "\n",
      "ğŸ“„ core/archive/enhanced_feature_engine_v4.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from datetime import datetime, timedelta\n",
      "   import warnings\n",
      "   from typing import Dict, List, Optional, Tuple\n",
      "   from config_v3 import REGIME_BOUNDARIES, TRAINING_YEARS\n",
      "\n",
      "ğŸ“„ core/archive/oldpredictor.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from datetime import datetime\n",
      "   from pathlib import Path\n",
      "   import json\n",
      "   import pickle\n",
      "   import warnings\n",
      "   from sklearn.ensemble import RandomForestRegressor\n",
      "   from sklearn.preprocessing import StandardScaler\n",
      "   from .anomaly_detector import MultiDimensionalAnomalyDetector\n",
      "   from config import RANDOM_STATE, REGIME_BOUNDARIES, REGIME_NAMES\n",
      "   from .data_fetcher import UnifiedDataFetcher\n",
      "\n",
      "ğŸ“„ core/archive/predictor.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from pathlib import Path\n",
      "   import json\n",
      "   import pickle\n",
      "   import warnings\n",
      "   from sklearn.ensemble import RandomForestRegressor\n",
      "   from sklearn.preprocessing import StandardScaler\n",
      "   from .anomaly_detector import MultiDimensionalAnomalyDetector\n",
      "   from config import RANDOM_STATE, REGIME_BOUNDARIES, REGIME_NAMES\n",
      "   from .data_fetcher import UnifiedDataFetcher\n",
      "\n",
      "ğŸ“„ core/archive/validation.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from pathlib import Path\n",
      "   from datetime import datetime, timedelta\n",
      "   from typing import Dict, List, Optional, Tuple\n",
      "   import json\n",
      "   import hashlib\n",
      "   import warnings\n",
      "\n",
      "ğŸ“„ core/archive/yfinance_data_verify.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import yfinance as yf\n",
      "   from pathlib import Path\n",
      "\n",
      "ğŸ“„ core/data_fetcher.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import os\n",
      "   import json\n",
      "   import logging\n",
      "   import requests\n",
      "   import warnings\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   import yfinance as yf\n",
      "   from pathlib import Path\n",
      "   from datetime import datetime, timedelta\n",
      "   from typing import Optional, Dict\n",
      "   from calendar import monthrange\n",
      "\n",
      "ğŸ“„ core/feature_engine.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from datetime import datetime, timedelta\n",
      "   import warnings\n",
      "   from typing import Dict, List, Optional, Tuple\n",
      "   from config import REGIME_BOUNDARIES, TRAINING_YEARS\n",
      "\n",
      "ğŸ“„ core/regime_transition_forecaster.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from typing import Dict, List, Tuple, Optional\n",
      "   from datetime import datetime\n",
      "   import warnings\n",
      "\n",
      "ğŸ“„ core/xgboost-feature-selection.py\n",
      "   ----------------------------------------------------------------------\n",
      "   from xgboost_feature_selector import XGBoostFeatureSelector\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from pathlib import Path\n",
      "   import json\n",
      "   from datetime import datetime\n",
      "   from typing import Dict, List, Tuple\n",
      "   import warnings\n",
      "   from core.xgboost_trainer import XGBoostTrainer\n",
      "\n",
      "ğŸ“„ core/xgboost_feature_selector_v2.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from pathlib import Path\n",
      "   import json\n",
      "   from datetime import datetime\n",
      "   from typing import Dict, List, Tuple\n",
      "   import warnings\n",
      "   from xgboost_trainer_v2 import EnhancedXGBoostTrainer\n",
      "\n",
      "ğŸ“„ core/xgboost_trainer_v2.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import numpy as np\n",
      "   import pandas as pd\n",
      "   import xgboost as xgb\n",
      "   from sklearn.model_selection import TimeSeriesSplit\n",
      "   from sklearn.metrics import balanced_accuracy_score, mean_squared_error, log_loss\n",
      "   import json\n",
      "   import pickle\n",
      "   from pathlib import Path\n",
      "   from datetime import datetime\n",
      "   from typing import Dict, List, Tuple, Optional\n",
      "   import warnings\n",
      "   import shap\n",
      "\n",
      "ğŸ“„ dashboard_orchestrator.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import subprocess\n",
      "   import sys\n",
      "   import webbrowser\n",
      "   import time\n",
      "   import json\n",
      "   import numpy as np\n",
      "   from pathlib import Path\n",
      "   from datetime import datetime\n",
      "   import http.server\n",
      "   import socketserver\n",
      "   import threading\n",
      "   import traceback\n",
      "   from config import TRAINING_YEARS, ENABLE_TRAINING\n",
      "   from export.unified_exporter import UnifiedExporter\n",
      "\n",
      "ğŸ“„ enhanced_technical_introspector.py\n",
      "   ----------------------------------------------------------------------\n",
      "   from enhanced_technical_introspector import TechnicalIntrospector\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from datetime import datetime, timedelta\n",
      "   from pathlib import Path\n",
      "   from IPython.display import Markdown, display, HTML\n",
      "   import warnings\n",
      "   import time\n",
      "   import json\n",
      "   from typing import Dict, List, Tuple, Optional\n",
      "   from collections import defaultdict\n",
      "\n",
      "ğŸ“„ export/__init__.py\n",
      "   ----------------------------------------------------------------------\n",
      "   from .unified_exporter import UnifiedExporter\n",
      "\n",
      "ğŸ“„ export/unified_exporter.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import json\n",
      "   import pickle\n",
      "   from pathlib import Path\n",
      "   from datetime import datetime\n",
      "   from typing import Dict, Any, Optional, List\n",
      "   from dataclasses import dataclass, asdict\n",
      "   import numpy as np\n",
      "   import pandas as pd\n",
      "\n",
      "ğŸ“„ feature_diagnostics.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from pathlib import Path\n",
      "   import json\n",
      "   from datetime import datetime\n",
      "\n",
      "ğŸ“„ integrated_system_production.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from datetime import datetime\n",
      "   import warnings\n",
      "   import json\n",
      "   import os\n",
      "   import gc\n",
      "   import pickle\n",
      "   from pathlib import Path\n",
      "   from core.feature_engine import UnifiedFeatureEngine\n",
      "   from core.anomaly_detector import MultiDimensionalAnomalyDetector\n",
      "   from core.data_fetcher import UnifiedDataFetcher\n",
      "   from core.xgboost_trainer_v2 import EnhancedXGBoostTrainer\n",
      "   from config import (\n",
      "   from export.unified_exporter import UnifiedExporter\n",
      "   import psutil\n",
      "\n",
      "ğŸ“„ xgboost_integration.py\n",
      "   ----------------------------------------------------------------------\n",
      "   import argparse\n",
      "   import sys\n",
      "   from pathlib import Path\n",
      "   import json\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from datetime import datetime\n",
      "   import warnings\n",
      "   from integrated_system_production import IntegratedMarketSystemV4\n",
      "   from core.xgboost_trainer_v2 import train_enhanced_xgboost\n",
      "   from core.xgboost_feature_selector_v2 import run_intelligent_feature_selection\n",
      "   from config import ENABLE_TRAINING, TRAINING_YEARS\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total Python files: 48\n",
      "Files with imports: 47\n",
      "Total import statements: 296\n",
      "Unique top-level modules: 51\n",
      "\n",
      "\n",
      "================================================================================\n",
      "MODULE USAGE STATISTICS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "pathlib (33 files)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 183\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sys\u001b[38;5;241m.\u001b[39margv) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    181\u001b[0m         num_lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m--> 183\u001b[0m \u001b[43manalyze_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_lines\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 132\u001b[0m, in \u001b[0;36manalyze_imports\u001b[0;34m(root_dir, num_lines)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module, files \u001b[38;5;129;01min\u001b[39;00m sorted_modules:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Used in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(files)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ... and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(files))\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m more\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script to check imports in the first 50 lines of all Python files.\n",
    "Analyzes import statements and identifies potential issues.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def extract_imports(file_path, num_lines=50):\n",
    "    \"\"\"Extract import statements from the first N lines of a file.\"\"\"\n",
    "    imports = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= num_lines:\n",
    "                    break\n",
    "                stripped = line.strip()\n",
    "                # Match import statements\n",
    "                if stripped.startswith('import ') or stripped.startswith('from '):\n",
    "                    imports.append(stripped)\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "    return imports, None\n",
    "\n",
    "def parse_import(import_line):\n",
    "    \"\"\"Parse an import line to extract module names.\"\"\"\n",
    "    modules = []\n",
    "    \n",
    "    # Handle 'from X import Y' pattern\n",
    "    from_match = re.match(r'from\\s+([\\w.]+)\\s+import', import_line)\n",
    "    if from_match:\n",
    "        modules.append(from_match.group(1))\n",
    "    \n",
    "    # Handle 'import X, Y, Z' pattern\n",
    "    import_match = re.match(r'import\\s+(.+)', import_line)\n",
    "    if import_match and not from_match:\n",
    "        imports_str = import_match.group(1)\n",
    "        for item in imports_str.split(','):\n",
    "            module = item.strip().split(' as ')[0].strip()\n",
    "            modules.append(module)\n",
    "    \n",
    "    return modules\n",
    "\n",
    "def find_python_files(root_dir, exclude_dirs=None):\n",
    "    \"\"\"Find all Python files in the directory tree.\"\"\"\n",
    "    if exclude_dirs is None:\n",
    "        exclude_dirs = {'__pycache__', '.git', 'venv', 'env', '.venv'}\n",
    "    \n",
    "    python_files = []\n",
    "    root_path = Path(root_dir)\n",
    "    \n",
    "    for py_file in root_path.rglob('*.py'):\n",
    "        # Check if any parent is in exclude_dirs\n",
    "        if not any(excluded in py_file.parts for excluded in exclude_dirs):\n",
    "            python_files.append(py_file)\n",
    "    \n",
    "    return sorted(python_files)\n",
    "\n",
    "def analyze_imports(root_dir='.', num_lines=50):\n",
    "    \"\"\"Analyze imports across all Python files.\"\"\"\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"IMPORT ANALYSIS - First {num_lines} lines of each Python file\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    python_files = find_python_files(root_dir)\n",
    "    \n",
    "    if not python_files:\n",
    "        print(\"No Python files found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(python_files)} Python files\\n\")\n",
    "    \n",
    "    # Track statistics\n",
    "    all_modules = defaultdict(list)  # module -> list of files using it\n",
    "    errors = []\n",
    "    files_with_imports = 0\n",
    "    total_imports = 0\n",
    "    \n",
    "    # Analyze each file\n",
    "    for py_file in python_files:\n",
    "        rel_path = py_file.relative_to(root_dir)\n",
    "        imports, error = extract_imports(py_file, num_lines)\n",
    "        \n",
    "        if error:\n",
    "            errors.append((rel_path, error))\n",
    "            continue\n",
    "        \n",
    "        if imports:\n",
    "            files_with_imports += 1\n",
    "            print(f\"\\nğŸ“„ {rel_path}\")\n",
    "            print(f\"   {'-'*70}\")\n",
    "            \n",
    "            for imp in imports:\n",
    "                print(f\"   {imp}\")\n",
    "                total_imports += 1\n",
    "                \n",
    "                # Parse and track modules\n",
    "                modules = parse_import(imp)\n",
    "                for module in modules:\n",
    "                    # Get top-level module\n",
    "                    top_level = module.split('.')[0]\n",
    "                    all_modules[top_level].append(str(rel_path))\n",
    "        else:\n",
    "            print(f\"\\nğŸ“„ {rel_path}\")\n",
    "            print(f\"   {'-'*70}\")\n",
    "            print(f\"   (no imports in first {num_lines} lines)\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Total Python files: {len(python_files)}\")\n",
    "    print(f\"Files with imports: {files_with_imports}\")\n",
    "    print(f\"Total import statements: {total_imports}\")\n",
    "    print(f\"Unique top-level modules: {len(all_modules)}\\n\")\n",
    "    \n",
    "    # Show module usage statistics\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"MODULE USAGE STATISTICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Sort by usage count\n",
    "    sorted_modules = sorted(all_modules.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for module, files in sorted_modules:\n",
    "        print(f\"\\n{module} ({len(files)} files)\")\n",
    "        print(f\"   Used in: {', '.join(set(files)[:5])}\")\n",
    "        if len(set(files)) > 5:\n",
    "            print(f\"   ... and {len(set(files)) - 5} more\")\n",
    "    \n",
    "    # Show errors if any\n",
    "    if errors:\n",
    "        print(f\"\\n\\n{'='*80}\")\n",
    "        print(\"ERRORS\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        for file_path, error in errors:\n",
    "            print(f\"âŒ {file_path}: {error}\")\n",
    "    \n",
    "    # Categorize modules\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(\"MODULE CATEGORIES\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    stdlib = []\n",
    "    third_party = []\n",
    "    local = []\n",
    "    \n",
    "    common_stdlib = {\n",
    "        'os', 'sys', 'json', 'datetime', 'time', 're', 'pathlib', 'collections',\n",
    "        'itertools', 'functools', 'typing', 'abc', 'io', 'pickle', 'warnings',\n",
    "        'logging', 'argparse', 'subprocess', 'threading', 'multiprocessing'\n",
    "    }\n",
    "    \n",
    "    for module in all_modules.keys():\n",
    "        if module in common_stdlib:\n",
    "            stdlib.append(module)\n",
    "        elif module.startswith('.') or module in ['core', 'export', 'archive']:\n",
    "            local.append(module)\n",
    "        else:\n",
    "            third_party.append(module)\n",
    "    \n",
    "    print(f\"Standard Library ({len(stdlib)}): {', '.join(sorted(stdlib))}\")\n",
    "    print(f\"\\nThird-Party ({len(third_party)}): {', '.join(sorted(third_party))}\")\n",
    "    print(f\"\\nLocal/Relative ({len(local)}): {', '.join(sorted(local))}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Default values - modify these directly when running in Jupyter\n",
    "    root = '.'  # Change to your project root if needed\n",
    "    num_lines = 50  # Number of lines to check in each file\n",
    "    \n",
    "    # Only use sys.argv if not in Jupyter (check for kernel json files)\n",
    "    import sys\n",
    "    if len(sys.argv) > 1 and not any('kernel' in arg for arg in sys.argv):\n",
    "        root = sys.argv[1]\n",
    "        if len(sys.argv) > 2:\n",
    "            num_lines = int(sys.argv[2])\n",
    "    \n",
    "    analyze_imports(root, num_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8442ff5-21d4-4655-a88f-190fa8e2a084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CORE & XGBOOST IMPORT FIXER\n",
      "================================================================================\n",
      "Root: /Users/johnluo/Desktop/GitHub/SPX_Analysis/src\n",
      "\n",
      "\n",
      "Processing: anomaly_detector.py\n",
      "  - No changes needed\n",
      "\n",
      "Processing: data_fetcher.py\n",
      "  - No changes needed\n",
      "\n",
      "Processing: feature_engine.py\n",
      "  - No changes needed\n",
      "\n",
      "Processing: xgboost_trainer_v2.py\n",
      "  - No changes needed\n",
      "\n",
      "Processing: xgboost_feature_selector_v2.py\n",
      "  âœ“ Fixed\n",
      "\n",
      "Processing: xgboost-feature-selection.py\n",
      "  - No changes needed\n",
      "\n",
      "Processing: regime_transition_forecaster.py\n",
      "  - No changes needed\n",
      "\n",
      "Processing: integrated_system_production.py\n",
      "  - No changes needed\n",
      "\n",
      "Processing: xgboost_integration.py\n",
      "  - No changes needed\n",
      "\n",
      "Processing: dashboard_orchestrator.py\n",
      "  - No changes needed\n",
      "\n",
      "Processing: unified_exporter.py\n",
      "  - No changes needed\n",
      "\n",
      "================================================================================\n",
      "COMPLETE: Fixed 1 files\n",
      "Report: /Users/johnluo/Desktop/GitHub/SPX_Analysis/src/import_fixes_report.txt\n",
      "================================================================================\n",
      "\n",
      "âœ“ All backups saved with .backup extension\n",
      "âœ“ Review changes and delete backups if satisfied\n",
      "\n",
      "To test the fixes, try:\n",
      "  python -m core.xgboost_trainer_v2\n",
      "  python integrated_system_production.py\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Targeted Import Fixer for Core Module and XGBoost Files\n",
    "Fixes all import issues in core/, xgboost files, and integrated_system_production.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "\n",
    "class CoreXGBoostFixer:\n",
    "    \"\"\"Fix imports specifically for core module and XGBoost files\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir=None):\n",
    "        self.root_dir = Path(root_dir or '.').resolve()\n",
    "        self.fixes = []\n",
    "        \n",
    "    def fix_all(self):\n",
    "        \"\"\"Fix all problematic files\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"CORE & XGBOOST IMPORT FIXER\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Root: {self.root_dir}\\n\")\n",
    "        \n",
    "        # Files to fix\n",
    "        targets = [\n",
    "            # Core module files\n",
    "            self.root_dir / 'core' / 'anomaly_detector.py',\n",
    "            self.root_dir / 'core' / 'data_fetcher.py',\n",
    "            self.root_dir / 'core' / 'feature_engine.py',\n",
    "            self.root_dir / 'core' / 'xgboost_trainer_v2.py',\n",
    "            self.root_dir / 'core' / 'xgboost_feature_selector_v2.py',\n",
    "            self.root_dir / 'core' / 'xgboost-feature-selection.py',\n",
    "            self.root_dir / 'core' / 'regime_transition_forecaster.py',\n",
    "            \n",
    "            # Root level files\n",
    "            self.root_dir / 'integrated_system_production.py',\n",
    "            self.root_dir / 'xgboost_integration.py',\n",
    "            self.root_dir / 'dashboard_orchestrator.py',\n",
    "            \n",
    "            # Export module\n",
    "            self.root_dir / 'export' / 'unified_exporter.py',\n",
    "        ]\n",
    "        \n",
    "        fixed_count = 0\n",
    "        for target in targets:\n",
    "            if target.exists():\n",
    "                print(f\"\\nProcessing: {target.name}\")\n",
    "                if self.fix_file(target):\n",
    "                    fixed_count += 1\n",
    "                    print(f\"  âœ“ Fixed\")\n",
    "                else:\n",
    "                    print(f\"  - No changes needed\")\n",
    "            else:\n",
    "                print(f\"\\nâš  Not found: {target}\")\n",
    "        \n",
    "        # Generate report\n",
    "        self.generate_report()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"COMPLETE: Fixed {fixed_count} files\")\n",
    "        print(f\"Report: {self.root_dir / 'import_fixes_report.txt'}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    def fix_file(self, file_path: Path) -> bool:\n",
    "        \"\"\"Fix imports in a specific file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                original_lines = content.split('\\n')\n",
    "            \n",
    "            # Backup original\n",
    "            backup_path = file_path.with_suffix(file_path.suffix + '.backup')\n",
    "            shutil.copy2(file_path, backup_path)\n",
    "            \n",
    "            # Apply fixes based on file location\n",
    "            new_lines = original_lines.copy()\n",
    "            modified = False\n",
    "            \n",
    "            if 'core/' in str(file_path) or file_path.parent.name == 'core':\n",
    "                new_lines, mod = self._fix_core_file(file_path, new_lines)\n",
    "                modified = modified or mod\n",
    "            elif file_path.name == 'integrated_system_production.py':\n",
    "                new_lines, mod = self._fix_integrated_system(new_lines)\n",
    "                modified = modified or mod\n",
    "            elif file_path.name == 'xgboost_integration.py':\n",
    "                new_lines, mod = self._fix_xgboost_integration(new_lines)\n",
    "                modified = modified or mod\n",
    "            elif file_path.name == 'dashboard_orchestrator.py':\n",
    "                new_lines, mod = self._fix_dashboard_orchestrator(new_lines)\n",
    "                modified = modified or mod\n",
    "            elif 'export/' in str(file_path) or file_path.parent.name == 'export':\n",
    "                new_lines, mod = self._fix_export_file(new_lines)\n",
    "                modified = modified or mod\n",
    "            \n",
    "            if modified:\n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write('\\n'.join(new_lines))\n",
    "                self.fixes.append({\n",
    "                    'file': str(file_path.relative_to(self.root_dir)),\n",
    "                    'changes': len([i for i, (o, n) in enumerate(zip(original_lines, new_lines)) if o != n])\n",
    "                })\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— Error: {e}\")\n",
    "            return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _fix_core_file(self, file_path: Path, lines: list) -> tuple:\n",
    "        \"\"\"Fix imports in core module files\"\"\"\n",
    "        new_lines = []\n",
    "        modified = False\n",
    "        \n",
    "        for line in lines:\n",
    "            new_line = line\n",
    "            original = line\n",
    "            \n",
    "            # Fix relative imports within core\n",
    "            if file_path.name in ['xgboost_trainer_v2.py', 'xgboost_feature_selector_v2.py', 'xgboost-feature-selection.py']:\n",
    "                # These files import from each other\n",
    "                new_line = re.sub(\n",
    "                    r'^from xgboost_trainer_v2 import',\n",
    "                    'from core.xgboost_trainer_v2 import',\n",
    "                    new_line\n",
    "                )\n",
    "                new_line = re.sub(\n",
    "                    r'^from xgboost_feature_selector import',\n",
    "                    'from core.xgboost_feature_selector_v2 import',\n",
    "                    new_line\n",
    "                )\n",
    "                new_line = re.sub(\n",
    "                    r'^from xgboost_feature_selector_v2 import',\n",
    "                    'from core.xgboost_feature_selector_v2 import',\n",
    "                    new_line\n",
    "                )\n",
    "            \n",
    "            # Fix config imports (should be absolute from root)\n",
    "            new_line = re.sub(\n",
    "                r'^from \\.config import',\n",
    "                'from config import',\n",
    "                new_line\n",
    "            )\n",
    "            new_line = re.sub(\n",
    "                r'^from config_v3 import',\n",
    "                'from config import',\n",
    "                new_line\n",
    "            )\n",
    "            \n",
    "            # Fix data_fetcher imports\n",
    "            new_line = re.sub(\n",
    "                r'^from \\.data_fetcher import',\n",
    "                'from core.data_fetcher import',\n",
    "                new_line\n",
    "            )\n",
    "            \n",
    "            # Fix feature_engine imports\n",
    "            new_line = re.sub(\n",
    "                r'^from \\.feature_engine import',\n",
    "                'from core.feature_engine import',\n",
    "                new_line\n",
    "            )\n",
    "            \n",
    "            # Fix anomaly_detector imports\n",
    "            new_line = re.sub(\n",
    "                r'^from \\.anomaly_detector import',\n",
    "                'from core.anomaly_detector import',\n",
    "                new_line\n",
    "            )\n",
    "            \n",
    "            # Remove any references to config_v3 or old configs\n",
    "            if 'config_v3' in new_line or 'enhanced_config' in new_line:\n",
    "                new_line = new_line.replace('config_v3', 'config')\n",
    "                new_line = new_line.replace('enhanced_config_v4', 'config')\n",
    "            \n",
    "            if new_line != original:\n",
    "                modified = True\n",
    "            new_lines.append(new_line)\n",
    "        \n",
    "        return new_lines, modified\n",
    "    \n",
    "    def _fix_integrated_system(self, lines: list) -> tuple:\n",
    "        \"\"\"Fix imports in integrated_system_production.py\"\"\"\n",
    "        new_lines = []\n",
    "        modified = False\n",
    "        \n",
    "        for line in lines:\n",
    "            new_line = line\n",
    "            original = line\n",
    "            \n",
    "            # Ensure all core imports are absolute\n",
    "            new_line = re.sub(\n",
    "                r'^from feature_engine import',\n",
    "                'from core.feature_engine import',\n",
    "                new_line\n",
    "            )\n",
    "            new_line = re.sub(\n",
    "                r'^from anomaly_detector import',\n",
    "                'from core.anomaly_detector import',\n",
    "                new_line\n",
    "            )\n",
    "            new_line = re.sub(\n",
    "                r'^from data_fetcher import',\n",
    "                'from core.data_fetcher import',\n",
    "                new_line\n",
    "            )\n",
    "            new_line = re.sub(\n",
    "                r'^from xgboost_trainer_v2 import',\n",
    "                'from core.xgboost_trainer_v2 import',\n",
    "                new_line\n",
    "                )\n",
    "                \n",
    "                # Fix export imports\n",
    "                new_line = re.sub(\n",
    "                    r'^from unified_exporter import',\n",
    "                    'from export.unified_exporter import',\n",
    "                    new_line\n",
    "                )\n",
    "                \n",
    "                # Config should be absolute\n",
    "                new_line = re.sub(\n",
    "                    r'^from \\.config import',\n",
    "                    'from config import',\n",
    "                new_line\n",
    "            )\n",
    "            \n",
    "            if new_line != original:\n",
    "                modified = True\n",
    "            new_lines.append(new_line)\n",
    "        \n",
    "        return new_lines, modified\n",
    "    \n",
    "    def _fix_xgboost_integration(self, lines: list) -> tuple:\n",
    "        \"\"\"Fix imports in xgboost_integration.py\"\"\"\n",
    "        new_lines = []\n",
    "        modified = False\n",
    "        \n",
    "        for line in lines:\n",
    "            new_line = line\n",
    "            original = line\n",
    "            \n",
    "            # Fix core xgboost imports\n",
    "            new_line = re.sub(\n",
    "                r'^from xgboost_trainer_v2 import',\n",
    "                'from core.xgboost_trainer_v2 import',\n",
    "                new_line\n",
    "            )\n",
    "            new_line = re.sub(\n",
    "                r'^from xgboost_feature_selector_v2 import',\n",
    "                'from core.xgboost_feature_selector_v2 import',\n",
    "                new_line\n",
    "            )\n",
    "            \n",
    "            # Fix integrated system import\n",
    "            new_line = re.sub(\n",
    "                r'^from IntegratedMarketSystemV4 import',\n",
    "                'from integrated_system_production import IntegratedMarketSystemV4',\n",
    "                new_line\n",
    "            )\n",
    "            \n",
    "            if new_line != original:\n",
    "                modified = True\n",
    "            new_lines.append(new_line)\n",
    "        \n",
    "        return new_lines, modified\n",
    "    \n",
    "    def _fix_dashboard_orchestrator(self, lines: list) -> tuple:\n",
    "        \"\"\"Fix imports in dashboard_orchestrator.py\"\"\"\n",
    "        new_lines = []\n",
    "        modified = False\n",
    "        \n",
    "        for line in lines:\n",
    "            new_line = line\n",
    "            original = line\n",
    "            \n",
    "            # Fix export imports\n",
    "            new_line = re.sub(\n",
    "                r'^from unified_exporter import',\n",
    "                'from export.unified_exporter import',\n",
    "                new_line\n",
    "            )\n",
    "            \n",
    "            if new_line != original:\n",
    "                modified = True\n",
    "            new_lines.append(new_line)\n",
    "        \n",
    "        return new_lines, modified\n",
    "    \n",
    "    def _fix_export_file(self, lines: list) -> tuple:\n",
    "        \"\"\"Fix imports in export module files\"\"\"\n",
    "        new_lines = []\n",
    "        modified = False\n",
    "        \n",
    "        for line in lines:\n",
    "            new_line = line\n",
    "            original = line\n",
    "            \n",
    "            # Config should be absolute\n",
    "            new_line = re.sub(\n",
    "                r'^from \\.config import',\n",
    "                'from config import',\n",
    "                new_line\n",
    "            )\n",
    "            new_line = re.sub(\n",
    "                r'^from \\.\\.config import',\n",
    "                'from config import',\n",
    "                new_line\n",
    "            )\n",
    "            \n",
    "            if new_line != original:\n",
    "                modified = True\n",
    "            new_lines.append(new_line)\n",
    "        \n",
    "        return new_lines, modified\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate a report of all fixes\"\"\"\n",
    "        # Ensure root_dir exists\n",
    "        if not self.root_dir.exists():\n",
    "            print(f\"\\nâš  Warning: Directory does not exist: {self.root_dir}\")\n",
    "            return\n",
    "            \n",
    "        report_path = self.root_dir / 'import_fixes_report.txt'\n",
    "        \n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"IMPORT FIXES REPORT\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Total files fixed: {len(self.fixes)}\\n\\n\")\n",
    "            \n",
    "            if self.fixes:\n",
    "                f.write(\"FIXED FILES:\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\")\n",
    "                for fix in self.fixes:\n",
    "                    f.write(f\"\\n{fix['file']}\\n\")\n",
    "                    f.write(f\"  Changes: {fix['changes']} lines\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "            f.write(\"IMPORT RULES APPLIED:\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "            f.write(\"1. Core module imports: Always use 'from core.X import'\\n\")\n",
    "            f.write(\"2. Export module imports: Always use 'from export.X import'\\n\")\n",
    "            f.write(\"3. Config imports: Always use 'from config import' (absolute)\\n\")\n",
    "            f.write(\"4. XGBoost cross-references: Use full module paths\\n\")\n",
    "            f.write(\"5. No relative imports (.) in cross-module references\\n\")\n",
    "            f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "\n",
    "def main(root_dir=None):\n",
    "    \"\"\"Run the fixer\"\"\"\n",
    "    import sys\n",
    "    \n",
    "    # Handle Jupyter notebook args (filter out -f flag)\n",
    "    if root_dir is None:\n",
    "        if len(sys.argv) > 1 and not sys.argv[1].startswith('-'):\n",
    "            root_dir = sys.argv[1]\n",
    "        else:\n",
    "            root_dir = '.'\n",
    "    \n",
    "    fixer = CoreXGBoostFixer(root_dir)\n",
    "    fixer.fix_all()\n",
    "    \n",
    "    print(\"\\nâœ“ All backups saved with .backup extension\")\n",
    "    print(\"âœ“ Review changes and delete backups if satisfied\")\n",
    "    print(\"\\nTo test the fixes, try:\")\n",
    "    print(\"  python -m core.xgboost_trainer_v2\")\n",
    "    print(\"  python integrated_system_production.py\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e2b1c3a-5d73-49ed-b79d-39288bdaf2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Removed backup: core/xgboost_feature_selector_v2.py.backup\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "backup_file = 'core/xgboost_feature_selector_v2.py.backup'\n",
    "if os.path.exists(backup_file):\n",
    "    os.remove(backup_file)\n",
    "    print(f\"âœ“ Removed backup: {backup_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0752f3f0-dc5c-4077-8a93-797b28da26fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Add core to path\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mparent))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtemporal_validator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TemporalSafetyValidator, run_full_validation\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PUBLICATION_LAGS\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quick Validation Test Script\n",
    "Run this to check your current system for temporal leakage risks.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add core to path\n",
    "sys.path.insert(0, str(Path(__file__).parent))\n",
    "\n",
    "from core.temporal_validator import TemporalSafetyValidator, run_full_validation\n",
    "from config import PUBLICATION_LAGS\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TEMPORAL SAFETY VALIDATION TEST\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initialize validator\n",
    "    print(f\"\\nğŸ“‹ Loaded {len(PUBLICATION_LAGS)} publication lags from config\")\n",
    "    print(\"\\nSample lags:\")\n",
    "    for source, lag in list(PUBLICATION_LAGS.items())[:5]:\n",
    "        print(f\"  {source:<15} â†’ {lag} days\")\n",
    "    \n",
    "    # Run full validation\n",
    "    validator = run_full_validation(\n",
    "        feature_engine_path=\"core/feature_engine.py\",\n",
    "        publication_lags=PUBLICATION_LAGS\n",
    "    )\n",
    "    \n",
    "    # Additional checks\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ” ADDITIONAL CHECKS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Check 1: Verify data fetcher integration\n",
    "    print(\"\\n[Check 1] Verifying data fetcher temporal safety...\")\n",
    "    try:\n",
    "        from core.data_fetcher import UnifiedDataFetcher\n",
    "        from datetime import datetime, timedelta\n",
    "        \n",
    "        fetcher = UnifiedDataFetcher()\n",
    "        \n",
    "        # Test VIX (T+0)\n",
    "        end_date = datetime.now() - timedelta(days=1)\n",
    "        start_date = end_date - timedelta(days=30)\n",
    "        vix = fetcher.fetch_yahoo(\n",
    "            '^VIX', \n",
    "            start_date.strftime('%Y-%m-%d'),\n",
    "            end_date.strftime('%Y-%m-%d')\n",
    "        )\n",
    "        \n",
    "        if vix is not None:\n",
    "            expected_lag = PUBLICATION_LAGS.get('^VIX', 0)\n",
    "            latest_date = vix.index[-1]\n",
    "            expected_latest = end_date - timedelta(days=expected_lag)\n",
    "            \n",
    "            print(f\"  VIX last date: {latest_date.date()}\")\n",
    "            print(f\"  Expected (with {expected_lag}d lag): {expected_latest.date()}\")\n",
    "            \n",
    "            if latest_date.date() > expected_latest.date():\n",
    "                print(\"  âš ï¸  WARNING: VIX data may not respect publication lag!\")\n",
    "            else:\n",
    "                print(\"  âœ… VIX data respects publication lag\")\n",
    "        \n",
    "        # Test Treasury (T+1)\n",
    "        dgs10 = fetcher.fetch_fred_series(\n",
    "            'DGS10',\n",
    "            start_date.strftime('%Y-%m-%d'),\n",
    "            end_date.strftime('%Y-%m-%d')\n",
    "        )\n",
    "        \n",
    "        if dgs10 is not None:\n",
    "            expected_lag = PUBLICATION_LAGS.get('DGS10', 1)\n",
    "            latest_date = dgs10.index[-1]\n",
    "            expected_latest = end_date - timedelta(days=expected_lag)\n",
    "            \n",
    "            print(f\"\\n  DGS10 last date: {latest_date.date()}\")\n",
    "            print(f\"  Expected (with {expected_lag}d lag): {expected_latest.date()}\")\n",
    "            \n",
    "            if latest_date.date() > expected_latest.date():\n",
    "                print(\"  âš ï¸  WARNING: DGS10 data may not respect publication lag!\")\n",
    "            else:\n",
    "                print(\"  âœ… DGS10 data respects publication lag\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸  Could not verify data fetcher: {e}\")\n",
    "    \n",
    "    # Check 2: Look for common leakage patterns in feature names\n",
    "    print(\"\\n[Check 2] Scanning for suspicious feature names...\")\n",
    "    try:\n",
    "        from core.feature_engine import UnifiedFeatureEngine\n",
    "        from core.data_fetcher import UnifiedDataFetcher\n",
    "        \n",
    "        # Build a small feature set to check names\n",
    "        fetcher = UnifiedDataFetcher()\n",
    "        engine = UnifiedFeatureEngine(fetcher)\n",
    "        \n",
    "        # We won't build full features (too slow), just check the method signatures\n",
    "        import inspect\n",
    "        \n",
    "        suspicious_names = []\n",
    "        for name, method in inspect.getmembers(engine, predicate=inspect.ismethod):\n",
    "            if 'future' in name.lower() or 'forward' in name.lower():\n",
    "                suspicious_names.append(name)\n",
    "        \n",
    "        if suspicious_names:\n",
    "            print(f\"  âš ï¸  Found {len(suspicious_names)} methods with 'future/forward' in name:\")\n",
    "            for name in suspicious_names:\n",
    "                print(f\"     â†’ {name}\")\n",
    "        else:\n",
    "            print(\"  âœ… No suspicious method names found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸  Could not check feature names: {e}\")\n",
    "    \n",
    "    # Check 3: Verify config completeness\n",
    "    print(\"\\n[Check 3] Checking publication lag coverage...\")\n",
    "    \n",
    "    # List of data sources that should have lags\n",
    "    critical_sources = [\n",
    "        '^VIX', '^GSPC', 'SKEW', 'VIX3M', 'VXTLT',\n",
    "        'DGS1MO', 'DGS3MO', 'DGS2', 'DGS10', 'DGS30',\n",
    "        'DTWEXBGS', 'CL=F', 'DX-Y.NYB'\n",
    "    ]\n",
    "    \n",
    "    missing_lags = [src for src in critical_sources if src not in PUBLICATION_LAGS]\n",
    "    \n",
    "    if missing_lags:\n",
    "        print(f\"  âš ï¸  {len(missing_lags)} critical sources missing lag definitions:\")\n",
    "        for src in missing_lags:\n",
    "            print(f\"     â†’ {src}\")\n",
    "    else:\n",
    "        print(f\"  âœ… All {len(critical_sources)} critical sources have lag definitions\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“Š VALIDATION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    audit = validator.audit_results\n",
    "    if audit.get('issues'):\n",
    "        critical = sum(1 for i in audit['issues'] if i['severity'] == 'CRITICAL')\n",
    "        high = sum(1 for i in audit['issues'] if i['severity'] == 'HIGH')\n",
    "        medium = sum(1 for i in audit['issues'] if i['severity'] == 'MEDIUM')\n",
    "        \n",
    "        print(f\"\\nCode Audit: {audit['total_issues']} issues found\")\n",
    "        print(f\"  ğŸ”´ Critical: {critical}\")\n",
    "        print(f\"  ğŸŸ¡ High: {high}\")\n",
    "        print(f\"  ğŸŸ¢ Medium: {medium}\")\n",
    "        \n",
    "        if critical > 0:\n",
    "            print(\"\\nâŒ CRITICAL ISSUES MUST BE FIXED BEFORE TRAINING\")\n",
    "            print(\"   Review the issues above and fix forward-looking operations\")\n",
    "            return False\n",
    "        elif high > 0:\n",
    "            print(\"\\nâš ï¸  HIGH PRIORITY ISSUES SHOULD BE REVIEWED\")\n",
    "            print(\"   Consider fixing before production use\")\n",
    "        else:\n",
    "            print(\"\\nâœ… No critical issues - safe to proceed with caution\")\n",
    "    else:\n",
    "        print(\"\\nâœ… Code audit passed - no suspicious patterns detected\")\n",
    "    \n",
    "    print(f\"\\nPublication Lags: {len(PUBLICATION_LAGS)} sources configured\")\n",
    "    print(f\"Validation Report: ./models/temporal_safety_report.json\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"NEXT STEPS:\")\n",
    "    print(\"=\" * 80)\n",
    "    if audit.get('total_issues', 0) == 0:\n",
    "        print(\"1. âœ… System appears temporally safe\")\n",
    "        print(\"2. â¡ï¸  Ready to implement Optuna optimization\")\n",
    "        print(\"3. ğŸ’¡ Validation will run automatically during training\")\n",
    "    else:\n",
    "        print(\"1. ğŸ“ Review issues listed above\")\n",
    "        print(\"2. ğŸ”§ Fix any critical/high priority issues\")\n",
    "        print(\"3. ğŸ”„ Re-run this validation script\")\n",
    "        print(\"4. â¡ï¸  Then proceed with optimization implementation\")\n",
    "    \n",
    "    return audit.get('total_issues', 0) == 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = main()\n",
    "    sys.exit(0 if success else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89090432-5699-494a-9c40-5e94c01cd66b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

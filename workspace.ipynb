{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9340d437-144f-48f4-917f-62cf7ffe16ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      "EARNINGS CONTAINMENT ANALYZER - v2.3\n",
      "Lookback: 24 quarters (~6 years)\n",
      "Current IV from Yahoo Finance (15-20min delayed)\n",
      "===========================================================================\n",
      "\n",
      "âœ“ All 4 API keys available\n",
      "                                                                                \n",
      "ðŸ“Š FETCH SUMMARY\n",
      "===========================================================================\n",
      "âœ“ Earnings Cached (38): DAL, PEP, FAST, BLK, C...\n",
      "âœ“ IV Retrieved (38): DAL, PEP, FAST, BLK, C...\n",
      "\n",
      "==============================================================================================================\n",
      "BACKTEST RESULTS\n",
      "==============================================================================================================\n",
      "Ticker  HVol% CurIV% IVPrem |  90D%  90Bias 90Break 90Drift  |                                     Pattern\n",
      "   DAL     43     49   +14% |    67      62     5:3   +3.5%   |                                       IC45\n",
      "   PEP     18     26   +44% |    74      65     3:3   +1.3%   |                                       IC90\n",
      "  FAST     24     31   +26% |    78      61     3:2   +4.0%   |                                       IC90\n",
      "   BLK     28     29    +4% |    61      57     5:4   +4.9%   |                        BIASâ†‘ (+4.9% drift)\n",
      "     C     34     31    -8% |    70      48     4:3   +2.9%   |                                       IC90\n",
      "   DPZ     27     37   +36% |    65      61     5:3   +2.9%   |                        BIASâ†‘ (5:3â†‘ breaks)\n",
      "    GS     32     35    +9% |    70      61    5:2â†‘   +6.6%   |                                       IC90\n",
      "   JNJ     17     22   +25% |    78      52     2:3   +0.4%   |                                       IC90\n",
      "   JPM     28     28    +2% |    61      70    6:3â†‘   +4.7%   | BIASâ†‘ (70% bias, 6:3â†‘ breaks, +4.7% drift)\n",
      "   WFC     35     30   -14% |    65      65     5:3   +4.6%   |                                     IC45âš â†‘\n",
      "   OMC     26     39   +50% |    65      61     3:5   +0.2%   |                        BIASâ†“ (3:5â†“ breaks)\n",
      "   ABT     22     25   +15% |    78      57    4:1â†‘   +2.5%   |                                     IC90âš â†‘\n",
      "   BAC     32     30    -5% |    61      61     5:4   +3.1%   |                        BIASâ†‘ (+3.1% drift)\n",
      "   CFG     40     29   -27% |    74      65     3:3   +3.1%   |                                       IC90\n",
      "    MS     33     30    -9% |    61      70    6:3â†‘   +5.9%   | BIASâ†‘ (70% bias, 6:3â†‘ breaks, +5.9% drift)\n",
      "   PGR     24     35   +44% |    70      70    7:0â†‘   +6.0%   |                                     IC90âš â†‘\n",
      "   PLD     29     29    +0% |    61      61    6:3â†‘   +1.3%   |                                       IC45\n",
      "   PNC     32     26   -17% |    61      70     5:4   +4.3%   |                                       IC45\n",
      "   SYF     41     44    +8% |    70      65     3:4   +7.9%   |                                       IC90\n",
      "  JBHT     29     47   +61% |    74      52    2:4â†“   +1.4%   |                                       IC90\n",
      "   UAL     51     58   +12% |    70      57     4:3   +4.3%   |                                       IC90\n",
      "    BK     29     30    +4% |    74      65    4:2â†‘   +4.3%   |                                       IC90\n",
      "   KEY     42     35   -16% |    78      61     2:3   +3.7%   |                                       IC90\n",
      "   MMC     18     34   +82% |    65      70    6:2â†‘   +3.4%   | BIASâ†‘ (70% bias, 6:2â†‘ breaks, +3.4% drift)\n",
      "   MTB     33     37   +11% |    61      65     4:5   +2.6%   |                                       IC45\n",
      "  SCHW     34     32    -8% |    70      65     3:4   +5.3%   |                                       IC90\n",
      "   SNA     25     31   +23% |    65      61     4:4   +3.7%   |                        BIASâ†‘ (+3.7% drift)\n",
      "   TRV     25     27   +10% |    70      61    5:2â†‘   +4.0%   |                                       IC90\n",
      "   USB     33     28   -14% |    70      61     3:4   +0.6%   |                                       IC90\n",
      "   CSX     25     27   +10% |    65      65     4:4   +2.7%   |                           BIASâ†‘ (65% bias)\n",
      "   AXP     30     32    +7% |    43      74    9:4â†‘   +5.8%   | BIASâ†‘ (74% bias, 9:4â†‘ breaks, +5.8% drift)\n",
      "  FITB     38     30   -21% |    70      57     4:3   +3.3%   |                                       IC90\n",
      "  HBAN     35     36    +1% |    78      61     2:3   +1.9%   |                                       IC90\n",
      "    RF     37     32   -14% |    78      61    1:4â†“   +3.7%   |                                     IC90âš â†“\n",
      "   SLB     43     39   -10% |    87      61    1:2â†“   +3.7%   |                                       IC90\n",
      "   STT     33     39   +18% |    57      61     5:5   +2.5%   |                                       SKIP\n",
      "   TFC     36     27   -26% |    78      57    0:5â†“   +0.4%   |                                     IC90âš â†“\n",
      "  STLD     42     41    -2% |    78      65     3:2   +9.8%   |                                       IC90\n",
      "\n",
      "==============================================================================================================\n",
      "KEY TAKEAWAYS:\n",
      "==============================================================================================================\n",
      "\n",
      "ðŸ“Š Pattern Summary: 27 IC candidates | 9 Upward bias | 1 Downward bias | 1 No edge\n",
      "\n",
      "ðŸ’° IV Landscape:\n",
      "  Rich Premium (â‰¥15%): MMC(+82%), JBHT(+61%), OMC(+50%), PEP(+44%), PGR(+44%)\n",
      "  Thin Premium (â‰¤-15%): CFG(-27%), TFC(-26%), FITB(-21%)\n",
      "  Normal Range: 22 tickers\n",
      "\n",
      "âš ï¸  Asymmetric ICs:\n",
      "  Upside risk: WFC, ABT, PGR\n",
      "  Downside risk: RF, TFC\n",
      "\n",
      "ðŸ“ˆ Strong Directional Signals:\n",
      "  JPM: 70% bias â†‘, 6:3â†‘ breaks, +4.7% drift\n",
      "  MS: 70% bias â†‘, 6:3â†‘ breaks, +5.9% drift\n",
      "  MMC: 70% bias â†‘, 6:2â†‘ breaks, +3.4% drift\n",
      "  AXP: 74% bias â†‘, 9:4â†‘ breaks, +5.8% drift\n",
      "\n",
      "ðŸ’¡ Remember: Past patterns â‰  Future results. IV context shows current opportunity cost.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    YFINANCE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    YFINANCE_AVAILABLE = False\n",
    "    print(\"âš ï¸  yfinance not installed - IV data will not be available\")\n",
    "    print(\"   Install with: pip install yfinance\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "ALPHAVANTAGE_KEYS = [\n",
    "    \"HPCFVLGHWHQU0QTY\",\n",
    "    \"VL7Z4WRK8T5MJPK5\",\n",
    "    \"DYU6F4AG3IL03321\",\n",
    "    \"EXMUX4OSACRK51NZ\"\n",
    "]\n",
    "CURRENT_KEY_INDEX = 0\n",
    "RATE_LIMITED_KEYS = set()\n",
    "CACHE_FILE = \"earnings_cache.json\"\n",
    "RATE_LIMIT_FILE = \"rate_limits.json\"\n",
    "\n",
    "# ============================================================================\n",
    "# CACHING & RATE LIMITS\n",
    "# ============================================================================\n",
    "\n",
    "def load_cache():\n",
    "    \"\"\"Load cached earnings data\"\"\"\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_cache(cache):\n",
    "    \"\"\"Save earnings data to cache\"\"\"\n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        json.dump(cache, f, indent=2, default=str)\n",
    "\n",
    "def load_rate_limits():\n",
    "    \"\"\"Load rate limit state with 24-hour reset\"\"\"\n",
    "    if os.path.exists(RATE_LIMIT_FILE):\n",
    "        try:\n",
    "            with open(RATE_LIMIT_FILE, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            now = datetime.now().timestamp()\n",
    "            active_limits = {}\n",
    "            \n",
    "            for key_idx, info in data.items():\n",
    "                reset_time = info.get('reset_time', 0)\n",
    "                if reset_time > now:\n",
    "                    active_limits[int(key_idx)] = info\n",
    "            \n",
    "            return set(active_limits.keys())\n",
    "        except:\n",
    "            return set()\n",
    "    return set()\n",
    "\n",
    "def save_rate_limits(rate_limited_keys):\n",
    "    \"\"\"Persist rate limit state with 24-hour expiry\"\"\"\n",
    "    reset_time = (datetime.now() + timedelta(hours=24)).timestamp()\n",
    "    \n",
    "    data = {\n",
    "        str(k): {\n",
    "            'reset_time': reset_time,\n",
    "            'limited_at': datetime.now().isoformat()\n",
    "        } \n",
    "        for k in rate_limited_keys\n",
    "    }\n",
    "    \n",
    "    with open(RATE_LIMIT_FILE, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "# ============================================================================\n",
    "# IMPLIED VOLATILITY\n",
    "# ============================================================================\n",
    "\n",
    "def get_current_iv(ticker, dte_target=45, retry_count=2):\n",
    "    \"\"\"\n",
    "    Fetch current implied volatility from Yahoo Finance options chain\n",
    "    Returns ATM IV for expiration closest to target DTE\n",
    "    \"\"\"\n",
    "    if not YFINANCE_AVAILABLE:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        \n",
    "        # Get current price\n",
    "        hist = stock.history(period='1d')\n",
    "        if hist.empty:\n",
    "            return None\n",
    "        current_price = hist['Close'].iloc[-1]\n",
    "        \n",
    "        # Get available expirations\n",
    "        expirations = stock.options\n",
    "        if not expirations:\n",
    "            return None\n",
    "        \n",
    "        # Find expiration closest to target DTE\n",
    "        today = datetime.now()\n",
    "        target_exp = None\n",
    "        min_diff = 999\n",
    "        \n",
    "        for exp_str in expirations:\n",
    "            exp_date = datetime.strptime(exp_str, '%Y-%m-%d')\n",
    "            dte = (exp_date - today).days\n",
    "            if abs(dte - dte_target) < min_diff:\n",
    "                min_diff = abs(dte - dte_target)\n",
    "                target_exp = exp_str\n",
    "        \n",
    "        if not target_exp:\n",
    "            return None\n",
    "        \n",
    "        # Get options chain\n",
    "        chain = stock.option_chain(target_exp)\n",
    "        calls = chain.calls\n",
    "        \n",
    "        if calls.empty or 'impliedVolatility' not in calls.columns:\n",
    "            return None\n",
    "        \n",
    "        # Find ATM strike\n",
    "        calls['strike_diff'] = abs(calls['strike'] - current_price)\n",
    "        atm_idx = calls['strike_diff'].idxmin()\n",
    "        atm_call = calls.loc[atm_idx]\n",
    "        \n",
    "        iv_pct = atm_call['impliedVolatility'] * 100\n",
    "        actual_dte = (datetime.strptime(target_exp, '%Y-%m-%d') - today).days\n",
    "        \n",
    "        return {\n",
    "            'iv': round(iv_pct, 1),\n",
    "            'dte': actual_dte,\n",
    "            'strike': atm_call['strike'],\n",
    "            'expiration': target_exp\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        if retry_count > 0:\n",
    "            time.sleep(1)\n",
    "            return get_current_iv(ticker, dte_target, retry_count - 1)\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# EARNINGS DATA\n",
    "# ============================================================================\n",
    "\n",
    "def get_earnings_details(ticker, use_cache=True, debug=False):\n",
    "    \"\"\"Get earnings announcement dates from Alpha Vantage with automatic key rotation\"\"\"\n",
    "    global CURRENT_KEY_INDEX, RATE_LIMITED_KEYS\n",
    "    \n",
    "    # Check cache first\n",
    "    cache = load_cache()\n",
    "    if use_cache and ticker in cache:\n",
    "        return [\n",
    "            {'date': datetime.fromisoformat(e['date']), 'time': e['time']} \n",
    "            for e in cache[ticker]\n",
    "        ], \"cached\"\n",
    "    \n",
    "    # Load persisted rate limits\n",
    "    if not RATE_LIMITED_KEYS:\n",
    "        RATE_LIMITED_KEYS = load_rate_limits()\n",
    "    \n",
    "    # Check if all keys exhausted\n",
    "    if len(RATE_LIMITED_KEYS) >= len(ALPHAVANTAGE_KEYS):\n",
    "        if debug:\n",
    "            print(f\"  {ticker}: All API keys exhausted\")\n",
    "        return [], \"rate_limited_all\"\n",
    "    \n",
    "    # Try available keys\n",
    "    max_attempts = len(ALPHAVANTAGE_KEYS) - len(RATE_LIMITED_KEYS)\n",
    "    for attempt in range(max_attempts):\n",
    "        # Skip rate-limited keys\n",
    "        while CURRENT_KEY_INDEX in RATE_LIMITED_KEYS:\n",
    "            CURRENT_KEY_INDEX = (CURRENT_KEY_INDEX + 1) % len(ALPHAVANTAGE_KEYS)\n",
    "        \n",
    "        current_key = ALPHAVANTAGE_KEYS[CURRENT_KEY_INDEX]\n",
    "        url = f\"https://www.alphavantage.co/query?function=EARNINGS&symbol={ticker}&apikey={current_key}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            # Handle API errors\n",
    "            error_msg = data.get('Note', data.get('Information', ''))\n",
    "            if error_msg:\n",
    "                # Check for rate limit\n",
    "                if any(phrase in error_msg.lower() for phrase in ['rate limit', 'call frequency']):\n",
    "                    RATE_LIMITED_KEYS.add(CURRENT_KEY_INDEX)\n",
    "                    save_rate_limits(RATE_LIMITED_KEYS)\n",
    "                    \n",
    "                    CURRENT_KEY_INDEX = (CURRENT_KEY_INDEX + 1) % len(ALPHAVANTAGE_KEYS)\n",
    "                    \n",
    "                    if len(RATE_LIMITED_KEYS) >= len(ALPHAVANTAGE_KEYS):\n",
    "                        return [], \"rate_limited_all\"\n",
    "                    \n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                \n",
    "                return [], \"api_error\"\n",
    "            \n",
    "            # Parse earnings data\n",
    "            if 'quarterlyEarnings' not in data:\n",
    "                return [], \"no_earnings\"\n",
    "            \n",
    "            earnings_info = []\n",
    "            for quarter in data['quarterlyEarnings']:\n",
    "                reported_date = quarter.get('reportedDate')\n",
    "                reported_time = quarter.get('reportTime') or quarter.get('reportedTime', 'amc')\n",
    "                \n",
    "                if reported_date:\n",
    "                    # Normalize time values\n",
    "                    time_map = {'pre-market': 'bmo', 'post-market': 'amc'}\n",
    "                    normalized_time = time_map.get(reported_time.lower(), reported_time.lower())\n",
    "                    \n",
    "                    earnings_info.append({\n",
    "                        'date': datetime.strptime(reported_date, '%Y-%m-%d'),\n",
    "                        'time': normalized_time\n",
    "                    })\n",
    "            \n",
    "            # Cache the results\n",
    "            cache[ticker] = [\n",
    "                {'date': e['date'].isoformat(), 'time': e['time']} \n",
    "                for e in earnings_info\n",
    "            ]\n",
    "            save_cache(cache)\n",
    "            \n",
    "            return sorted(earnings_info, key=lambda x: x['date'], reverse=True), \"success\"\n",
    "        \n",
    "        except Exception:\n",
    "            if attempt < max_attempts - 1:\n",
    "                CURRENT_KEY_INDEX = (CURRENT_KEY_INDEX + 1) % len(ALPHAVANTAGE_KEYS)\n",
    "                continue\n",
    "            return [], \"exception\"\n",
    "    \n",
    "    return [], \"unknown_error\"\n",
    "\n",
    "# ============================================================================\n",
    "# PRICE DATA\n",
    "# ============================================================================\n",
    "\n",
    "def get_yahoo_price_data(ticker, start_date, end_date):\n",
    "    \"\"\"Get historical closing prices from Yahoo Finance\"\"\"\n",
    "    start_ts = int(start_date.timestamp())\n",
    "    end_ts = int(end_date.timestamp())\n",
    "    \n",
    "    url = f\"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}\"\n",
    "    params = {'period1': start_ts, 'period2': end_ts, 'interval': '1d'}\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers, timeout=10)\n",
    "        data = response.json()\n",
    "        \n",
    "        result = data['chart']['result'][0]\n",
    "        timestamps = result['timestamp']\n",
    "        closes = result['indicators']['quote'][0]['close']\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'date': [datetime.fromtimestamp(ts) for ts in timestamps],\n",
    "            'close': closes\n",
    "        })\n",
    "        df.set_index('date', inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching prices for {ticker}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# CALCULATIONS\n",
    "# ============================================================================\n",
    "\n",
    "def find_nearest_price(price_data, target_date):\n",
    "    \"\"\"Find closing price on nearest trading day\"\"\"\n",
    "    if price_data.empty:\n",
    "        return None, None\n",
    "    \n",
    "    start = target_date - timedelta(days=7)\n",
    "    end = target_date + timedelta(days=7)\n",
    "    nearby = price_data[(price_data.index >= start) & (price_data.index <= end)]\n",
    "    \n",
    "    if nearby.empty:\n",
    "        return None, None\n",
    "    \n",
    "    time_diffs = (nearby.index - target_date).to_series().abs()\n",
    "    closest_idx = time_diffs.argmin()\n",
    "    return nearby.iloc[closest_idx]['close'], nearby.index[closest_idx]\n",
    "\n",
    "def get_reference_price(price_data, earnings_date, timing):\n",
    "    \"\"\"Get entry price based on earnings timing\"\"\"\n",
    "    target_date = earnings_date - timedelta(days=1) if timing == 'bmo' else earnings_date\n",
    "    return find_nearest_price(price_data, target_date)\n",
    "\n",
    "def calculate_historical_volatility(price_data, earnings_date, lookback_days=30):\n",
    "    \"\"\"Calculate annualized historical volatility\"\"\"\n",
    "    end_date = earnings_date - timedelta(days=1)\n",
    "    start_date = end_date - timedelta(days=lookback_days + 10)\n",
    "    \n",
    "    window = price_data[(price_data.index >= start_date) & (price_data.index <= end_date)]\n",
    "    \n",
    "    if len(window) < 20:\n",
    "        return None\n",
    "    \n",
    "    returns = window['close'].pct_change().dropna()\n",
    "    daily_vol = returns.std()\n",
    "    annual_vol = daily_vol * np.sqrt(252)\n",
    "    \n",
    "    return annual_vol\n",
    "\n",
    "def get_volatility_tier(hvol):\n",
    "    \"\"\"Map historical volatility to strike width multiplier\"\"\"\n",
    "    hvol_pct = hvol * 100\n",
    "    \n",
    "    if hvol_pct < 25:\n",
    "        return 1.0\n",
    "    elif hvol_pct < 35:\n",
    "        return 1.2\n",
    "    elif hvol_pct < 45:\n",
    "        return 1.4\n",
    "    else:\n",
    "        return 1.5\n",
    "\n",
    "def calculate_stats(data):\n",
    "    \"\"\"Calculate containment and directional statistics\"\"\"\n",
    "    total = len(data)\n",
    "    moves = np.array([d['move'] for d in data])\n",
    "    widths = np.array([d['width'] for d in data])\n",
    "    \n",
    "    # Containment\n",
    "    stays_within = sum(1 for i, m in enumerate(moves) if abs(m) <= widths[i])\n",
    "    breaks_up = sum(1 for i, m in enumerate(moves) if m > widths[i])\n",
    "    breaks_down = sum(1 for i, m in enumerate(moves) if m < -widths[i])\n",
    "    \n",
    "    # Overall directional bias\n",
    "    up_moves = sum(1 for m in moves if m > 0)\n",
    "    overall_bias = (up_moves / total) * 100\n",
    "    \n",
    "    # Break directional bias\n",
    "    total_breaks = breaks_up + breaks_down\n",
    "    if total_breaks > 0:\n",
    "        break_bias = (breaks_up / total_breaks) * 100\n",
    "    else:\n",
    "        break_bias = 50.0\n",
    "    \n",
    "    # Magnitude-weighted average move\n",
    "    avg_move = np.mean(moves)\n",
    "    avg_move_vs_width = (avg_move / np.mean(widths)) * 100 if np.mean(widths) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total': total,\n",
    "        'containment': (stays_within / total) * 100,\n",
    "        'breaks_up': breaks_up,\n",
    "        'breaks_down': breaks_down,\n",
    "        'overall_bias': overall_bias,\n",
    "        'break_bias': break_bias,\n",
    "        'avg_move_pct': avg_move,\n",
    "        'drift_vs_width': avg_move_vs_width,\n",
    "        'avg_width': np.mean(widths)\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# STRATEGY LOGIC\n",
    "# ============================================================================\n",
    "\n",
    "def determine_strategy(stats_45, stats_90):\n",
    "    \"\"\"\n",
    "    Determine trading strategy based on containment and directional patterns\n",
    "    Returns: (strategy_name, reason_string)\n",
    "    \"\"\"\n",
    "    rec_parts = []\n",
    "    bias_reasons = []\n",
    "    \n",
    "    # CATEGORY 1: CONTAINMENT STRATEGIES (IC)\n",
    "    if stats_90['containment'] >= 69.5:\n",
    "        break_ratio = max(stats_90['breaks_up'], stats_90['breaks_down']) / (\n",
    "            min(stats_90['breaks_up'], stats_90['breaks_down']) + 1\n",
    "        )\n",
    "        \n",
    "        if break_ratio < 2:\n",
    "            rec_parts.append(\"IC90\")\n",
    "        elif stats_90['break_bias'] >= 70:\n",
    "            rec_parts.append(\"IC90âš â†‘\")\n",
    "        else:\n",
    "            rec_parts.append(\"IC90âš â†“\")\n",
    "            \n",
    "    elif stats_45['containment'] >= 69.5:\n",
    "        break_ratio = max(stats_45['breaks_up'], stats_45['breaks_down']) / (\n",
    "            min(stats_45['breaks_up'], stats_45['breaks_down']) + 1\n",
    "        )\n",
    "        \n",
    "        if break_ratio < 2:\n",
    "            rec_parts.append(\"IC45\")\n",
    "        elif stats_45['break_bias'] >= 70:\n",
    "            rec_parts.append(\"IC45âš â†‘\")\n",
    "        else:\n",
    "            rec_parts.append(\"IC45âš â†“\")\n",
    "    \n",
    "    # CATEGORY 2: DIRECTIONAL BIAS\n",
    "    else:\n",
    "        has_upward_edge = False\n",
    "        has_downward_edge = False\n",
    "        \n",
    "        # Check for UPWARD edge\n",
    "        if stats_90['overall_bias'] >= 65:\n",
    "            has_upward_edge = True\n",
    "            bias_reasons.append(f\"{stats_90['overall_bias']:.0f}% bias\")\n",
    "        \n",
    "        if stats_90['breaks_up'] >= stats_90['breaks_down'] * 1.5 and stats_90['breaks_up'] >= 2:\n",
    "            has_upward_edge = True\n",
    "            bias_reasons.append(f\"{stats_90['breaks_up']}:{stats_90['breaks_down']}â†‘ breaks\")\n",
    "        \n",
    "        if stats_90['avg_move_pct'] >= 3.0:\n",
    "            has_upward_edge = True\n",
    "            bias_reasons.append(f\"{stats_90['avg_move_pct']:+.1f}% drift\")\n",
    "        \n",
    "        # Check for DOWNWARD edge\n",
    "        if stats_90['overall_bias'] <= 35:\n",
    "            has_downward_edge = True\n",
    "            bias_reasons.append(f\"{stats_90['overall_bias']:.0f}% bias\")\n",
    "        \n",
    "        if stats_90['breaks_down'] >= stats_90['breaks_up'] * 1.5 and stats_90['breaks_down'] >= 2:\n",
    "            has_downward_edge = True\n",
    "            bias_reasons.append(f\"{stats_90['breaks_up']}:{stats_90['breaks_down']}â†“ breaks\")\n",
    "        \n",
    "        if stats_90['avg_move_pct'] <= -3.0:\n",
    "            has_downward_edge = True\n",
    "            bias_reasons.append(f\"{stats_90['avg_move_pct']:+.1f}% drift\")\n",
    "        \n",
    "        # Assign directional recommendation\n",
    "        if has_upward_edge:\n",
    "            reason_str = \", \".join(bias_reasons)\n",
    "            rec_parts.append(f\"BIASâ†‘ ({reason_str})\")\n",
    "        elif has_downward_edge:\n",
    "            reason_str = \", \".join(bias_reasons)\n",
    "            rec_parts.append(f\"BIASâ†“ ({reason_str})\")\n",
    "    \n",
    "    return rec_parts[0] if rec_parts else \"SKIP\"\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_earnings_movement(ticker, lookback_quarters=24, verbose=True, debug=False):\n",
    "    \"\"\"Analyze post-earnings movements with volatility-adjusted strikes\"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*75}\")\n",
    "        print(f\"ðŸ“Š {ticker} - Post-Earnings Containment Analysis\")\n",
    "        print(f\"{'='*75}\")\n",
    "    \n",
    "    # Get earnings dates\n",
    "    earnings_info, status = get_earnings_details(ticker, debug=debug)\n",
    "    if not earnings_info:\n",
    "        return None, status\n",
    "    \n",
    "    # Filter to past earnings\n",
    "    today = datetime.now()\n",
    "    past_earnings = [e for e in earnings_info if e['date'] < today][:lookback_quarters]\n",
    "    \n",
    "    if len(past_earnings) < 10:\n",
    "        if verbose:\n",
    "            print(f\"âš ï¸  Insufficient data: only {len(past_earnings)} earnings periods\")\n",
    "        return None, \"insufficient_quarters\"\n",
    "    \n",
    "    # Get price data\n",
    "    oldest = min([e['date'] for e in past_earnings]) - timedelta(days=120)\n",
    "    price_data = get_yahoo_price_data(ticker, oldest, today)\n",
    "    \n",
    "    if price_data.empty:\n",
    "        return None, \"no_price_data\"\n",
    "    \n",
    "    # Collect movement data\n",
    "    data_45 = []\n",
    "    data_90 = []\n",
    "    hvol_list = []\n",
    "    \n",
    "    for earnings in past_earnings:\n",
    "        hvol = calculate_historical_volatility(price_data, earnings['date'])\n",
    "        if hvol is None:\n",
    "            continue\n",
    "        \n",
    "        hvol_list.append(hvol * 100)\n",
    "        strike_std = get_volatility_tier(hvol)\n",
    "        \n",
    "        # Get entry price\n",
    "        ref_price, ref_date = get_reference_price(price_data, earnings['date'], earnings['time'])\n",
    "        if ref_price is None:\n",
    "            continue\n",
    "        \n",
    "        # Calculate strike widths\n",
    "        dte_45_factor = np.sqrt(45 / 365)\n",
    "        dte_90_factor = np.sqrt(90 / 365)\n",
    "        strike_width_45 = hvol * dte_45_factor * strike_std * 100\n",
    "        strike_width_90 = hvol * dte_90_factor * strike_std * 100\n",
    "        \n",
    "        # Test 45-day outcome\n",
    "        target_45 = earnings['date'] + timedelta(days=45)\n",
    "        if target_45 <= today:\n",
    "            price_45, date_45 = find_nearest_price(price_data, target_45)\n",
    "            if price_45 is not None:\n",
    "                move_45 = (price_45 - ref_price) / ref_price * 100\n",
    "                data_45.append({\n",
    "                    'move': move_45,\n",
    "                    'width': strike_width_45,\n",
    "                    'hvol': hvol * 100,\n",
    "                    'date': earnings['date'].strftime('%Y-%m-%d')\n",
    "                })\n",
    "        \n",
    "        # Test 90-day outcome\n",
    "        target_90 = earnings['date'] + timedelta(days=90)\n",
    "        if target_90 <= today:\n",
    "            price_90, date_90 = find_nearest_price(price_data, target_90)\n",
    "            if price_90 is not None:\n",
    "                move_90 = (price_90 - ref_price) / ref_price * 100\n",
    "                data_90.append({\n",
    "                    'move': move_90,\n",
    "                    'width': strike_width_90,\n",
    "                    'hvol': hvol * 100,\n",
    "                    'date': earnings['date'].strftime('%Y-%m-%d')\n",
    "                })\n",
    "    \n",
    "    if len(data_45) < 10 or len(data_90) < 10:\n",
    "        if verbose:\n",
    "            print(f\"âš ï¸  Insufficient valid data\")\n",
    "        return None, \"insufficient_valid_data\"\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats_45 = calculate_stats(data_45)\n",
    "    stats_90 = calculate_stats(data_90)\n",
    "    avg_hvol = np.mean(hvol_list)\n",
    "    avg_tier = get_volatility_tier(avg_hvol / 100)\n",
    "    \n",
    "    # Determine strategy\n",
    "    recommendation = determine_strategy(stats_45, stats_90)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nðŸ“Š {ticker} | {avg_hvol:.1f}% HVol | {avg_tier:.1f} std (Â±{stats_90['avg_width']:.1f}%)\")\n",
    "        print(f\"\\n  45-Day: {stats_45['total']}/{lookback_quarters} tested\")\n",
    "        print(f\"    Containment: {stats_45['containment']:.0f}%\")\n",
    "        print(f\"    Breaks: Up {stats_45['breaks_up']}, Down {stats_45['breaks_down']}\")\n",
    "        print(f\"    Overall Bias: {stats_45['overall_bias']:.0f}% up\")\n",
    "        print(f\"    Break Bias: {stats_45['break_bias']:.0f}% of breaks were upward\")\n",
    "        print(f\"    Avg Drift: {stats_45['avg_move_pct']:+.1f}% ({stats_45['drift_vs_width']:+.0f}% of width)\")\n",
    "        \n",
    "        print(f\"\\n  90-Day: {stats_90['total']}/{lookback_quarters} tested\")\n",
    "        print(f\"    Containment: {stats_90['containment']:.0f}%\")\n",
    "        print(f\"    Breaks: Up {stats_90['breaks_up']}, Down {stats_90['breaks_down']}\")\n",
    "        print(f\"    Overall Bias: {stats_90['overall_bias']:.0f}% up\")\n",
    "        print(f\"    Break Bias: {stats_90['break_bias']:.0f}% of breaks were upward\")\n",
    "        print(f\"    Avg Drift: {stats_90['avg_move_pct']:+.1f}% ({stats_90['drift_vs_width']:+.0f}% of width)\")\n",
    "        \n",
    "        print(f\"\\n  ðŸ’¡ Strategy: {recommendation}\")\n",
    "    \n",
    "    summary = {\n",
    "        'ticker': ticker,\n",
    "        'hvol': round(avg_hvol, 1),\n",
    "        'tier': round(avg_tier, 1),\n",
    "        'strike_width': round(stats_90['avg_width'], 1),\n",
    "        '45d_contain': round(stats_45['containment'], 0),\n",
    "        '45d_breaks_up': stats_45['breaks_up'],\n",
    "        '45d_breaks_dn': stats_45['breaks_down'],\n",
    "        '45d_overall_bias': round(stats_45['overall_bias'], 0),\n",
    "        '45d_break_bias': round(stats_45['break_bias'], 0),\n",
    "        '45d_drift': round(stats_45['avg_move_pct'], 1),\n",
    "        '90d_contain': round(stats_90['containment'], 0),\n",
    "        '90d_breaks_up': stats_90['breaks_up'],\n",
    "        '90d_breaks_dn': stats_90['breaks_down'],\n",
    "        '90d_overall_bias': round(stats_90['overall_bias'], 0),\n",
    "        '90d_break_bias': round(stats_90['break_bias'], 0),\n",
    "        '90d_drift': round(stats_90['avg_move_pct'], 1),\n",
    "        'strategy': recommendation\n",
    "    }\n",
    "    \n",
    "    return summary, \"success\"\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def format_break_ratio(up_breaks, down_breaks, break_bias):\n",
    "    \"\"\"Format break ratio with directional arrow if 2:1 edge exists\"\"\"\n",
    "    if up_breaks == 0 and down_breaks == 0:\n",
    "        return \"0:0\"\n",
    "    \n",
    "    if break_bias >= 66.7:\n",
    "        return f\"{up_breaks}:{down_breaks}â†‘\"\n",
    "    elif break_bias <= 33.3:\n",
    "        return f\"{up_breaks}:{down_breaks}â†“\"\n",
    "    else:\n",
    "        return f\"{up_breaks}:{down_breaks}\"\n",
    "\n",
    "def batch_analyze(tickers, lookback_quarters=24, debug=False, fetch_iv=True):\n",
    "    \"\"\"Analyze multiple tickers with progress tracking\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*75)\n",
    "    print(f\"EARNINGS CONTAINMENT ANALYZER - v2.3\")\n",
    "    print(f\"Lookback: {lookback_quarters} quarters (~{lookback_quarters/4:.0f} years)\")\n",
    "    if fetch_iv and YFINANCE_AVAILABLE:\n",
    "        print(f\"Current IV from Yahoo Finance (15-20min delayed)\")\n",
    "    print(\"=\"*75)\n",
    "    \n",
    "    # Show rate limit status\n",
    "    rate_limited_keys = load_rate_limits()\n",
    "    if rate_limited_keys:\n",
    "        available = len(ALPHAVANTAGE_KEYS) - len(rate_limited_keys)\n",
    "        print(f\"\\nâš ï¸  Rate Limit: {available}/{len(ALPHAVANTAGE_KEYS)} API keys available\")\n",
    "    else:\n",
    "        print(f\"\\nâœ“ All {len(ALPHAVANTAGE_KEYS)} API keys available\")\n",
    "    \n",
    "    results = []\n",
    "    fetch_summary = {'cached': [], 'api': [], 'failed': []}\n",
    "    iv_summary = {'success': [], 'failed': []}\n",
    "    \n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        print(f\"\\r[{i}/{len(tickers)}] Processing {ticker}...\", end='', flush=True)\n",
    "        \n",
    "        cache = load_cache()\n",
    "        from_cache = ticker in cache\n",
    "        \n",
    "        summary, status = analyze_earnings_movement(ticker, lookback_quarters, verbose=False, debug=debug)\n",
    "        \n",
    "        if summary:\n",
    "            # Fetch current IV\n",
    "            if fetch_iv and YFINANCE_AVAILABLE:\n",
    "                iv_data = get_current_iv(ticker, dte_target=45)\n",
    "                if iv_data:\n",
    "                    summary['current_iv'] = iv_data['iv']\n",
    "                    summary['iv_dte'] = iv_data['dte']\n",
    "                    iv_premium = ((iv_data['iv'] - summary['hvol']) / summary['hvol']) * 100\n",
    "                    summary['iv_premium'] = round(iv_premium, 1)\n",
    "                    iv_summary['success'].append(ticker)\n",
    "                else:\n",
    "                    summary['current_iv'] = None\n",
    "                    summary['iv_dte'] = None\n",
    "                    summary['iv_premium'] = None\n",
    "                    iv_summary['failed'].append(ticker)\n",
    "            else:\n",
    "                summary['current_iv'] = None\n",
    "                summary['iv_dte'] = None\n",
    "                summary['iv_premium'] = None\n",
    "            \n",
    "            results.append(summary)\n",
    "            if from_cache:\n",
    "                fetch_summary['cached'].append(ticker)\n",
    "            else:\n",
    "                fetch_summary['api'].append(ticker)\n",
    "        else:\n",
    "            fetch_summary['failed'].append(ticker)\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(\"\\r\" + \" \" * 80 + \"\\r\", end='')\n",
    "    \n",
    "    # Print fetch summary\n",
    "    print(f\"\\nðŸ“Š FETCH SUMMARY\")\n",
    "    print(f\"{'='*75}\")\n",
    "    if fetch_summary['cached']:\n",
    "        print(f\"âœ“ Earnings Cached ({len(fetch_summary['cached'])}): {', '.join(fetch_summary['cached'][:5])}{'...' if len(fetch_summary['cached']) > 5 else ''}\")\n",
    "    if fetch_summary['api']:\n",
    "        print(f\"âœ“ Earnings API ({len(fetch_summary['api'])}): {', '.join(fetch_summary['api'])}\")\n",
    "    if fetch_iv and YFINANCE_AVAILABLE:\n",
    "        if iv_summary['success']:\n",
    "            print(f\"âœ“ IV Retrieved ({len(iv_summary['success'])}): {', '.join(iv_summary['success'][:5])}{'...' if len(iv_summary['success']) > 5 else ''}\")\n",
    "        if iv_summary['failed']:\n",
    "            print(f\"âœ— IV Failed ({len(iv_summary['failed'])}): {', '.join(iv_summary['failed'])}\")\n",
    "    if fetch_summary['failed']:\n",
    "        print(f\"âœ— Analysis Failed ({len(fetch_summary['failed'])}): {', '.join(fetch_summary['failed'])}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"\\nâš ï¸  No valid results\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate 45-day strike width\n",
    "    df['45d_width'] = df.apply(lambda x: round(x['strike_width'] * np.sqrt(45/90), 1), axis=1)\n",
    "    \n",
    "    # Format break ratios\n",
    "    df['45_break_fmt'] = df.apply(\n",
    "        lambda x: format_break_ratio(x['45d_breaks_up'], x['45d_breaks_dn'], x['45d_break_bias']), \n",
    "        axis=1\n",
    "    )\n",
    "    df['90_break_fmt'] = df.apply(\n",
    "        lambda x: format_break_ratio(x['90d_breaks_up'], x['90d_breaks_dn'], x['90d_break_bias']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Format strategies\n",
    "    df['strategy_display'] = df['strategy'].apply(lambda x: \n",
    "        x.replace('BIASâ†‘', 'BIASâ†‘').replace('BIASâ†“', 'BIASâ†“') if 'BIAS' in x else x\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n{'='*110}\")\n",
    "    print(\"BACKTEST RESULTS\")\n",
    "    print(\"=\"*110)\n",
    "    \n",
    "    # Prepare display columns\n",
    "    display_cols = {\n",
    "        'Ticker': df['ticker'],\n",
    "        'HVol%': df['hvol'].astype(int),\n",
    "    }\n",
    "    \n",
    "    # Add IV columns if available\n",
    "    if 'current_iv' in df.columns and df['current_iv'].notna().any():\n",
    "        display_cols['CurIV%'] = df['current_iv'].apply(lambda x: f\"{int(x)}\" if pd.notna(x) else \"N/A\")\n",
    "        display_cols['IVPrem'] = df['iv_premium'].apply(lambda x: f\"{x:+.0f}%\" if pd.notna(x) else \"N/A\")\n",
    "        display_cols['|'] = '|'\n",
    "    \n",
    "    display_cols.update({\n",
    "        '90D%': df['90d_contain'].astype(int),\n",
    "        '90Bias': df['90d_overall_bias'].astype(int),\n",
    "        '90Break': df['90_break_fmt'],\n",
    "        '90Drift': df['90d_drift'].apply(lambda x: f\"{x:+.1f}%\"),\n",
    "        ' | ': '|',\n",
    "        'Pattern': df['strategy_display']\n",
    "    })\n",
    "    \n",
    "    display_df = pd.DataFrame(display_cols)\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Summary insights\n",
    "    print(f\"\\n{'='*110}\")\n",
    "    print(\"KEY TAKEAWAYS:\")\n",
    "    print(\"=\"*110)\n",
    "    \n",
    "    # Pattern counts\n",
    "    ic_count = len(df[df['strategy'].str.contains('IC', na=False)])\n",
    "    bias_up_count = len(df[df['strategy'].str.contains('BIASâ†‘', na=False)])\n",
    "    bias_down_count = len(df[df['strategy'].str.contains('BIASâ†“', na=False)])\n",
    "    skip_count = len(df[df['strategy'] == 'SKIP'])\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Pattern Summary: {ic_count} IC candidates | {bias_up_count} Upward bias | {bias_down_count} Downward bias | {skip_count} No edge\")\n",
    "    \n",
    "    # IV Context (if available)\n",
    "    if 'iv_premium' in df.columns and df['iv_premium'].notna().any():\n",
    "        elevated = df[df['iv_premium'] >= 15].sort_values('iv_premium', ascending=False)\n",
    "        depressed = df[df['iv_premium'] <= -15].sort_values('iv_premium')\n",
    "        \n",
    "        print(f\"\\nðŸ’° IV Landscape:\")\n",
    "        if not elevated.empty:\n",
    "            tickers_str = ', '.join([f\"{row['ticker']}(+{row['iv_premium']:.0f}%)\" for _, row in elevated.head(5).iterrows()])\n",
    "            print(f\"  Rich Premium (â‰¥15%): {tickers_str}\")\n",
    "        if not depressed.empty:\n",
    "            tickers_str = ', '.join([f\"{row['ticker']}({row['iv_premium']:.0f}%)\" for _, row in depressed.head(3).iterrows()])\n",
    "            print(f\"  Thin Premium (â‰¤-15%): {tickers_str}\")\n",
    "        \n",
    "        normal_count = len(df[(df['iv_premium'] > -15) & (df['iv_premium'] < 15)])\n",
    "        if normal_count > 0:\n",
    "            print(f\"  Normal Range: {normal_count} tickers\")\n",
    "    \n",
    "    # Asymmetric ICs\n",
    "    ic_up_skew = df[(df['strategy'].str.contains('IC.*âš â†‘', regex=True, na=False))]\n",
    "    ic_down_skew = df[(df['strategy'].str.contains('IC.*âš â†“', regex=True, na=False))]\n",
    "    \n",
    "    if not ic_up_skew.empty or not ic_down_skew.empty:\n",
    "        print(f\"\\nâš ï¸  Asymmetric ICs:\")\n",
    "        if not ic_up_skew.empty:\n",
    "            print(f\"  Upside risk: {', '.join(ic_up_skew['ticker'].tolist())}\")\n",
    "        if not ic_down_skew.empty:\n",
    "            print(f\"  Downside risk: {', '.join(ic_down_skew['ticker'].tolist())}\")\n",
    "    \n",
    "    # Strong directional edges\n",
    "    strong_bias = df[\n",
    "        (df['strategy'].str.contains('BIAS', na=False)) &\n",
    "        ((df['90d_overall_bias'] >= 70) | (df['90d_overall_bias'] <= 30))\n",
    "    ]\n",
    "    if not strong_bias.empty:\n",
    "        print(f\"\\nðŸ“ˆ Strong Directional Signals:\")\n",
    "        for _, row in strong_bias.iterrows():\n",
    "            direction = \"â†‘\" if row['90d_overall_bias'] >= 70 else \"â†“\"\n",
    "            print(f\"  {row['ticker']}: {row['90d_overall_bias']:.0f}% bias {direction}, {row['90_break_fmt']} breaks, {row['90d_drift']:+.1f}% drift\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Remember: Past patterns â‰  Future results. IV context shows current opportunity cost.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# RUN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test tickers\n",
    "    tickers = [\"DAL\", \"PEP\", \"FAST\", \"BLK\", \"C\", \"DPZ\", \"GS\", \"JNJ\", \"JPM\", \"WFC\", \n",
    "               \"OMC\", \"ABT\", \"BAC\", \"CFG\", \"MS\", \"PGR\", \"PLD\", \"PNC\", \"SYF\", \"JBHT\", \n",
    "               \"UAL\", \"BK\", \"KEY\", \"MMC\", \"MTB\", \"SCHW\", \"SNA\", \"TRV\", \"USB\", \"CSX\", \n",
    "               \"AXP\", \"FITB\", \"HBAN\", \"RF\", \"SLB\", \"STT\", \"TFC\", \"STLD\"]\n",
    "    \n",
    "    results = batch_analyze(tickers, lookback_quarters=24, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "727bd8b6-f5e4-45d5-97ee-3c7d2566691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      "EARNINGS CONTAINMENT ANALYZER - v2.2\n",
      "Lookback: 24 quarters (~6 years)\n",
      "Volatility Tiers: <25%=1.0std | 25-35%=1.2std | 35-45%=1.4std | >45%=1.5std\n",
      "Logic: Containment (IC) vs Directional Bias vs Skip\n",
      "NEW v2.2: Current IV from Yahoo Finance (15-20min delayed)\n",
      "===========================================================================\n",
      "\n",
      "âœ“ All 4 API keys available\n",
      "                                                                                \n",
      "ðŸ“Š FETCH SUMMARY\n",
      "===========================================================================\n",
      "âœ“ Earnings From Cache (38): DAL, PEP, FAST, BLK, C...\n",
      "âœ“ IV Data Retrieved (38): DAL, PEP, FAST, BLK, C...\n",
      "\n",
      "==================================================================================================================================\n",
      "EARNINGS BACKTEST RESULTS (v2.2 - with Current IV Context)\n",
      "==================================================================================================================================\n",
      "Historical: HVol = past volatility | Contain% = stayed within strikes | Bias = % up moves | Drift = avg move\n",
      "Current: CurIV = current implied vol | IVPrem = IV vs historical (+ means elevated)\n",
      "Note: This shows what happened historically. Current IV context helps assess if premium is attractive NOW.\n",
      "==================================================================================================================================\n",
      "Ticker  HVol% CurIV% IVPrem |  90D%  90Bias 90Break 90Drift  |                                     Pattern\n",
      "   DAL     43     49   +14% |    67      62     5:3   +3.5%   |                                       IC45\n",
      "   PEP     18     26   +44% |    74      65     3:3   +1.3%   |                                       IC90\n",
      "  FAST     24     31   +26% |    78      61     3:2   +4.0%   |                                       IC90\n",
      "   BLK     28     29    +4% |    61      57     5:4   +4.9%   |                        BIASâ†‘ (+4.9% drift)\n",
      "     C     34     31    -8% |    70      48     4:3   +2.9%   |                                       IC90\n",
      "   DPZ     27     37   +36% |    65      61     5:3   +2.9%   |                        BIASâ†‘ (5:3â†‘ breaks)\n",
      "    GS     32     35    +9% |    70      61    5:2â†‘   +6.6%   |                                       IC90\n",
      "   JNJ     17     22   +25% |    78      52     2:3   +0.4%   |                                       IC90\n",
      "   JPM     28     28    +2% |    61      70    6:3â†‘   +4.7%   | BIASâ†‘ (70% bias, 6:3â†‘ breaks, +4.7% drift)\n",
      "   WFC     35     30   -14% |    65      65     5:3   +4.6%   |                                     IC45âš â†‘\n",
      "   OMC     26     39   +50% |    65      61     3:5   +0.2%   |                        BIASâ†“ (3:5â†“ breaks)\n",
      "   ABT     22     25   +15% |    78      57    4:1â†‘   +2.5%   |                                     IC90âš â†‘\n",
      "   BAC     32     30    -5% |    61      61     5:4   +3.1%   |                        BIASâ†‘ (+3.1% drift)\n",
      "   CFG     40     29   -27% |    74      65     3:3   +3.1%   |                                       IC90\n",
      "    MS     33     30    -9% |    61      70    6:3â†‘   +5.9%   | BIASâ†‘ (70% bias, 6:3â†‘ breaks, +5.9% drift)\n",
      "   PGR     24     35   +44% |    70      70    7:0â†‘   +6.0%   |                                     IC90âš â†‘\n",
      "   PLD     29     29    +0% |    61      61    6:3â†‘   +1.3%   |                                       IC45\n",
      "   PNC     32     26   -17% |    61      70     5:4   +4.3%   |                                       IC45\n",
      "   SYF     41     44    +8% |    70      65     3:4   +7.9%   |                                       IC90\n",
      "  JBHT     29     47   +61% |    74      52    2:4â†“   +1.4%   |                                       IC90\n",
      "   UAL     51     58   +12% |    70      57     4:3   +4.3%   |                                       IC90\n",
      "    BK     29     30    +4% |    74      65    4:2â†‘   +4.3%   |                                       IC90\n",
      "   KEY     42     35   -16% |    78      61     2:3   +3.7%   |                                       IC90\n",
      "   MMC     18     34   +82% |    65      70    6:2â†‘   +3.4%   | BIASâ†‘ (70% bias, 6:2â†‘ breaks, +3.4% drift)\n",
      "   MTB     33     37   +11% |    61      65     4:5   +2.6%   |                                       IC45\n",
      "  SCHW     34     32    -8% |    70      65     3:4   +5.3%   |                                       IC90\n",
      "   SNA     25     31   +23% |    65      61     4:4   +3.7%   |                        BIASâ†‘ (+3.7% drift)\n",
      "   TRV     25     27   +10% |    70      61    5:2â†‘   +4.0%   |                                       IC90\n",
      "   USB     33     28   -14% |    70      61     3:4   +0.6%   |                                       IC90\n",
      "   CSX     25     27   +10% |    65      65     4:4   +2.7%   |                           BIASâ†‘ (65% bias)\n",
      "   AXP     30     32    +7% |    43      74    9:4â†‘   +5.8%   | BIASâ†‘ (74% bias, 9:4â†‘ breaks, +5.8% drift)\n",
      "  FITB     38     30   -21% |    70      57     4:3   +3.3%   |                                       IC90\n",
      "  HBAN     35     36    +1% |    78      61     2:3   +1.9%   |                                       IC90\n",
      "    RF     37     32   -14% |    78      61    1:4â†“   +3.7%   |                                     IC90âš â†“\n",
      "   SLB     43     39   -10% |    87      61    1:2â†“   +3.7%   |                                       IC90\n",
      "   STT     33     39   +18% |    57      61     5:5   +2.5%   |                                       SKIP\n",
      "   TFC     36     27   -26% |    78      57    0:5â†“   +0.4%   |                                     IC90âš â†“\n",
      "  STLD     42     41    -2% |    78      65     3:2   +9.8%   |                                       IC90\n",
      "\n",
      "==================================================================================================================================\n",
      "KEY TAKEAWAYS:\n",
      "==================================================================================================================================\n",
      "\n",
      "ðŸ“Š Pattern Summary: 27 IC candidates | 9 Upward bias | 1 Downward bias | 1 No edge\n",
      "\n",
      "ðŸ’° IV Landscape:\n",
      "  Rich Premium (IV elevated â‰¥15%): MMC(+82%), JBHT(+61%), OMC(+50%), PEP(+44%), PGR(+44%)\n",
      "  Thin Premium (IV depressed â‰¤-15%): CFG(-27%), TFC(-26%), FITB(-21%)\n",
      "  Normal Range: 22 tickers\n",
      "\n",
      "âš ï¸  Asymmetric ICs:\n",
      "  Upside risk: WFC, ABT, PGR\n",
      "  Downside risk: RF, TFC\n",
      "\n",
      "ðŸ“ˆ Strong Directional Signals:\n",
      "  JPM: 70% bias â†‘, 6:3â†‘ breaks, +4.7% drift\n",
      "  MS: 70% bias â†‘, 6:3â†‘ breaks, +5.9% drift\n",
      "  MMC: 70% bias â†‘, 6:2â†‘ breaks, +3.4% drift\n",
      "  AXP: 74% bias â†‘, 9:4â†‘ breaks, +5.8% drift\n",
      "\n",
      "ðŸ’¡ Remember: Past patterns â‰  Future results. IV context shows current opportunity cost.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    YFINANCE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    YFINANCE_AVAILABLE = False\n",
    "    print(\"âš ï¸  yfinance not installed - IV data will not be available\")\n",
    "    print(\"   Install with: pip install yfinance\")\n",
    "\n",
    "# API Configuration\n",
    "ALPHAVANTAGE_KEYS = [\n",
    "    \"HPCFVLGHWHQU0QTY\",\n",
    "    \"VL7Z4WRK8T5MJPK5\",\n",
    "    \"DYU6F4AG3IL03321\",\n",
    "    \"EXMUX4OSACRK51NZ\"\n",
    "]\n",
    "CURRENT_KEY_INDEX = 0\n",
    "RATE_LIMITED_KEYS = set()\n",
    "CACHE_FILE = \"earnings_cache.json\"\n",
    "RATE_LIMIT_FILE = \"rate_limits.json\"  # NEW v2.1: Persistent rate limit tracking\n",
    "\n",
    "# ============================================================================\n",
    "# CACHING\n",
    "# ============================================================================\n",
    "\n",
    "def load_cache():\n",
    "    \"\"\"Load cached earnings data\"\"\"\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_cache(cache):\n",
    "    \"\"\"Save earnings data to cache\"\"\"\n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        json.dump(cache, f, indent=2, default=str)\n",
    "\n",
    "# ============================================================================\n",
    "# RATE LIMIT PERSISTENCE - NEW v2.1\n",
    "# ============================================================================\n",
    "\n",
    "def load_rate_limits():\n",
    "    \"\"\"\n",
    "    Load rate limit state with timestamps\n",
    "    Alpha Vantage: 25 calls per day, resets daily\n",
    "    \"\"\"\n",
    "    if os.path.exists(RATE_LIMIT_FILE):\n",
    "        try:\n",
    "            with open(RATE_LIMIT_FILE, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Reset limits older than 24 hours\n",
    "            now = datetime.now().timestamp()\n",
    "            active_limits = {}\n",
    "            \n",
    "            for key_idx, info in data.items():\n",
    "                reset_time = info.get('reset_time', 0)\n",
    "                if reset_time > now:\n",
    "                    active_limits[int(key_idx)] = info\n",
    "            \n",
    "            return set(active_limits.keys())\n",
    "        except:\n",
    "            return set()\n",
    "    return set()\n",
    "\n",
    "def save_rate_limits(rate_limited_keys):\n",
    "    \"\"\"\n",
    "    Persist rate limit state with 24-hour expiry\n",
    "    \"\"\"\n",
    "    reset_time = (datetime.now() + timedelta(hours=24)).timestamp()\n",
    "    \n",
    "    data = {\n",
    "        str(k): {\n",
    "            'reset_time': reset_time,\n",
    "            'limited_at': datetime.now().isoformat()\n",
    "        } \n",
    "        for k in rate_limited_keys\n",
    "    }\n",
    "    \n",
    "    with open(RATE_LIMIT_FILE, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "# ============================================================================\n",
    "# CURRENT IMPLIED VOLATILITY - NEW v2.2\n",
    "# ============================================================================\n",
    "\n",
    "def get_current_iv(ticker, dte_target=45, retry_count=2):\n",
    "    \"\"\"\n",
    "    Fetch current implied volatility from Yahoo Finance options chain\n",
    "    \n",
    "    Returns ATM IV for expiration closest to target DTE\n",
    "    Data is delayed 15-20 minutes (free tier)\n",
    "    \n",
    "    Returns None if:\n",
    "    - yfinance not installed\n",
    "    - No options available\n",
    "    - API error\n",
    "    \"\"\"\n",
    "    if not YFINANCE_AVAILABLE:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        \n",
    "        # Get current price\n",
    "        hist = stock.history(period='1d')\n",
    "        if hist.empty:\n",
    "            return None\n",
    "        current_price = hist['Close'].iloc[-1]\n",
    "        \n",
    "        # Get available expirations\n",
    "        expirations = stock.options\n",
    "        if not expirations:\n",
    "            return None\n",
    "        \n",
    "        # Find expiration closest to target DTE\n",
    "        today = datetime.now()\n",
    "        target_exp = None\n",
    "        min_diff = 999\n",
    "        \n",
    "        for exp_str in expirations:\n",
    "            exp_date = datetime.strptime(exp_str, '%Y-%m-%d')\n",
    "            dte = (exp_date - today).days\n",
    "            if abs(dte - dte_target) < min_diff:\n",
    "                min_diff = abs(dte - dte_target)\n",
    "                target_exp = exp_str\n",
    "        \n",
    "        if not target_exp:\n",
    "            return None\n",
    "        \n",
    "        # Get options chain\n",
    "        chain = stock.option_chain(target_exp)\n",
    "        calls = chain.calls\n",
    "        \n",
    "        if calls.empty or 'impliedVolatility' not in calls.columns:\n",
    "            return None\n",
    "        \n",
    "        # Find ATM strike (closest to current price)\n",
    "        calls['strike_diff'] = abs(calls['strike'] - current_price)\n",
    "        atm_idx = calls['strike_diff'].idxmin()\n",
    "        atm_call = calls.loc[atm_idx]\n",
    "        \n",
    "        iv_decimal = atm_call['impliedVolatility']\n",
    "        iv_pct = iv_decimal * 100\n",
    "        \n",
    "        actual_dte = (datetime.strptime(target_exp, '%Y-%m-%d') - today).days\n",
    "        \n",
    "        return {\n",
    "            'iv': round(iv_pct, 1),\n",
    "            'dte': actual_dte,\n",
    "            'strike': atm_call['strike'],\n",
    "            'expiration': target_exp\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        if retry_count > 0:\n",
    "            time.sleep(1)\n",
    "            return get_current_iv(ticker, dte_target, retry_count - 1)\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# ALPHA VANTAGE - EARNINGS DATA\n",
    "# ============================================================================\n",
    "\n",
    "def get_earnings_details(ticker, use_cache=True, debug=False):\n",
    "    \"\"\"Get earnings announcement dates from Alpha Vantage with automatic key rotation\"\"\"\n",
    "    global CURRENT_KEY_INDEX, RATE_LIMITED_KEYS\n",
    "    \n",
    "    # Check cache first\n",
    "    cache = load_cache()\n",
    "    if use_cache and ticker in cache:\n",
    "        return [\n",
    "            {'date': datetime.fromisoformat(e['date']), 'time': e['time']} \n",
    "            for e in cache[ticker]\n",
    "        ], \"cached\"\n",
    "    \n",
    "    # NEW v2.1: Load persisted rate limits at start\n",
    "    if not RATE_LIMITED_KEYS:  # Only load once per run\n",
    "        RATE_LIMITED_KEYS = load_rate_limits()\n",
    "        if RATE_LIMITED_KEYS and debug:\n",
    "            print(f\"  Loaded {len(RATE_LIMITED_KEYS)} rate-limited keys from previous run\")\n",
    "    \n",
    "    # Check if all keys exhausted\n",
    "    if len(RATE_LIMITED_KEYS) >= len(ALPHAVANTAGE_KEYS):\n",
    "        if debug:\n",
    "            print(f\"  {ticker}: All API keys exhausted\")\n",
    "        return [], \"rate_limited_all\"\n",
    "    \n",
    "    # Try available keys\n",
    "    max_attempts = len(ALPHAVANTAGE_KEYS) - len(RATE_LIMITED_KEYS)\n",
    "    for attempt in range(max_attempts):\n",
    "        # Skip rate-limited keys\n",
    "        while CURRENT_KEY_INDEX in RATE_LIMITED_KEYS:\n",
    "            CURRENT_KEY_INDEX = (CURRENT_KEY_INDEX + 1) % len(ALPHAVANTAGE_KEYS)\n",
    "        \n",
    "        current_key = ALPHAVANTAGE_KEYS[CURRENT_KEY_INDEX]\n",
    "        key_suffix = current_key[-4:]  # Last 4 chars for identification\n",
    "        url = f\"https://www.alphavantage.co/query?function=EARNINGS&symbol={ticker}&apikey={current_key}\"\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  {ticker}: Trying key ...{key_suffix} (attempt {attempt + 1}/{max_attempts})\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            # Handle API errors\n",
    "            error_msg = data.get('Note', data.get('Information', ''))\n",
    "            if error_msg:\n",
    "                if debug:\n",
    "                    print(f\"  {ticker}: Key ...{key_suffix} returned: {error_msg[:80]}\")\n",
    "                \n",
    "                # Check for rate limit\n",
    "                if any(phrase in error_msg.lower() for phrase in ['rate limit', 'call frequency']):\n",
    "                    RATE_LIMITED_KEYS.add(CURRENT_KEY_INDEX)\n",
    "                    save_rate_limits(RATE_LIMITED_KEYS)  # NEW v2.1: Persist immediately\n",
    "                    \n",
    "                    if debug:\n",
    "                        print(f\"  {ticker}: Key ...{key_suffix} marked as rate-limited ({len(RATE_LIMITED_KEYS)}/{len(ALPHAVANTAGE_KEYS)} exhausted)\")\n",
    "                        print(f\"  {ticker}: Rate limit saved - will skip this key for 24 hours\")\n",
    "                    \n",
    "                    CURRENT_KEY_INDEX = (CURRENT_KEY_INDEX + 1) % len(ALPHAVANTAGE_KEYS)\n",
    "                    \n",
    "                    if len(RATE_LIMITED_KEYS) >= len(ALPHAVANTAGE_KEYS):\n",
    "                        if debug:\n",
    "                            print(f\"  {ticker}: All keys exhausted!\")\n",
    "                        return [], \"rate_limited_all\"\n",
    "                    \n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                \n",
    "                return [], \"api_error\"\n",
    "            \n",
    "            # Parse earnings data\n",
    "            if 'quarterlyEarnings' not in data:\n",
    "                if debug:\n",
    "                    print(f\"  {ticker}: No 'quarterlyEarnings' field found\")\n",
    "                return [], \"no_earnings\"\n",
    "            \n",
    "            earnings_info = []\n",
    "            for quarter in data['quarterlyEarnings']:\n",
    "                reported_date = quarter.get('reportedDate')\n",
    "                # API changed field name from 'reportedTime' to 'reportTime'\n",
    "                reported_time = quarter.get('reportTime') or quarter.get('reportedTime', 'amc')\n",
    "                \n",
    "                if reported_date:\n",
    "                    # Normalize time values: 'pre-market' -> 'bmo', 'post-market' -> 'amc'\n",
    "                    time_map = {'pre-market': 'bmo', 'post-market': 'amc'}\n",
    "                    normalized_time = time_map.get(reported_time.lower(), reported_time.lower())\n",
    "                    \n",
    "                    earnings_info.append({\n",
    "                        'date': datetime.strptime(reported_date, '%Y-%m-%d'),\n",
    "                        'time': normalized_time\n",
    "                    })\n",
    "            \n",
    "            # Cache the results\n",
    "            cache[ticker] = [\n",
    "                {'date': e['date'].isoformat(), 'time': e['time']} \n",
    "                for e in earnings_info\n",
    "            ]\n",
    "            save_cache(cache)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"  {ticker}: âœ“ Success with key ...{key_suffix}\")\n",
    "            \n",
    "            return sorted(earnings_info, key=lambda x: x['date'], reverse=True), \"success\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"  {ticker}: Exception - {str(e)[:80]}\")\n",
    "            if attempt < max_attempts - 1:\n",
    "                CURRENT_KEY_INDEX = (CURRENT_KEY_INDEX + 1) % len(ALPHAVANTAGE_KEYS)\n",
    "                continue\n",
    "            return [], \"exception\"\n",
    "    \n",
    "    return [], \"unknown_error\"\n",
    "\n",
    "# ============================================================================\n",
    "# YAHOO FINANCE - PRICE DATA\n",
    "# ============================================================================\n",
    "\n",
    "def get_yahoo_price_data(ticker, start_date, end_date):\n",
    "    \"\"\"Get historical closing prices from Yahoo Finance\"\"\"\n",
    "    start_ts = int(start_date.timestamp())\n",
    "    end_ts = int(end_date.timestamp())\n",
    "    \n",
    "    url = f\"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}\"\n",
    "    params = {'period1': start_ts, 'period2': end_ts, 'interval': '1d'}\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers, timeout=10)\n",
    "        data = response.json()\n",
    "        \n",
    "        result = data['chart']['result'][0]\n",
    "        timestamps = result['timestamp']\n",
    "        closes = result['indicators']['quote'][0]['close']\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'date': [datetime.fromtimestamp(ts) for ts in timestamps],\n",
    "            'close': closes\n",
    "        })\n",
    "        df.set_index('date', inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching prices for {ticker}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def find_nearest_price(price_data, target_date):\n",
    "    \"\"\"Find closing price on nearest trading day\"\"\"\n",
    "    if price_data.empty:\n",
    "        return None, None\n",
    "    \n",
    "    start = target_date - timedelta(days=7)\n",
    "    end = target_date + timedelta(days=7)\n",
    "    nearby = price_data[(price_data.index >= start) & (price_data.index <= end)]\n",
    "    \n",
    "    if nearby.empty:\n",
    "        return None, None\n",
    "    \n",
    "    time_diffs = (nearby.index - target_date).to_series().abs()\n",
    "    closest_idx = time_diffs.argmin()\n",
    "    return nearby.iloc[closest_idx]['close'], nearby.index[closest_idx]\n",
    "\n",
    "def get_reference_price(price_data, earnings_date, timing):\n",
    "    \"\"\"Get entry price based on earnings timing\"\"\"\n",
    "    target_date = earnings_date - timedelta(days=1) if timing == 'bmo' else earnings_date\n",
    "    return find_nearest_price(price_data, target_date)\n",
    "\n",
    "def calculate_historical_volatility(price_data, earnings_date, lookback_days=30):\n",
    "    \"\"\"Calculate annualized historical volatility\"\"\"\n",
    "    end_date = earnings_date - timedelta(days=1)\n",
    "    start_date = end_date - timedelta(days=lookback_days + 10)\n",
    "    \n",
    "    window = price_data[(price_data.index >= start_date) & (price_data.index <= end_date)]\n",
    "    \n",
    "    if len(window) < 20:\n",
    "        return None\n",
    "    \n",
    "    returns = window['close'].pct_change().dropna()\n",
    "    daily_vol = returns.std()\n",
    "    annual_vol = daily_vol * np.sqrt(252)\n",
    "    \n",
    "    return annual_vol\n",
    "\n",
    "def get_volatility_tier(hvol):\n",
    "    \"\"\"Map historical volatility to strike width multiplier\"\"\"\n",
    "    hvol_pct = hvol * 100\n",
    "    \n",
    "    if hvol_pct < 25:\n",
    "        return 1.0\n",
    "    elif hvol_pct < 35:\n",
    "        return 1.2\n",
    "    elif hvol_pct < 45:\n",
    "        return 1.4\n",
    "    else:\n",
    "        return 1.5\n",
    "\n",
    "def calculate_stats(data):\n",
    "    \"\"\"\n",
    "    Calculate containment and directional statistics\n",
    "    \n",
    "    CHANGED v2: Added break_bias (directional bias among breaks only)\n",
    "                and magnitude-weighted metrics\n",
    "    \"\"\"\n",
    "    total = len(data)\n",
    "    moves = np.array([d['move'] for d in data])\n",
    "    widths = np.array([d['width'] for d in data])\n",
    "    \n",
    "    # Containment\n",
    "    stays_within = sum(1 for i, m in enumerate(moves) if abs(m) <= widths[i])\n",
    "    breaks_up = sum(1 for i, m in enumerate(moves) if m > widths[i])\n",
    "    breaks_down = sum(1 for i, m in enumerate(moves) if m < -widths[i])\n",
    "    \n",
    "    # EXISTING: Overall directional bias (% of all moves that went up)\n",
    "    up_moves = sum(1 for m in moves if m > 0)\n",
    "    overall_bias = (up_moves / total) * 100\n",
    "    \n",
    "    # NEW v2: Break directional bias (among breaks only, which direction?)\n",
    "    total_breaks = breaks_up + breaks_down\n",
    "    if total_breaks > 0:\n",
    "        break_bias = (breaks_up / total_breaks) * 100\n",
    "    else:\n",
    "        break_bias = 50.0  # Neutral if no breaks\n",
    "    \n",
    "    # NEW v2: Magnitude-weighted average move\n",
    "    avg_move = np.mean(moves)\n",
    "    avg_move_vs_width = (avg_move / np.mean(widths)) * 100 if np.mean(widths) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total': total,\n",
    "        'containment': (stays_within / total) * 100,\n",
    "        'breaks_up': breaks_up,\n",
    "        'breaks_down': breaks_down,\n",
    "        'overall_bias': overall_bias,      # % of all moves that were positive\n",
    "        'break_bias': break_bias,          # NEW: % of breaks that were upward\n",
    "        'avg_move_pct': avg_move,          # NEW: Average move magnitude\n",
    "        'drift_vs_width': avg_move_vs_width,  # NEW: Drift relative to strike width\n",
    "        'avg_width': np.mean(widths)\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_earnings_movement(ticker, lookback_quarters=24, verbose=True, debug=False):\n",
    "    \"\"\"Analyze post-earnings movements with volatility-adjusted strikes\"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*75}\")\n",
    "        print(f\"ðŸ“Š {ticker} - Post-Earnings Containment Analysis\")\n",
    "        print(f\"{'='*75}\")\n",
    "    \n",
    "    # Get earnings dates\n",
    "    earnings_info, status = get_earnings_details(ticker, debug=debug)\n",
    "    if not earnings_info:\n",
    "        return None, status\n",
    "    \n",
    "    # Filter to past earnings\n",
    "    today = datetime.now()\n",
    "    past_earnings = [e for e in earnings_info if e['date'] < today][:lookback_quarters]\n",
    "    \n",
    "    if len(past_earnings) < 10:\n",
    "        if verbose:\n",
    "            print(f\"âš ï¸  Insufficient data: only {len(past_earnings)} earnings periods\")\n",
    "        return None, \"insufficient_quarters\"\n",
    "    \n",
    "    # Get price data\n",
    "    oldest = min([e['date'] for e in past_earnings]) - timedelta(days=120)\n",
    "    price_data = get_yahoo_price_data(ticker, oldest, today)\n",
    "    \n",
    "    if price_data.empty:\n",
    "        return None, \"no_price_data\"\n",
    "    \n",
    "    # Collect movement data\n",
    "    data_45 = []\n",
    "    data_90 = []\n",
    "    hvol_list = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nAnalyzing {len(past_earnings)} earnings periods...\")\n",
    "    \n",
    "    for earnings in past_earnings:\n",
    "        # Calculate historical volatility\n",
    "        hvol = calculate_historical_volatility(price_data, earnings['date'])\n",
    "        if hvol is None:\n",
    "            continue\n",
    "        \n",
    "        hvol_list.append(hvol * 100)\n",
    "        strike_std = get_volatility_tier(hvol)\n",
    "        \n",
    "        # Get entry price\n",
    "        ref_price, ref_date = get_reference_price(price_data, earnings['date'], earnings['time'])\n",
    "        if ref_price is None:\n",
    "            continue\n",
    "        \n",
    "        # Calculate strike widths\n",
    "        dte_45_factor = np.sqrt(45 / 365)\n",
    "        dte_90_factor = np.sqrt(90 / 365)\n",
    "        strike_width_45 = hvol * dte_45_factor * strike_std * 100\n",
    "        strike_width_90 = hvol * dte_90_factor * strike_std * 100\n",
    "        \n",
    "        # Test 45-day outcome\n",
    "        target_45 = earnings['date'] + timedelta(days=45)\n",
    "        if target_45 <= today:\n",
    "            price_45, date_45 = find_nearest_price(price_data, target_45)\n",
    "            if price_45 is not None:\n",
    "                move_45 = (price_45 - ref_price) / ref_price * 100\n",
    "                data_45.append({\n",
    "                    'move': move_45,\n",
    "                    'width': strike_width_45,\n",
    "                    'hvol': hvol * 100,\n",
    "                    'date': earnings['date'].strftime('%Y-%m-%d')\n",
    "                })\n",
    "        \n",
    "        # Test 90-day outcome\n",
    "        target_90 = earnings['date'] + timedelta(days=90)\n",
    "        if target_90 <= today:\n",
    "            price_90, date_90 = find_nearest_price(price_data, target_90)\n",
    "            if price_90 is not None:\n",
    "                move_90 = (price_90 - ref_price) / ref_price * 100\n",
    "                data_90.append({\n",
    "                    'move': move_90,\n",
    "                    'width': strike_width_90,\n",
    "                    'hvol': hvol * 100,\n",
    "                    'date': earnings['date'].strftime('%Y-%m-%d')\n",
    "                })\n",
    "    \n",
    "    if len(data_45) < 10 or len(data_90) < 10:\n",
    "        if verbose:\n",
    "            print(f\"âš ï¸  Insufficient valid data\")\n",
    "        return None, \"insufficient_valid_data\"\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats_45 = calculate_stats(data_45)\n",
    "    stats_90 = calculate_stats(data_90)\n",
    "    avg_hvol = np.mean(hvol_list)\n",
    "    avg_tier = get_volatility_tier(avg_hvol / 100)\n",
    "    \n",
    "    # CHANGED v2.1: Simplified logic - Containment vs Directional Bias vs Skip\n",
    "    rec_parts = []\n",
    "    bias_reasons = []\n",
    "    \n",
    "    # DEBUG: Print stats before logic\n",
    "    if verbose:\n",
    "        print(f\"\\n  ðŸ” DEBUG - Recommendation Logic:\")\n",
    "        print(f\"     90D containment: {stats_90['containment']:.2f}%\")\n",
    "        print(f\"     90D overall_bias: {stats_90['overall_bias']:.2f}%\")\n",
    "        print(f\"     90D breaks: {stats_90['breaks_up']} up, {stats_90['breaks_down']} down\")\n",
    "        print(f\"     90D drift: {stats_90['avg_move_pct']:+.2f}%\")\n",
    "        print(f\"     45D containment: {stats_45['containment']:.2f}%\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CATEGORY 1: CONTAINMENT STRATEGIES (IC)\n",
    "    # ========================================================================\n",
    "    if stats_90['containment'] >= 69.5:\n",
    "        break_ratio = max(stats_90['breaks_up'], stats_90['breaks_down']) / (\n",
    "            min(stats_90['breaks_up'], stats_90['breaks_down']) + 1\n",
    "        )\n",
    "        \n",
    "        if break_ratio < 2:  # Breaks are balanced\n",
    "            rec_parts.append(\"IC90\")\n",
    "            if verbose:\n",
    "                print(f\"     âœ“ IC90: Balanced breaks (ratio={break_ratio:.2f})\")\n",
    "        elif stats_90['break_bias'] >= 70:  # Most breaks are upward\n",
    "            rec_parts.append(\"IC90âš â†‘\")\n",
    "            if verbose:\n",
    "                print(f\"     âœ“ IC90âš â†‘: Watch upside ({stats_90['break_bias']:.0f}% breaks upward)\")\n",
    "        else:  # Most breaks are downward\n",
    "            rec_parts.append(\"IC90âš â†“\")\n",
    "            if verbose:\n",
    "                print(f\"     âœ“ IC90âš â†“: Watch downside ({100-stats_90['break_bias']:.0f}% breaks downward)\")\n",
    "            \n",
    "    elif stats_45['containment'] >= 69.5:\n",
    "        if verbose:\n",
    "            print(f\"     â†’ 90D didn't qualify, checking 45D...\")\n",
    "        \n",
    "        break_ratio = max(stats_45['breaks_up'], stats_45['breaks_down']) / (\n",
    "            min(stats_45['breaks_up'], stats_45['breaks_down']) + 1\n",
    "        )\n",
    "        \n",
    "        if break_ratio < 2:\n",
    "            rec_parts.append(\"IC45\")\n",
    "            if verbose:\n",
    "                print(f\"     âœ“ IC45: Balanced breaks (ratio={break_ratio:.2f})\")\n",
    "        elif stats_45['break_bias'] >= 70:\n",
    "            rec_parts.append(\"IC45âš â†‘\")\n",
    "            if verbose:\n",
    "                print(f\"     âœ“ IC45âš â†‘: Watch upside ({stats_45['break_bias']:.0f}% breaks upward)\")\n",
    "        else:\n",
    "            rec_parts.append(\"IC45âš â†“\")\n",
    "            if verbose:\n",
    "                print(f\"     âœ“ IC45âš â†“: Watch downside ({100-stats_45['break_bias']:.0f}% breaks downward)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CATEGORY 2: DIRECTIONAL BIAS (No containment edge, but directional signal)\n",
    "    # ========================================================================\n",
    "    else:\n",
    "        # Check for UPWARD edge (any of these conditions)\n",
    "        has_upward_edge = False\n",
    "        \n",
    "        if stats_90['overall_bias'] >= 65:\n",
    "            has_upward_edge = True\n",
    "            bias_reasons.append(f\"{stats_90['overall_bias']:.0f}% bias\")\n",
    "        \n",
    "        if stats_90['breaks_up'] >= stats_90['breaks_down'] * 1.5 and stats_90['breaks_up'] >= 2:\n",
    "            has_upward_edge = True\n",
    "            bias_reasons.append(f\"{stats_90['breaks_up']}:{stats_90['breaks_down']}â†‘ breaks\")\n",
    "        \n",
    "        if stats_90['avg_move_pct'] >= 3.0:\n",
    "            has_upward_edge = True\n",
    "            bias_reasons.append(f\"{stats_90['avg_move_pct']:+.1f}% drift\")\n",
    "        \n",
    "        # Check for DOWNWARD edge\n",
    "        has_downward_edge = False\n",
    "        \n",
    "        if stats_90['overall_bias'] <= 35:\n",
    "            has_downward_edge = True\n",
    "            bias_reasons.append(f\"{stats_90['overall_bias']:.0f}% bias\")\n",
    "        \n",
    "        if stats_90['breaks_down'] >= stats_90['breaks_up'] * 1.5 and stats_90['breaks_down'] >= 2:\n",
    "            has_downward_edge = True\n",
    "            bias_reasons.append(f\"{stats_90['breaks_up']}:{stats_90['breaks_down']}â†“ breaks\")\n",
    "        \n",
    "        if stats_90['avg_move_pct'] <= -3.0:\n",
    "            has_downward_edge = True\n",
    "            bias_reasons.append(f\"{stats_90['avg_move_pct']:+.1f}% drift\")\n",
    "        \n",
    "        # Assign directional recommendation\n",
    "        if has_upward_edge:\n",
    "            reason_str = \", \".join(bias_reasons)\n",
    "            rec_parts.append(f\"BIASâ†‘ ({reason_str})\")\n",
    "            if verbose:\n",
    "                print(f\"     âœ“ Upward edge detected: {reason_str}\")\n",
    "        elif has_downward_edge:\n",
    "            reason_str = \", \".join(bias_reasons)\n",
    "            rec_parts.append(f\"BIASâ†“ ({reason_str})\")\n",
    "            if verbose:\n",
    "                print(f\"     âœ“ Downward edge detected: {reason_str}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FINAL RECOMMENDATION\n",
    "    # ========================================================================\n",
    "    recommendation = rec_parts[0] if rec_parts else \"SKIP\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"     Final recommendation: {recommendation}\")\n",
    "    \n",
    "    # Print results\n",
    "    if verbose:\n",
    "        print(f\"\\nðŸ“Š {ticker} | {avg_hvol:.1f}% HVol | {avg_tier:.1f} std (Â±{stats_90['avg_width']:.1f}%)\")\n",
    "        print(f\"\\n  45-Day: {stats_45['total']}/{lookback_quarters} tested\")\n",
    "        print(f\"    Containment: {stats_45['containment']:.0f}%\")\n",
    "        print(f\"    Breaks: Up {stats_45['breaks_up']}, Down {stats_45['breaks_down']}\")\n",
    "        print(f\"    Overall Bias: {stats_45['overall_bias']:.0f}% up\")\n",
    "        print(f\"    Break Bias: {stats_45['break_bias']:.0f}% of breaks were upward\")\n",
    "        print(f\"    Avg Drift: {stats_45['avg_move_pct']:+.1f}% ({stats_45['drift_vs_width']:+.0f}% of width)\")\n",
    "        \n",
    "        print(f\"\\n  90-Day: {stats_90['total']}/{lookback_quarters} tested\")\n",
    "        print(f\"    Containment: {stats_90['containment']:.0f}%\")\n",
    "        print(f\"    Breaks: Up {stats_90['breaks_up']}, Down {stats_90['breaks_down']}\")\n",
    "        print(f\"    Overall Bias: {stats_90['overall_bias']:.0f}% up\")\n",
    "        print(f\"    Break Bias: {stats_90['break_bias']:.0f}% of breaks were upward\")\n",
    "        print(f\"    Avg Drift: {stats_90['avg_move_pct']:+.1f}% ({stats_90['drift_vs_width']:+.0f}% of width)\")\n",
    "        \n",
    "        print(f\"\\n  ðŸ’¡ Strategy: {recommendation}\")\n",
    "    \n",
    "    summary = {\n",
    "        'ticker': ticker,\n",
    "        'hvol': round(avg_hvol, 1),\n",
    "        'tier': round(avg_tier, 1),\n",
    "        'strike_width': round(stats_90['avg_width'], 1),\n",
    "        '45d_contain': round(stats_45['containment'], 0),\n",
    "        '45d_breaks_up': stats_45['breaks_up'],\n",
    "        '45d_breaks_dn': stats_45['breaks_down'],\n",
    "        '45d_overall_bias': round(stats_45['overall_bias'], 0),\n",
    "        '45d_break_bias': round(stats_45['break_bias'], 0),\n",
    "        '45d_drift': round(stats_45['avg_move_pct'], 1),\n",
    "        '90d_contain': round(stats_90['containment'], 0),\n",
    "        '90d_breaks_up': stats_90['breaks_up'],\n",
    "        '90d_breaks_dn': stats_90['breaks_down'],\n",
    "        '90d_overall_bias': round(stats_90['overall_bias'], 0),\n",
    "        '90d_break_bias': round(stats_90['break_bias'], 0),\n",
    "        '90d_drift': round(stats_90['avg_move_pct'], 1),\n",
    "        'strategy': recommendation\n",
    "    }\n",
    "    \n",
    "    return summary, \"success\"\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def format_break_ratio(up_breaks, down_breaks, break_bias):\n",
    "    \"\"\"\n",
    "    Format break ratio with directional arrow if 2:1 edge exists\n",
    "    \n",
    "    CHANGED v2: Now uses break_bias to determine arrow direction\n",
    "    \"\"\"\n",
    "    if up_breaks == 0 and down_breaks == 0:\n",
    "        return \"0:0\"\n",
    "    \n",
    "    # Use break_bias for more accurate directional assessment\n",
    "    if break_bias >= 66.7:  # 2:1 or better upward\n",
    "        return f\"{up_breaks}:{down_breaks}â†‘\"\n",
    "    elif break_bias <= 33.3:  # 2:1 or better downward\n",
    "        return f\"{up_breaks}:{down_breaks}â†“\"\n",
    "    else:\n",
    "        return f\"{up_breaks}:{down_breaks}\"\n",
    "\n",
    "def batch_analyze(tickers, lookback_quarters=24, debug=False, fetch_iv=True):\n",
    "    \"\"\"Analyze multiple tickers with progress tracking\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*75)\n",
    "    print(f\"EARNINGS CONTAINMENT ANALYZER - v2.2\")\n",
    "    print(f\"Lookback: {lookback_quarters} quarters (~{lookback_quarters/4:.0f} years)\")\n",
    "    print(f\"Volatility Tiers: <25%=1.0std | 25-35%=1.2std | 35-45%=1.4std | >45%=1.5std\")\n",
    "    print(f\"Logic: Containment (IC) vs Directional Bias vs Skip\")\n",
    "    if fetch_iv and YFINANCE_AVAILABLE:\n",
    "        print(f\"NEW v2.2: Current IV from Yahoo Finance (15-20min delayed)\")\n",
    "    print(\"=\"*75)\n",
    "    \n",
    "    # NEW v2.1: Show rate limit status at start\n",
    "    rate_limited_keys = load_rate_limits()\n",
    "    if rate_limited_keys:\n",
    "        available = len(ALPHAVANTAGE_KEYS) - len(rate_limited_keys)\n",
    "        print(f\"\\nâš ï¸  Rate Limit Status: {available}/{len(ALPHAVANTAGE_KEYS)} API keys available\")\n",
    "        print(f\"   {len(rate_limited_keys)} key(s) exhausted, will reset in ~24hrs\")\n",
    "    else:\n",
    "        print(f\"\\nâœ“ All {len(ALPHAVANTAGE_KEYS)} API keys available\")\n",
    "    \n",
    "    results = []\n",
    "    fetch_summary = {'cached': [], 'api': [], 'failed': []}\n",
    "    iv_summary = {'success': [], 'failed': []}\n",
    "    \n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        print(f\"\\r[{i}/{len(tickers)}] Processing {ticker}...\", end='', flush=True)\n",
    "        \n",
    "        cache = load_cache()\n",
    "        from_cache = ticker in cache\n",
    "        \n",
    "        summary, status = analyze_earnings_movement(ticker, lookback_quarters, verbose=False, debug=debug)\n",
    "        \n",
    "        if summary:\n",
    "            # NEW v2.2: Fetch current IV\n",
    "            if fetch_iv and YFINANCE_AVAILABLE:\n",
    "                iv_data = get_current_iv(ticker, dte_target=45)\n",
    "                if iv_data:\n",
    "                    summary['current_iv'] = iv_data['iv']\n",
    "                    summary['iv_dte'] = iv_data['dte']\n",
    "                    # Calculate IV premium vs historical volatility\n",
    "                    iv_premium = ((iv_data['iv'] - summary['hvol']) / summary['hvol']) * 100\n",
    "                    summary['iv_premium'] = round(iv_premium, 1)\n",
    "                    iv_summary['success'].append(ticker)\n",
    "                else:\n",
    "                    summary['current_iv'] = None\n",
    "                    summary['iv_dte'] = None\n",
    "                    summary['iv_premium'] = None\n",
    "                    iv_summary['failed'].append(ticker)\n",
    "            else:\n",
    "                summary['current_iv'] = None\n",
    "                summary['iv_dte'] = None\n",
    "                summary['iv_premium'] = None\n",
    "            \n",
    "            results.append(summary)\n",
    "            if from_cache:\n",
    "                fetch_summary['cached'].append(ticker)\n",
    "            else:\n",
    "                fetch_summary['api'].append(ticker)\n",
    "        else:\n",
    "            fetch_summary['failed'].append(ticker)\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(\"\\r\" + \" \" * 80 + \"\\r\", end='')\n",
    "    \n",
    "    # Print fetch summary\n",
    "    print(f\"\\nðŸ“Š FETCH SUMMARY\")\n",
    "    print(f\"{'='*75}\")\n",
    "    if fetch_summary['cached']:\n",
    "        print(f\"âœ“ Earnings From Cache ({len(fetch_summary['cached'])}): {', '.join(fetch_summary['cached'][:5])}{'...' if len(fetch_summary['cached']) > 5 else ''}\")\n",
    "    if fetch_summary['api']:\n",
    "        print(f\"âœ“ Earnings From API ({len(fetch_summary['api'])}): {', '.join(fetch_summary['api'])}\")\n",
    "    if fetch_iv and YFINANCE_AVAILABLE:\n",
    "        if iv_summary['success']:\n",
    "            print(f\"âœ“ IV Data Retrieved ({len(iv_summary['success'])}): {', '.join(iv_summary['success'][:5])}{'...' if len(iv_summary['success']) > 5 else ''}\")\n",
    "        if iv_summary['failed']:\n",
    "            print(f\"âœ— IV Data Failed ({len(iv_summary['failed'])}): {', '.join(iv_summary['failed'])}\")\n",
    "    if fetch_summary['failed']:\n",
    "        print(f\"âœ— Analysis Failed ({len(fetch_summary['failed'])}): {', '.join(fetch_summary['failed'])}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"\\nâš ï¸  No valid results\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate 45-day strike width\n",
    "    df['45d_width'] = df.apply(lambda x: round(x['strike_width'] * np.sqrt(45/90), 1), axis=1)\n",
    "    \n",
    "    # Format break ratios using new break_bias\n",
    "    df['45_break_fmt'] = df.apply(\n",
    "        lambda x: format_break_ratio(x['45d_breaks_up'], x['45d_breaks_dn'], x['45d_break_bias']), \n",
    "        axis=1\n",
    "    )\n",
    "    df['90_break_fmt'] = df.apply(\n",
    "        lambda x: format_break_ratio(x['90d_breaks_up'], x['90d_breaks_dn'], x['90d_break_bias']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Format strategies - now includes reason strings\n",
    "    df['strategy_fmt'] = df['strategy'].apply(lambda x: x if len(x) <= 20 else x.split('(')[0].strip())\n",
    "    \n",
    "    # For display, shorten BIAS reasons\n",
    "    df['strategy_display'] = df['strategy'].apply(lambda x: \n",
    "        x.replace('BIASâ†‘', 'BIASâ†‘').replace('BIASâ†“', 'BIASâ†“') if 'BIAS' in x else x\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n{'='*130}\")\n",
    "    print(\"EARNINGS BACKTEST RESULTS (v2.2 - with Current IV Context)\")\n",
    "    print(\"=\"*130)\n",
    "    print(\"Historical: HVol = past volatility | Contain% = stayed within strikes | Bias = % up moves | Drift = avg move\")\n",
    "    print(\"Current: CurIV = current implied vol | IVPrem = IV vs historical (+ means elevated)\")\n",
    "    print(\"Note: This shows what happened historically. Current IV context helps assess if premium is attractive NOW.\")\n",
    "    print(\"=\"*130)\n",
    "    \n",
    "    # Prepare display columns\n",
    "    display_cols = {\n",
    "        'Ticker': df['ticker'],\n",
    "        'HVol%': df['hvol'].astype(int),\n",
    "    }\n",
    "    \n",
    "    # Add IV columns if available\n",
    "    if 'current_iv' in df.columns and df['current_iv'].notna().any():\n",
    "        display_cols['CurIV%'] = df['current_iv'].apply(lambda x: f\"{int(x)}\" if pd.notna(x) else \"N/A\")\n",
    "        display_cols['IVPrem'] = df['iv_premium'].apply(lambda x: f\"{x:+.0f}%\" if pd.notna(x) else \"N/A\")\n",
    "        display_cols['|'] = '|'\n",
    "    \n",
    "    display_cols.update({\n",
    "        '90D%': df['90d_contain'].astype(int),\n",
    "        '90Bias': df['90d_overall_bias'].astype(int),\n",
    "        '90Break': df['90_break_fmt'],\n",
    "        '90Drift': df['90d_drift'].apply(lambda x: f\"{x:+.1f}%\"),\n",
    "        ' | ': '|',\n",
    "        'Pattern': df['strategy_display']\n",
    "    })\n",
    "    \n",
    "    display_df = pd.DataFrame(display_cols)\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Summary insights\n",
    "    print(f\"\\n{'='*130}\")\n",
    "    print(\"KEY TAKEAWAYS:\")\n",
    "    print(\"=\"*130)\n",
    "    \n",
    "    # Simplified pattern counts\n",
    "    ic_count = len(df[df['strategy_fmt'].str.contains('IC', na=False)])\n",
    "    bias_up_count = len(df[df['strategy_fmt'].str.contains('BIASâ†‘', na=False)])\n",
    "    bias_down_count = len(df[df['strategy_fmt'].str.contains('BIASâ†“', na=False)])\n",
    "    skip_count = len(df[df['strategy_fmt'] == 'SKIP'])\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Pattern Summary: {ic_count} IC candidates | {bias_up_count} Upward bias | {bias_down_count} Downward bias | {skip_count} No edge\")\n",
    "    \n",
    "    # IV Context (if available) - CONDENSED\n",
    "    if 'iv_premium' in df.columns and df['iv_premium'].notna().any():\n",
    "        elevated = df[df['iv_premium'] >= 15].sort_values('iv_premium', ascending=False)\n",
    "        depressed = df[df['iv_premium'] <= -15].sort_values('iv_premium')\n",
    "        \n",
    "        print(f\"\\nðŸ’° IV Landscape:\")\n",
    "        if not elevated.empty:\n",
    "            tickers_str = ', '.join([f\"{row['ticker']}(+{row['iv_premium']:.0f}%)\" for _, row in elevated.head(5).iterrows()])\n",
    "            print(f\"  Rich Premium (IV elevated â‰¥15%): {tickers_str}\")\n",
    "        if not depressed.empty:\n",
    "            tickers_str = ', '.join([f\"{row['ticker']}({row['iv_premium']:.0f}%)\" for _, row in depressed.head(3).iterrows()])\n",
    "            print(f\"  Thin Premium (IV depressed â‰¤-15%): {tickers_str}\")\n",
    "        \n",
    "        normal_count = len(df[(df['iv_premium'] > -15) & (df['iv_premium'] < 15)])\n",
    "        print(f\"  Normal Range: {normal_count} tickers\")\n",
    "    \n",
    "    # Asymmetric ICs - CONDENSED\n",
    "    ic_up_skew = df[(df['strategy_fmt'].str.contains('IC.*âš â†‘', regex=True, na=False))]\n",
    "    ic_down_skew = df[(df['strategy_fmt'].str.contains('IC.*âš â†“', regex=True, na=False))]\n",
    "    \n",
    "    if not ic_up_skew.empty or not ic_down_skew.empty:\n",
    "        print(f\"\\nâš ï¸  Asymmetric ICs:\")\n",
    "        if not ic_up_skew.empty:\n",
    "            print(f\"  Upside risk: {', '.join(ic_up_skew['ticker'].tolist())}\")\n",
    "        if not ic_down_skew.empty:\n",
    "            print(f\"  Downside risk: {', '.join(ic_down_skew['ticker'].tolist())}\")\n",
    "    \n",
    "    # Strong directional edges - CONDENSED\n",
    "    strong_bias = df[\n",
    "        (df['strategy_fmt'].str.contains('BIAS', na=False)) &\n",
    "        ((df['90d_overall_bias'] >= 70) | (df['90d_overall_bias'] <= 30))\n",
    "    ]\n",
    "    if not strong_bias.empty:\n",
    "        print(f\"\\nðŸ“ˆ Strong Directional Signals:\")\n",
    "        for _, row in strong_bias.iterrows():\n",
    "            direction = \"â†‘\" if row['90d_overall_bias'] >= 70 else \"â†“\"\n",
    "            print(f\"  {row['ticker']}: {row['90d_overall_bias']:.0f}% bias {direction}, {row['90_break_fmt']} breaks, {row['90d_drift']:+.1f}% drift\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Remember: Past patterns â‰  Future results. IV context shows current opportunity cost.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# RUN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test tickers\n",
    "    tickers = [\"DAL\", \"PEP\", \"FAST\", \"BLK\", \"C\", \"DPZ\", \"GS\", \"JNJ\", \"JPM\", \"WFC\", \n",
    "               \"OMC\", \"ABT\", \"BAC\", \"CFG\", \"MS\", \"PGR\", \"PLD\", \"PNC\", \"SYF\", \"JBHT\", \n",
    "               \"UAL\", \"BK\", \"KEY\", \"MMC\", \"MTB\", \"SCHW\", \"SNA\", \"TRV\", \"USB\", \"CSX\", \n",
    "               \"AXP\", \"FITB\", \"HBAN\", \"RF\", \"SLB\", \"STT\", \"TFC\", \"STLD\"]\n",
    "    \n",
    "    results = batch_analyze(tickers, lookback_quarters=24, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9869647-7936-40c6-954a-060789e3c6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0759ce9b-cb8b-4ddb-abe2-ac6e962a87d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      "EARNINGS CONTAINMENT ANALYZER\n",
      "Lookback: 24 quarters (~6 years)\n",
      "Volatility Tiers: <25%=1.0std | 25-35%=1.2std | 35-45%=1.4std | >45%=1.5std\n",
      "===========================================================================\n",
      "⏳ Fetching earnings from Alpha Vantage for DAL...\n",
      "✓ Cached 118 earnings dates for DAL\n",
      "⏳ Fetching earnings from Alpha Vantage for PEPE...\n",
      "⚠️  No earnings data for PEPE\n",
      "⏳ Fetching earnings from Alpha Vantage for FAST...\n",
      "✓ Cached 118 earnings dates for FAST\n",
      "⏳ Fetching earnings from Alpha Vantage for BLK...\n",
      "✓ Cached 103 earnings dates for BLK\n",
      "⏳ Fetching earnings from Alpha Vantage for C...\n",
      "✓ Cached 118 earnings dates for C\n",
      "⏳ Fetching earnings from Alpha Vantage for DPZ...\n",
      "✓ Cached 85 earnings dates for DPZ\n",
      "⏳ Fetching earnings from Alpha Vantage for GS...\n",
      "✓ Cached 105 earnings dates for GS\n",
      "⏳ Fetching earnings from Alpha Vantage for JNJ...\n",
      "✓ Cached 118 earnings dates for JNJ\n",
      "⏳ Fetching earnings from Alpha Vantage for JPM...\n",
      "✓ Cached 118 earnings dates for JPM\n",
      "⏳ Fetching earnings from Alpha Vantage for WBA...\n",
      "⚠️  No earnings data for WBA\n",
      "⏳ Fetching earnings from Alpha Vantage for WFC...\n",
      "✓ Cached 118 earnings dates for WFC\n",
      "⏳ Fetching earnings from Alpha Vantage for OMC...\n",
      "✓ Cached 118 earnings dates for OMC\n",
      "⏳ Fetching earnings from Alpha Vantage for ABT...\n",
      "✓ Cached 118 earnings dates for ABT\n",
      "⏳ Fetching earnings from Alpha Vantage for BAC...\n",
      "✓ Cached 118 earnings dates for BAC\n",
      "⏳ Fetching earnings from Alpha Vantage for CFG...\n",
      "✓ Cached 44 earnings dates for CFG\n",
      "⏳ Fetching earnings from Alpha Vantage for MS...\n",
      "✓ Cached 117 earnings dates for MS\n",
      "⏳ Fetching earnings from Alpha Vantage for PGR...\n",
      "✓ Cached 115 earnings dates for PGR\n",
      "⏳ Fetching earnings from Alpha Vantage for PLD...\n",
      "✓ Cached 110 earnings dates for PLD\n",
      "⏳ Fetching earnings from Alpha Vantage for PNC...\n",
      "✓ Cached 118 earnings dates for PNC\n",
      "⏳ Fetching earnings from Alpha Vantage for SYF...\n",
      "✓ Cached 45 earnings dates for SYF\n",
      "⏳ Fetching earnings from Alpha Vantage for JBHT...\n",
      "✓ Cached 118 earnings dates for JBHT\n",
      "⏳ Fetching earnings from Alpha Vantage for UAL...\n",
      "✓ Cached 118 earnings dates for UAL\n",
      "⏳ Fetching earnings from Alpha Vantage for BK...\n",
      "✓ Cached 118 earnings dates for BK\n",
      "⏳ Fetching earnings from Alpha Vantage for KEY...\n",
      "✓ Cached 118 earnings dates for KEY\n",
      "⏳ Fetching earnings from Alpha Vantage for MMC...\n",
      "✓ Cached 118 earnings dates for MMC\n",
      "⏳ Fetching earnings from Alpha Vantage for MTB...\n",
      "⚠️  API limit: We have detected your API key as HPCFVLGHWHQU0QTY and our standard API rate limi...\n",
      "⏳ Fetching earnings from Alpha Vantage for SCHW...\n",
      "⚠️  API limit: We have detected your API key as HPCFVLGHWHQU0QTY and our standard API rate limi...\n",
      "⏳ Fetching earnings from Alpha Vantage for SNA...\n",
      "⚠️  API limit: We have detected your API key as HPCFVLGHWHQU0QTY and our standard API rate limi...\n",
      "⏳ Fetching earnings from Alpha Vantage for TRV...\n",
      "⚠️  API limit: We have detected your API key as HPCFVLGHWHQU0QTY and our standard API rate limi...\n",
      "⏳ Fetching earnings from Alpha Vantage for USB...\n",
      "⚠️  API limit: We have detected your API key as HPCFVLGHWHQU0QTY and our standard API rate limi...\n",
      "⏳ Fetching earnings from Alpha Vantage for CSX...\n",
      "⚠️  API limit: We have detected your API key as HPCFVLGHWHQU0QTY and our standard API rate limi...\n",
      "⏳ Fetching earnings from Alpha Vantage for AXP...\n",
      "⚠️  API limit: We have detected your API key as HPCFVLGHWHQU0QTY and our standard API rate limi...\n",
      "⏳ Fetching earnings from Alpha Vantage for FITB...\n",
      "⚠️  API limit: We have detected your API key as HPCFVLGHWHQU0QTY and our standard API rate limi...\n",
      "⏳ Fetching earnings from Alpha Vantage for HBAN...\n",
      "⚠️  API limit: We have detected your API key as HPCFVLGHWHQU0QTY and our standard API rate limi...\n",
      "⏳ Fetching earnings from Alpha Vantage for RF...\n",
      "⚠️  API limit: We have detected your API key as HPCFVLGHWHQU0QTY and our standard API rate limi...\n",
      "⏳ Fetching earnings from Alpha Vantage for SLB...\n",
      "⚠️  API limit: We have detected your API key as HPCFVLGHWHQU0QTY and our standard API rate limi...\n",
      "⏳ Fetching earnings from Alpha Vantage for STT...\n",
      "⚠️  API limit: We have detected your API key as HPCFVLGHWHQU0QTY and our standard API rate limi...\n",
      "⏳ Fetching earnings from Alpha Vantage for TFC...\n",
      "⚠️  API limit: We have detected your API key as HPCFVLGHWHQU0QTY and our standard API rate limi...\n",
      "\n",
      "====================================================================================================\n",
      "TIMEFRAME COMPARISON\n",
      "====================================================================================================\n",
      "Ticker  HVol%  Tier    45D%  45Bias 45Break 45Width     90D%  90Bias 90Break 90Width     Strategy\n",
      "   DAL     43   1.4 |    75      71    4:2↑   20.7%  |    67      67     5:3   29.3%   |     IC45\n",
      "  FAST     24   1.0 |    67      71    6:2↑   10.3%  |    78      61     3:2   14.6%   |     IC90\n",
      "   BLK     28   1.2 |    62      67    6:3↑   12.2%  |    61      57     5:4   17.3%   |     SKIP\n",
      "     C     34   1.2 |    75      71     3:3   15.7%  |    70      48     4:3   22.2%   |     IC45\n",
      "   DPZ     27   1.2 |    62      67    6:3↑   11.8%  |    65      61     5:3   16.7%   |     SKIP\n",
      "    GS     32   1.2 |    71      75    6:1↑   14.6%  |    70      61    5:2↑   20.6%   |     IC45\n",
      "   JNJ     17   1.0 |    62      50     4:5    7.0%  |    78      52     2:3    9.9%   |     IC90\n",
      "   JPM     28   1.2 |    58      79    8:2↑   12.4%  |    61      70    6:3↑   17.5%   |     SKIP\n",
      "   WFC     35   1.4 |    71      79    6:1↑   16.6%  |    65      65     5:3   23.5%   |     IC45\n",
      "   OMC     26   1.2 |    67      54     4:4   10.5%  |    65      61     3:5   14.9%   |     SKIP\n",
      "   ABT     22   1.0 |    46      58     8:5    9.0%  |    78      57    4:1↑   12.7%   |     IC90\n",
      "   BAC     32   1.2 |    58      67    7:3↑   14.8%  |    61      61     5:4   21.0%   |     SKIP\n",
      "   CFG     40   1.4 |    92      62     1:1   19.6%  |    74      65     3:3   27.7%   |     IC90\n",
      "    MS     33   1.2 |    62      67    7:2↑   15.1%  |    61      70    6:3↑   21.3%   |     SKIP\n",
      "   PGR     24   1.0 |    71      71    6:1↑   10.0%  |    70      70    7:0↑   14.2%   |     IC45\n",
      "   PLD     29   1.2 |    71      75     4:3   12.9%  |    61      61    6:3↑   18.3%   |     IC45\n",
      "   PNC     32   1.2 |    71      75    5:2↑   14.4%  |    61      70     5:4   20.4%   |     IC45\n",
      "   SYF     41   1.4 |    50      62     7:5   19.9%  |    70      65     3:4   28.2%   |     SKIP\n",
      "  JBHT     29   1.2 |    83      67    1:3↓   12.4%  |    74      52    2:4↓   17.5%   |     IC90\n",
      "   UAL     51   1.5 |    79      62     3:2   25.5%  |    70      57     4:3   36.1%   |     IC45\n",
      "    BK     29   1.2 |    62      79    7:2↑   13.1%  |    74      65    4:2↑   18.5%   |     IC90\n",
      "   KEY     42   1.4 |    79      62    4:1↑   20.6%  |    78      61     2:3   29.1%   |     IC90\n",
      "   MMC     18   1.0 |    54      79    8:3↑    7.4%  |    65      70    6:2↑   10.5%   |     SKIP\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "ALPHAVANTAGE_KEY = \"HPCFVLGHWHQU0QTY\"\n",
    "CACHE_FILE = \"earnings_cache.json\"\n",
    "\n",
    "# ============================================================================\n",
    "# CACHING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_cache():\n",
    "    \"\"\"Load cached earnings data\"\"\"\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_cache(cache):\n",
    "    \"\"\"Save earnings data to cache\"\"\"\n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        json.dump(cache, f, indent=2, default=str)\n",
    "\n",
    "# ============================================================================\n",
    "# DATA FETCHING - ALPHA VANTAGE FOR EARNINGS\n",
    "# ============================================================================\n",
    "\n",
    "def get_earnings_details(ticker, use_cache=True):\n",
    "    \"\"\"Get exact earnings announcement dates from Alpha Vantage\"\"\"\n",
    "    \n",
    "    cache = load_cache()\n",
    "    \n",
    "    # Check cache first\n",
    "    if use_cache and ticker in cache:\n",
    "        print(f\"✓ Using cached earnings for {ticker}\")\n",
    "        return [\n",
    "            {'date': datetime.fromisoformat(e['date']), 'time': e['time']} \n",
    "            for e in cache[ticker]\n",
    "        ]\n",
    "    \n",
    "    # Fetch from API\n",
    "    print(f\"⏳ Fetching earnings from Alpha Vantage for {ticker}...\")\n",
    "    url = f\"https://www.alphavantage.co/query?function=EARNINGS&symbol={ticker}&apikey={ALPHAVANTAGE_KEY}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'Note' in data or 'Information' in data:\n",
    "            error_msg = data.get('Note', data.get('Information', ''))\n",
    "            print(f\"⚠️  API limit: {error_msg[:80]}...\")\n",
    "            return []\n",
    "        \n",
    "        if 'quarterlyEarnings' not in data:\n",
    "            print(f\"⚠️  No earnings data for {ticker}\")\n",
    "            return []\n",
    "        \n",
    "        earnings_info = []\n",
    "        for quarter in data['quarterlyEarnings']:\n",
    "            reported_date = quarter.get('reportedDate')\n",
    "            reported_time = quarter.get('reportedTime', 'amc')\n",
    "            \n",
    "            if reported_date:\n",
    "                earnings_info.append({\n",
    "                    'date': datetime.strptime(reported_date, '%Y-%m-%d'),\n",
    "                    'time': reported_time.lower()\n",
    "                })\n",
    "        \n",
    "        # Save to cache\n",
    "        cache[ticker] = [\n",
    "            {'date': e['date'].isoformat(), 'time': e['time']} \n",
    "            for e in earnings_info\n",
    "        ]\n",
    "        save_cache(cache)\n",
    "        print(f\"✓ Cached {len(earnings_info)} earnings dates for {ticker}\")\n",
    "        \n",
    "        return sorted(earnings_info, key=lambda x: x['date'], reverse=True)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error fetching earnings: {e}\")\n",
    "        return []\n",
    "\n",
    "# ============================================================================\n",
    "# DATA FETCHING - YAHOO FINANCE FOR PRICES\n",
    "# ============================================================================\n",
    "\n",
    "def get_yahoo_price_data(ticker, start_date, end_date):\n",
    "    \"\"\"Get historical closing prices from Yahoo Finance\"\"\"\n",
    "    start_ts = int(start_date.timestamp())\n",
    "    end_ts = int(end_date.timestamp())\n",
    "    \n",
    "    url = f\"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}\"\n",
    "    params = {'period1': start_ts, 'period2': end_ts, 'interval': '1d'}\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        data = response.json()\n",
    "        \n",
    "        result = data['chart']['result'][0]\n",
    "        timestamps = result['timestamp']\n",
    "        closes = result['indicators']['quote'][0]['close']\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'date': [datetime.fromtimestamp(ts) for ts in timestamps],\n",
    "            'close': closes\n",
    "        })\n",
    "        df.set_index('date', inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error fetching prices: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def find_nearest_price(price_data, target_date):\n",
    "    \"\"\"Find closing price on nearest trading day\"\"\"\n",
    "    if price_data.empty:\n",
    "        return None, None\n",
    "    \n",
    "    start = target_date - timedelta(days=7)\n",
    "    end = target_date + timedelta(days=7)\n",
    "    nearby = price_data[(price_data.index >= start) & (price_data.index <= end)]\n",
    "    \n",
    "    if nearby.empty:\n",
    "        return None, None\n",
    "    \n",
    "    time_diffs = (nearby.index - target_date).to_series().abs()\n",
    "    closest_idx = time_diffs.argmin()\n",
    "    return nearby.iloc[closest_idx]['close'], nearby.index[closest_idx]\n",
    "\n",
    "def get_reference_price(price_data, earnings_date, timing):\n",
    "    \"\"\"Get entry price based on earnings timing (BMO vs AMC)\"\"\"\n",
    "    # BMO: Enter at close day before | AMC: Enter at close of earnings day\n",
    "    target_date = earnings_date - timedelta(days=1) if timing == 'bmo' else earnings_date\n",
    "    return find_nearest_price(price_data, target_date)\n",
    "\n",
    "def calculate_historical_volatility(price_data, earnings_date, lookback_days=30):\n",
    "    \"\"\"Calculate 30-day historical volatility before earnings (annualized)\"\"\"\n",
    "    end_date = earnings_date - timedelta(days=1)\n",
    "    start_date = end_date - timedelta(days=lookback_days + 10)\n",
    "    \n",
    "    window = price_data[(price_data.index >= start_date) & (price_data.index <= end_date)]\n",
    "    \n",
    "    if len(window) < 20:\n",
    "        return None\n",
    "    \n",
    "    returns = window['close'].pct_change().dropna()\n",
    "    daily_vol = returns.std()\n",
    "    annual_vol = daily_vol * np.sqrt(252)\n",
    "    \n",
    "    return annual_vol\n",
    "\n",
    "def get_volatility_tier(hvol):\n",
    "    \"\"\"Map historical volatility to standard deviation multiplier\"\"\"\n",
    "    hvol_pct = hvol * 100\n",
    "    \n",
    "    if hvol_pct < 25:\n",
    "        return 1.0\n",
    "    elif hvol_pct < 35:\n",
    "        return 1.2\n",
    "    elif hvol_pct < 45:\n",
    "        return 1.4\n",
    "    else:\n",
    "        return 1.5\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_earnings_movement(ticker, lookback_quarters=24, verbose=True):\n",
    "    \"\"\"\n",
    "    Analyze post-earnings movements with volatility-adjusted strikes\n",
    "    \n",
    "    Args:\n",
    "        ticker: Stock symbol\n",
    "        lookback_quarters: Historical quarters to analyze (default 24 = 6 years)\n",
    "        verbose: Print detailed output\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*75}\")\n",
    "        print(f\"📊 {ticker} - Post-Earnings Containment Analysis\")\n",
    "        print(f\"{'='*75}\")\n",
    "    \n",
    "    # Get earnings dates from Alpha Vantage\n",
    "    earnings_info = get_earnings_details(ticker)\n",
    "    if not earnings_info:\n",
    "        return None\n",
    "    \n",
    "    today = datetime.now()\n",
    "    past_earnings = [e for e in earnings_info if e['date'] < today][:lookback_quarters]\n",
    "    \n",
    "    if len(past_earnings) < 10:\n",
    "        if verbose:\n",
    "            print(f\"⚠️  Insufficient data: only {len(past_earnings)} earnings periods\")\n",
    "        return None\n",
    "    \n",
    "    # Get price data from Yahoo Finance\n",
    "    oldest = min([e['date'] for e in past_earnings]) - timedelta(days=120)\n",
    "    price_data = get_yahoo_price_data(ticker, oldest, today)\n",
    "    \n",
    "    if price_data.empty:\n",
    "        return None\n",
    "    \n",
    "    # Collect movement data for both timeframes\n",
    "    data_45 = []\n",
    "    data_90 = []\n",
    "    hvol_list = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nAnalyzing {len(past_earnings)} earnings periods...\")\n",
    "    \n",
    "    for earnings in past_earnings:\n",
    "        # Calculate historical volatility\n",
    "        hvol = calculate_historical_volatility(price_data, earnings['date'])\n",
    "        if hvol is None:\n",
    "            continue\n",
    "        \n",
    "        hvol_list.append(hvol * 100)\n",
    "        strike_std = get_volatility_tier(hvol)\n",
    "        \n",
    "        # Get entry price (day before for BMO, same day for AMC)\n",
    "        ref_price, ref_date = get_reference_price(price_data, earnings['date'], earnings['time'])\n",
    "        if ref_price is None:\n",
    "            continue\n",
    "        \n",
    "        # Calculate strike width based on volatility tier\n",
    "        dte_45_factor = np.sqrt(45 / 365)\n",
    "        dte_90_factor = np.sqrt(90 / 365)\n",
    "        strike_width_45 = hvol * dte_45_factor * strike_std * 100\n",
    "        strike_width_90 = hvol * dte_90_factor * strike_std * 100\n",
    "        \n",
    "        # Test 45-day outcome\n",
    "        target_45 = earnings['date'] + timedelta(days=45)\n",
    "        if target_45 <= today:\n",
    "            price_45, date_45 = find_nearest_price(price_data, target_45)\n",
    "            if price_45 is not None:\n",
    "                move_45 = (price_45 - ref_price) / ref_price * 100\n",
    "                data_45.append({\n",
    "                    'move': move_45,\n",
    "                    'width': strike_width_45,\n",
    "                    'hvol': hvol * 100,\n",
    "                    'date': earnings['date'].strftime('%Y-%m-%d')\n",
    "                })\n",
    "        \n",
    "        # Test 90-day outcome\n",
    "        target_90 = earnings['date'] + timedelta(days=90)\n",
    "        if target_90 <= today:\n",
    "            price_90, date_90 = find_nearest_price(price_data, target_90)\n",
    "            if price_90 is not None:\n",
    "                move_90 = (price_90 - ref_price) / ref_price * 100\n",
    "                data_90.append({\n",
    "                    'move': move_90,\n",
    "                    'width': strike_width_90,\n",
    "                    'hvol': hvol * 100,\n",
    "                    'date': earnings['date'].strftime('%Y-%m-%d')\n",
    "                })\n",
    "    \n",
    "    if len(data_45) < 10 or len(data_90) < 10:\n",
    "        if verbose:\n",
    "            print(f\"⚠️  Insufficient valid data\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate statistics for both timeframes\n",
    "    def calc_stats(data):\n",
    "        total = len(data)\n",
    "        moves = np.array([d['move'] for d in data])\n",
    "        widths = np.array([d['width'] for d in data])\n",
    "        \n",
    "        # Containment\n",
    "        stays_within = sum(1 for i, m in enumerate(moves) if abs(m) <= widths[i])\n",
    "        breaks_up = sum(1 for i, m in enumerate(moves) if m > widths[i])\n",
    "        breaks_down = sum(1 for i, m in enumerate(moves) if m < -widths[i])\n",
    "        \n",
    "        # Directional bias\n",
    "        up_moves = sum(1 for m in moves if m > 0)\n",
    "        up_bias = (up_moves / total) * 100\n",
    "        \n",
    "        return {\n",
    "            'total': total,\n",
    "            'containment': (stays_within / total) * 100,\n",
    "            'breaks_up': breaks_up,\n",
    "            'breaks_down': breaks_down,\n",
    "            'up_bias': up_bias,\n",
    "            'avg_width': np.mean(widths)\n",
    "        }\n",
    "    \n",
    "    stats_45 = calc_stats(data_45)\n",
    "    stats_90 = calc_stats(data_90)\n",
    "    avg_hvol = np.mean(hvol_list)\n",
    "    avg_tier = get_volatility_tier(avg_hvol / 100)\n",
    "    \n",
    "    # Determine recommendation based on data\n",
    "    rec_parts = []\n",
    "    \n",
    "    # Check 90-day containment first (preferred timeframe)\n",
    "    if stats_90['containment'] >= 70:\n",
    "        rec_parts.append(\"IC (90 DTE)\")\n",
    "    elif stats_45['containment'] >= 70:\n",
    "        rec_parts.append(\"IC (45 DTE)\")\n",
    "    \n",
    "    # Check directional edge\n",
    "    if stats_90['up_bias'] >= 70 and stats_90['breaks_down'] <= stats_90['total'] * 0.15:\n",
    "        rec_parts.append(\"Bull Put Spread\")\n",
    "    elif stats_90['up_bias'] <= 30 and stats_90['breaks_up'] <= stats_90['total'] * 0.15:\n",
    "        rec_parts.append(\"Bear Call Spread\")\n",
    "    \n",
    "    recommendation = \" + \".join(rec_parts) if rec_parts else \"SKIP - No Edge\"\n",
    "    \n",
    "    # Print results\n",
    "    if verbose:\n",
    "        print(f\"\\n📊 {ticker} | {avg_hvol:.1f}% HVol | {avg_tier:.1f} std (±{stats_90['avg_width']:.1f}%)\")\n",
    "        print(f\"\\n  45-Day: {stats_45['total']}/{lookback_quarters} tested\")\n",
    "        print(f\"    Containment: {stats_45['containment']:.0f}%\")\n",
    "        print(f\"    Breaks: Up {stats_45['breaks_up']}, Down {stats_45['breaks_down']}\")\n",
    "        print(f\"    Bias: {stats_45['up_bias']:.0f}% up\")\n",
    "        \n",
    "        print(f\"\\n  90-Day: {stats_90['total']}/{lookback_quarters} tested\")\n",
    "        print(f\"    Containment: {stats_90['containment']:.0f}%\")\n",
    "        print(f\"    Breaks: Up {stats_90['breaks_up']}, Down {stats_90['breaks_down']}\")\n",
    "        print(f\"    Bias: {stats_90['up_bias']:.0f}% up\")\n",
    "        \n",
    "        print(f\"\\n  💡 Strategy: {recommendation}\")\n",
    "    \n",
    "    summary = {\n",
    "        'ticker': ticker,\n",
    "        'hvol': round(avg_hvol, 1),\n",
    "        'tier': round(avg_tier, 1),\n",
    "        'strike_width': round(stats_90['avg_width'], 1),\n",
    "        '45d_contain': round(stats_45['containment'], 0),\n",
    "        '45d_breaks_up': stats_45['breaks_up'],\n",
    "        '45d_breaks_dn': stats_45['breaks_down'],\n",
    "        '45d_bias': round(stats_45['up_bias'], 0),\n",
    "        '90d_contain': round(stats_90['containment'], 0),\n",
    "        '90d_breaks_up': stats_90['breaks_up'],\n",
    "        '90d_breaks_dn': stats_90['breaks_down'],\n",
    "        '90d_bias': round(stats_90['up_bias'], 0),\n",
    "        'strategy': recommendation\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def format_break_ratio(up_breaks, down_breaks):\n",
    "    \"\"\"Format break ratio with directional arrow if edge exists (2:1 threshold)\"\"\"\n",
    "    if up_breaks == 0 and down_breaks == 0:\n",
    "        return \"0:0\"\n",
    "    \n",
    "    total = up_breaks + down_breaks\n",
    "    if up_breaks >= 2 * down_breaks and up_breaks > 0:\n",
    "        return f\"{up_breaks}:{down_breaks}↑\"\n",
    "    elif down_breaks >= 2 * up_breaks and down_breaks > 0:\n",
    "        return f\"{up_breaks}:{down_breaks}↓\"\n",
    "    else:\n",
    "        return f\"{up_breaks}:{down_breaks}\"\n",
    "\n",
    "def batch_analyze(tickers, lookback_quarters=24):\n",
    "    \"\"\"Analyze multiple tickers\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*75)\n",
    "    print(f\"EARNINGS CONTAINMENT ANALYZER\")\n",
    "    print(f\"Lookback: {lookback_quarters} quarters (~{lookback_quarters/4:.0f} years)\")\n",
    "    print(f\"Volatility Tiers: <25%=1.0std | 25-35%=1.2std | 35-45%=1.4std | >45%=1.5std\")\n",
    "    print(\"=\"*75)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        summary = analyze_earnings_movement(ticker, lookback_quarters, verbose=False)\n",
    "        \n",
    "        if summary:\n",
    "            results.append(summary)\n",
    "        \n",
    "        time.sleep(1)  # Be nice to APIs\n",
    "    \n",
    "    if not results:\n",
    "        print(\"\\nâš ï¸  No valid results\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate 45-day strike width for comparison\n",
    "    for idx, row in df.iterrows():\n",
    "        # Approximate 45-day width using square root of time scaling\n",
    "        df.at[idx, '45d_width'] = round(row['strike_width'] * np.sqrt(45/90), 1)\n",
    "    \n",
    "    # Create formatted columns\n",
    "    df['45_break_fmt'] = df.apply(lambda x: format_break_ratio(x['45d_breaks_up'], x['45d_breaks_dn']), axis=1)\n",
    "    df['90_break_fmt'] = df.apply(lambda x: format_break_ratio(x['90d_breaks_up'], x['90d_breaks_dn']), axis=1)\n",
    "    \n",
    "    # Format strategy recommendations with shorthand\n",
    "    df['strategy_fmt'] = df['strategy'].replace({\n",
    "        'IC (90 DTE)': 'IC90',\n",
    "        'IC (45 DTE)': 'IC45',\n",
    "        'IC (90 DTE) + Bull Put Spread': 'IC90+BPS',\n",
    "        'IC (90 DTE) + Bear Call Spread': 'IC90+BCS',\n",
    "        'IC (45 DTE) + Bull Put Spread': 'IC45+BPS',\n",
    "        'IC (45 DTE) + Bear Call Spread': 'IC45+BCS',\n",
    "        'Bull Put Spread': 'BPS',\n",
    "        'Bear Call Spread': 'BCS',\n",
    "        'SKIP - No Edge': 'SKIP'\n",
    "    })\n",
    "    \n",
    "    # Compact overview with side-by-side comparison\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"TIMEFRAME COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Create display dataframe with formatting\n",
    "    display_df = pd.DataFrame({\n",
    "        'Ticker': df['ticker'],\n",
    "        'HVol%': df['hvol'].astype(int),\n",
    "        'Tier': df['tier'],\n",
    "        ' ': '|',\n",
    "        '45D%': df['45d_contain'].astype(int),\n",
    "        '45Bias': df['45d_bias'].astype(int),\n",
    "        '45Break': df['45_break_fmt'],\n",
    "        '45Width': df['45d_width'].astype(str) + '%',\n",
    "        '  ': '|',\n",
    "        '90D%': df['90d_contain'].astype(int),\n",
    "        '90Bias': df['90d_bias'].astype(int),\n",
    "        '90Break': df['90_break_fmt'],\n",
    "        '90Width': df['strike_width'].astype(str) + '%',\n",
    "        '   ': '|',\n",
    "        'Strategy': df['strategy_fmt']\n",
    "    })\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    return df\n",
    "# ============================================================================\n",
    "# RUN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with 2 tickers\n",
    "    tickers = [\"DAL\", \"PEP\", \"FAST\", \"BLK\", \"C\", \"DPZ\", \"GS\", \"JNJ\", \"JPM\", \n",
    "           \"WBA\", \"WFC\", \"OMC\", \"ABT\", \"BAC\", \"CFG\", \"MS\", \"PGR\", \"PLD\", \n",
    "           \"PNC\", \"SYF\", \"JBHT\", \"UAL\", \"BK\", \"KEY\", \"MMC\", \"MTB\", \"SCHW\", \n",
    "           \"SNA\", \"TRV\", \"USB\", \"CSX\", \"AXP\", \"FITB\", \"HBAN\", \"RF\", \"SLB\", \n",
    "           \"STT\", \"TFC\"]\n",
    "    \n",
    "    results = batch_analyze(tickers, lookback_quarters=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4be09987-304e-4329-b7ab-22970841a801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      "EARNINGS CONTAINMENT ANALYZER\n",
      "Lookback: 24 quarters (~6 years)\n",
      "Volatility Tiers: <25%=1.0std | 25-35%=1.2std | 35-45%=1.4std | >45%=1.5std\n",
      "===========================================================================\n",
      "[10/38] Processing WBA...\n",
      "  WBA: API Error (key ...0QTY): We have detected your API key as HPCFVLGHWHQU0QTY and our standard API rate limit is 25 requests per\n",
      "\n",
      "  WBA: Key ...0QTY marked as rate-limited (1/3 keys exhausted)\n",
      "\n",
      "  WBA: API Error (key ...JPK5): We have detected your API key as VL7Z4WRK8T5MJPK5 and our standard API rate limit is 25 requests per\n",
      "\n",
      "  WBA: Key ...JPK5 marked as rate-limited (2/3 keys exhausted)\n",
      "\n",
      "  WBA: API Error (key ...3321): We have detected your API key as DYU6F4AG3IL03321 and our standard API rate limit is 25 requests per\n",
      "\n",
      "  WBA: Key ...3321 marked as rate-limited (3/3 keys exhausted)\n",
      "\n",
      "  WBA: All 3 API keys now exhausted\n",
      "\n",
      "  WBA: All 3 API keys already exhausted\n",
      "[26/38] Processing MTB....\n",
      "  MTB: All 3 API keys already exhausted\n",
      "\n",
      "  MTB: All 3 API keys already exhausted\n",
      "[27/38] Processing SCHW...\n",
      "  SCHW: All 3 API keys already exhausted\n",
      "\n",
      "  SCHW: All 3 API keys already exhausted\n",
      "[28/38] Processing SNA...\n",
      "  SNA: All 3 API keys already exhausted\n",
      "\n",
      "  SNA: All 3 API keys already exhausted\n",
      "[29/38] Processing TRV...\n",
      "  TRV: All 3 API keys already exhausted\n",
      "\n",
      "  TRV: All 3 API keys already exhausted\n",
      "[30/38] Processing USB...\n",
      "  USB: All 3 API keys already exhausted\n",
      "\n",
      "  USB: All 3 API keys already exhausted\n",
      "[31/38] Processing CSX...\n",
      "  CSX: All 3 API keys already exhausted\n",
      "\n",
      "  CSX: All 3 API keys already exhausted\n",
      "[32/38] Processing AXP...\n",
      "  AXP: All 3 API keys already exhausted\n",
      "\n",
      "  AXP: All 3 API keys already exhausted\n",
      "[33/38] Processing FITB...\n",
      "  FITB: All 3 API keys already exhausted\n",
      "\n",
      "  FITB: All 3 API keys already exhausted\n",
      "[34/38] Processing HBAN...\n",
      "  HBAN: All 3 API keys already exhausted\n",
      "\n",
      "  HBAN: All 3 API keys already exhausted\n",
      "[35/38] Processing RF...\n",
      "  RF: All 3 API keys already exhausted\n",
      "\n",
      "  RF: All 3 API keys already exhausted\n",
      "[36/38] Processing SLB...\n",
      "  SLB: All 3 API keys already exhausted\n",
      "\n",
      "  SLB: All 3 API keys already exhausted\n",
      "[37/38] Processing STT...\n",
      "  STT: All 3 API keys already exhausted\n",
      "\n",
      "  STT: All 3 API keys already exhausted\n",
      "[38/38] Processing TFC...\n",
      "  TFC: All 3 API keys already exhausted\n",
      "\n",
      "  TFC: All 3 API keys already exhausted\n",
      "                                                                                \n",
      "📊 FETCH SUMMARY\n",
      "===========================================================================\n",
      "✓ From Cache (24): DAL, PEP, FAST, BLK, C, DPZ, GS, JNJ, JPM, WFC, OMC, ABT, BAC, CFG, MS, PGR, PLD, PNC, SYF, JBHT, UAL, BK, KEY, MMC\n",
      "✗ No Earnings Data (14): WBA, MTB, SCHW, SNA, TRV, USB, CSX, AXP, FITB, HBAN, RF, SLB, STT, TFC\n",
      "\n",
      "====================================================================================================\n",
      "TIMEFRAME COMPARISON\n",
      "====================================================================================================\n",
      "Ticker  HVol%  Tier    45D%  45Bias 45Break 45Width     90D%  90Bias 90Break 90Width     Strategy\n",
      "   DAL     43   1.4 |    75      71    4:2↑   20.7%  |    67      67     5:3   29.3%   |     IC45\n",
      "   PEP     18   1.0 |    71      58     4:3    7.1%  |    74      65     3:3   10.0%   |     IC90\n",
      "  FAST     24   1.0 |    67      71    6:2↑   10.3%  |    78      61     3:2   14.6%   |     IC90\n",
      "   BLK     28   1.2 |    62      67    6:3↑   12.2%  |    61      57     5:4   17.3%   |     SKIP\n",
      "     C     34   1.2 |    75      71     3:3   15.7%  |    70      48     4:3   22.2%   |     IC45\n",
      "   DPZ     27   1.2 |    62      67    6:3↑   11.8%  |    65      61     5:3   16.7%   |     SKIP\n",
      "    GS     32   1.2 |    71      75    6:1↑   14.6%  |    70      61    5:2↑   20.6%   |     IC45\n",
      "   JNJ     17   1.0 |    62      50     4:5    7.0%  |    78      52     2:3    9.9%   |     IC90\n",
      "   JPM     28   1.2 |    58      79    8:2↑   12.4%  |    61      70    6:3↑   17.5%   |     SKIP\n",
      "   WFC     35   1.4 |    71      79    6:1↑   16.6%  |    65      65     5:3   23.5%   |     IC45\n",
      "   OMC     26   1.2 |    67      54     4:4   10.5%  |    65      61     3:5   14.9%   |     SKIP\n",
      "   ABT     22   1.0 |    46      58     8:5    9.0%  |    78      57    4:1↑   12.7%   |     IC90\n",
      "   BAC     32   1.2 |    58      67    7:3↑   14.8%  |    61      61     5:4   21.0%   |     SKIP\n",
      "   CFG     40   1.4 |    92      62     1:1   19.6%  |    74      65     3:3   27.7%   |     IC90\n",
      "    MS     33   1.2 |    62      67    7:2↑   15.1%  |    61      70    6:3↑   21.3%   |     SKIP\n",
      "   PGR     24   1.0 |    71      71    6:1↑   10.0%  |    70      70    7:0↑   14.2%   |     IC45\n",
      "   PLD     29   1.2 |    71      75     4:3   12.9%  |    61      61    6:3↑   18.3%   |     IC45\n",
      "   PNC     32   1.2 |    71      75    5:2↑   14.4%  |    61      70     5:4   20.4%   |     IC45\n",
      "   SYF     41   1.4 |    50      62     7:5   19.9%  |    70      65     3:4   28.2%   |     SKIP\n",
      "  JBHT     29   1.2 |    83      67    1:3↓   12.4%  |    74      52    2:4↓   17.5%   |     IC90\n",
      "   UAL     51   1.5 |    79      62     3:2   25.5%  |    70      57     4:3   36.1%   |     IC45\n",
      "    BK     29   1.2 |    62      79    7:2↑   13.1%  |    74      65    4:2↑   18.5%   |     IC90\n",
      "   KEY     42   1.4 |    79      62    4:1↑   20.6%  |    78      61     2:3   29.1%   |     IC90\n",
      "   MMC     18   1.0 |    54      79    8:3↑    7.4%  |    65      70    6:2↑   10.5%   |     SKIP\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "CURRENT_KEY_INDEX = 0\n",
    "RATE_LIMITED_KEYS = set()  # Track globally rate-limited keys\n",
    "CACHE_FILE = \"earnings_cache.json\"\n",
    "\n",
    "# ============================================================================\n",
    "# CACHING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_cache():\n",
    "    \"\"\"Load cached earnings data\"\"\"\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_cache(cache):\n",
    "    \"\"\"Save earnings data to cache\"\"\"\n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        json.dump(cache, f, indent=2, default=str)\n",
    "\n",
    "# ============================================================================\n",
    "# DATA FETCHING - ALPHA VANTAGE FOR EARNINGS\n",
    "# ============================================================================\n",
    "\n",
    "def get_earnings_details(ticker, use_cache=True, debug=False):\n",
    "    \"\"\"Get exact earnings announcement dates from Alpha Vantage with key rotation\"\"\"\n",
    "    global CURRENT_KEY_INDEX, RATE_LIMITED_KEYS\n",
    "    \n",
    "    cache = load_cache()\n",
    "    \n",
    "    # Check cache first\n",
    "    if use_cache and ticker in cache:\n",
    "        return [\n",
    "            {'date': datetime.fromisoformat(e['date']), 'time': e['time']} \n",
    "            for e in cache[ticker]\n",
    "        ], \"cached\"\n",
    "    \n",
    "    # Check if all keys are exhausted before even trying\n",
    "    if len(RATE_LIMITED_KEYS) >= len(ALPHAVANTAGE_KEYS):\n",
    "        if debug:\n",
    "            print(f\"\\n  {ticker}: All {len(ALPHAVANTAGE_KEYS)} API keys already exhausted\")\n",
    "        return [], \"rate_limited_all_keys\"\n",
    "    \n",
    "    # Try all available API keys (but only those not yet rate-limited)\n",
    "    attempts = 0\n",
    "    max_attempts = len(ALPHAVANTAGE_KEYS) - len(RATE_LIMITED_KEYS)\n",
    "    \n",
    "    while attempts < max_attempts:\n",
    "        # Skip to next non-rate-limited key\n",
    "        while CURRENT_KEY_INDEX in RATE_LIMITED_KEYS:\n",
    "            CURRENT_KEY_INDEX = (CURRENT_KEY_INDEX + 1) % len(ALPHAVANTAGE_KEYS)\n",
    "        \n",
    "        current_key = ALPHAVANTAGE_KEYS[CURRENT_KEY_INDEX]\n",
    "        key_suffix = current_key[-4:]  # Last 4 chars for identification\n",
    "        url = f\"https://www.alphavantage.co/query?function=EARNINGS&symbol={ticker}&apikey={current_key}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            \n",
    "            # Check for rate limit or API errors\n",
    "            if 'Note' in data or 'Information' in data:\n",
    "                error_msg = data.get('Note', data.get('Information', ''))\n",
    "                if debug:\n",
    "                    print(f\"\\n  {ticker}: API Error (key ...{key_suffix}): {error_msg[:100]}\")\n",
    "                \n",
    "                if 'API rate limit' in error_msg or 'API call frequency' in error_msg or 'rate limit' in error_msg.lower():\n",
    "                    # Mark this key as globally rate limited\n",
    "                    RATE_LIMITED_KEYS.add(CURRENT_KEY_INDEX)\n",
    "                    if debug:\n",
    "                        print(f\"\\n  {ticker}: Key ...{key_suffix} marked as rate-limited ({len(RATE_LIMITED_KEYS)}/{len(ALPHAVANTAGE_KEYS)} keys exhausted)\")\n",
    "                    \n",
    "                    # Switch to next key\n",
    "                    CURRENT_KEY_INDEX = (CURRENT_KEY_INDEX + 1) % len(ALPHAVANTAGE_KEYS)\n",
    "                    \n",
    "                    # If all keys exhausted, stop\n",
    "                    if len(RATE_LIMITED_KEYS) >= len(ALPHAVANTAGE_KEYS):\n",
    "                        if debug:\n",
    "                            print(f\"\\n  {ticker}: All {len(ALPHAVANTAGE_KEYS)} API keys now exhausted\")\n",
    "                        return [], \"rate_limited_all_keys\"\n",
    "                    \n",
    "                    attempts += 1\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                    \n",
    "                # Other API errors (not rate limit)\n",
    "                return [], f\"api_error: {error_msg[:50]}\"\n",
    "            \n",
    "            if 'quarterlyEarnings' not in data:\n",
    "                if debug:\n",
    "                    print(f\"\\n  {ticker}: No 'quarterlyEarnings' in response\")\n",
    "                return [], \"no_earnings_field\"\n",
    "            \n",
    "            earnings_info = []\n",
    "            for quarter in data['quarterlyEarnings']:\n",
    "                reported_date = quarter.get('reportedDate')\n",
    "                reported_time = quarter.get('reportedTime', 'amc')\n",
    "                \n",
    "                if reported_date:\n",
    "                    earnings_info.append({\n",
    "                        'date': datetime.strptime(reported_date, '%Y-%m-%d'),\n",
    "                        'time': reported_time.lower()\n",
    "                    })\n",
    "            \n",
    "            # Save to cache\n",
    "            cache[ticker] = [\n",
    "                {'date': e['date'].isoformat(), 'time': e['time']} \n",
    "                for e in earnings_info\n",
    "            ]\n",
    "            save_cache(cache)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"\\n  {ticker}: ✓ Success with key ...{key_suffix}\")\n",
    "            \n",
    "            return sorted(earnings_info, key=lambda x: x['date'], reverse=True), \"success\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"\\n  {ticker}: Exception (key ...{key_suffix}): {str(e)[:100]}\")\n",
    "            # On exception, try next key if available\n",
    "            attempts += 1\n",
    "            if attempts < max_attempts:\n",
    "                CURRENT_KEY_INDEX = (CURRENT_KEY_INDEX + 1) % len(ALPHAVANTAGE_KEYS)\n",
    "                continue\n",
    "            return [], f\"exception: {str(e)[:50]}\"\n",
    "    \n",
    "    return [], \"unknown_error\"\n",
    "\n",
    "# ============================================================================\n",
    "# DATA FETCHING - YAHOO FINANCE FOR PRICES\n",
    "# ============================================================================\n",
    "\n",
    "def get_yahoo_price_data(ticker, start_date, end_date):\n",
    "    \"\"\"Get historical closing prices from Yahoo Finance\"\"\"\n",
    "    start_ts = int(start_date.timestamp())\n",
    "    end_ts = int(end_date.timestamp())\n",
    "    \n",
    "    url = f\"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}\"\n",
    "    params = {'period1': start_ts, 'period2': end_ts, 'interval': '1d'}\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        data = response.json()\n",
    "        \n",
    "        result = data['chart']['result'][0]\n",
    "        timestamps = result['timestamp']\n",
    "        closes = result['indicators']['quote'][0]['close']\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'date': [datetime.fromtimestamp(ts) for ts in timestamps],\n",
    "            'close': closes\n",
    "        })\n",
    "        df.set_index('date', inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error fetching prices: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def find_nearest_price(price_data, target_date):\n",
    "    \"\"\"Find closing price on nearest trading day\"\"\"\n",
    "    if price_data.empty:\n",
    "        return None, None\n",
    "    \n",
    "    start = target_date - timedelta(days=7)\n",
    "    end = target_date + timedelta(days=7)\n",
    "    nearby = price_data[(price_data.index >= start) & (price_data.index <= end)]\n",
    "    \n",
    "    if nearby.empty:\n",
    "        return None, None\n",
    "    \n",
    "    time_diffs = (nearby.index - target_date).to_series().abs()\n",
    "    closest_idx = time_diffs.argmin()\n",
    "    return nearby.iloc[closest_idx]['close'], nearby.index[closest_idx]\n",
    "\n",
    "def get_reference_price(price_data, earnings_date, timing):\n",
    "    \"\"\"Get entry price based on earnings timing (BMO vs AMC)\"\"\"\n",
    "    # BMO: Enter at close day before | AMC: Enter at close of earnings day\n",
    "    target_date = earnings_date - timedelta(days=1) if timing == 'bmo' else earnings_date\n",
    "    return find_nearest_price(price_data, target_date)\n",
    "\n",
    "def calculate_historical_volatility(price_data, earnings_date, lookback_days=30):\n",
    "    \"\"\"Calculate 30-day historical volatility before earnings (annualized)\"\"\"\n",
    "    end_date = earnings_date - timedelta(days=1)\n",
    "    start_date = end_date - timedelta(days=lookback_days + 10)\n",
    "    \n",
    "    window = price_data[(price_data.index >= start_date) & (price_data.index <= end_date)]\n",
    "    \n",
    "    if len(window) < 20:\n",
    "        return None\n",
    "    \n",
    "    returns = window['close'].pct_change().dropna()\n",
    "    daily_vol = returns.std()\n",
    "    annual_vol = daily_vol * np.sqrt(252)\n",
    "    \n",
    "    return annual_vol\n",
    "\n",
    "def get_volatility_tier(hvol):\n",
    "    \"\"\"Map historical volatility to standard deviation multiplier\"\"\"\n",
    "    hvol_pct = hvol * 100\n",
    "    \n",
    "    if hvol_pct < 25:\n",
    "        return 1.0\n",
    "    elif hvol_pct < 35:\n",
    "        return 1.2\n",
    "    elif hvol_pct < 45:\n",
    "        return 1.4\n",
    "    else:\n",
    "        return 1.5\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_earnings_movement(ticker, lookback_quarters=24, verbose=True, debug=False):\n",
    "    \"\"\"\n",
    "    Analyze post-earnings movements with volatility-adjusted strikes\n",
    "    \n",
    "    Args:\n",
    "        ticker: Stock symbol\n",
    "        lookback_quarters: Historical quarters to analyze (default 24 = 6 years)\n",
    "        verbose: Print detailed output\n",
    "        debug: Show API debugging info\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*75}\")\n",
    "        print(f\"📊 {ticker} - Post-Earnings Containment Analysis\")\n",
    "        print(f\"{'='*75}\")\n",
    "    \n",
    "    # Get earnings dates from Alpha Vantage\n",
    "    earnings_info, status = get_earnings_details(ticker, debug=debug)\n",
    "    if not earnings_info:\n",
    "        return None, status\n",
    "    \n",
    "    today = datetime.now()\n",
    "    past_earnings = [e for e in earnings_info if e['date'] < today][:lookback_quarters]\n",
    "    \n",
    "    if len(past_earnings) < 10:\n",
    "        if verbose:\n",
    "            print(f\"⚠️  Insufficient data: only {len(past_earnings)} earnings periods\")\n",
    "        return None, \"insufficient_quarters\"\n",
    "    \n",
    "    # Get price data from Yahoo Finance\n",
    "    oldest = min([e['date'] for e in past_earnings]) - timedelta(days=120)\n",
    "    price_data = get_yahoo_price_data(ticker, oldest, today)\n",
    "    \n",
    "    if price_data.empty:\n",
    "        return None, \"no_price_data\"\n",
    "    \n",
    "    # Collect movement data for both timeframes\n",
    "    data_45 = []\n",
    "    data_90 = []\n",
    "    hvol_list = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nAnalyzing {len(past_earnings)} earnings periods...\")\n",
    "    \n",
    "    for earnings in past_earnings:\n",
    "        # Calculate historical volatility\n",
    "        hvol = calculate_historical_volatility(price_data, earnings['date'])\n",
    "        if hvol is None:\n",
    "            continue\n",
    "        \n",
    "        hvol_list.append(hvol * 100)\n",
    "        strike_std = get_volatility_tier(hvol)\n",
    "        \n",
    "        # Get entry price (day before for BMO, same day for AMC)\n",
    "        ref_price, ref_date = get_reference_price(price_data, earnings['date'], earnings['time'])\n",
    "        if ref_price is None:\n",
    "            continue\n",
    "        \n",
    "        # Calculate strike width based on volatility tier\n",
    "        dte_45_factor = np.sqrt(45 / 365)\n",
    "        dte_90_factor = np.sqrt(90 / 365)\n",
    "        strike_width_45 = hvol * dte_45_factor * strike_std * 100\n",
    "        strike_width_90 = hvol * dte_90_factor * strike_std * 100\n",
    "        \n",
    "        # Test 45-day outcome\n",
    "        target_45 = earnings['date'] + timedelta(days=45)\n",
    "        if target_45 <= today:\n",
    "            price_45, date_45 = find_nearest_price(price_data, target_45)\n",
    "            if price_45 is not None:\n",
    "                move_45 = (price_45 - ref_price) / ref_price * 100\n",
    "                data_45.append({\n",
    "                    'move': move_45,\n",
    "                    'width': strike_width_45,\n",
    "                    'hvol': hvol * 100,\n",
    "                    'date': earnings['date'].strftime('%Y-%m-%d')\n",
    "                })\n",
    "        \n",
    "        # Test 90-day outcome\n",
    "        target_90 = earnings['date'] + timedelta(days=90)\n",
    "        if target_90 <= today:\n",
    "            price_90, date_90 = find_nearest_price(price_data, target_90)\n",
    "            if price_90 is not None:\n",
    "                move_90 = (price_90 - ref_price) / ref_price * 100\n",
    "                data_90.append({\n",
    "                    'move': move_90,\n",
    "                    'width': strike_width_90,\n",
    "                    'hvol': hvol * 100,\n",
    "                    'date': earnings['date'].strftime('%Y-%m-%d')\n",
    "                })\n",
    "    \n",
    "    if len(data_45) < 10 or len(data_90) < 10:\n",
    "        if verbose:\n",
    "            print(f\"⚠️  Insufficient valid data\")\n",
    "        return None, \"insufficient_valid_data\"\n",
    "    \n",
    "    # Calculate statistics for both timeframes\n",
    "    def calc_stats(data):\n",
    "        total = len(data)\n",
    "        moves = np.array([d['move'] for d in data])\n",
    "        widths = np.array([d['width'] for d in data])\n",
    "        \n",
    "        # Containment\n",
    "        stays_within = sum(1 for i, m in enumerate(moves) if abs(m) <= widths[i])\n",
    "        breaks_up = sum(1 for i, m in enumerate(moves) if m > widths[i])\n",
    "        breaks_down = sum(1 for i, m in enumerate(moves) if m < -widths[i])\n",
    "        \n",
    "        # Directional bias\n",
    "        up_moves = sum(1 for m in moves if m > 0)\n",
    "        up_bias = (up_moves / total) * 100\n",
    "        \n",
    "        return {\n",
    "            'total': total,\n",
    "            'containment': (stays_within / total) * 100,\n",
    "            'breaks_up': breaks_up,\n",
    "            'breaks_down': breaks_down,\n",
    "            'up_bias': up_bias,\n",
    "            'avg_width': np.mean(widths)\n",
    "        }\n",
    "    \n",
    "    stats_45 = calc_stats(data_45)\n",
    "    stats_90 = calc_stats(data_90)\n",
    "    avg_hvol = np.mean(hvol_list)\n",
    "    avg_tier = get_volatility_tier(avg_hvol / 100)\n",
    "    \n",
    "    # Determine recommendation based on data\n",
    "    rec_parts = []\n",
    "    \n",
    "    # Check 90-day containment first (preferred timeframe)\n",
    "    if stats_90['containment'] >= 70:\n",
    "        rec_parts.append(\"IC (90 DTE)\")\n",
    "    elif stats_45['containment'] >= 70:\n",
    "        rec_parts.append(\"IC (45 DTE)\")\n",
    "    \n",
    "    # Check directional edge\n",
    "    if stats_90['up_bias'] >= 70 and stats_90['breaks_down'] <= stats_90['total'] * 0.15:\n",
    "        rec_parts.append(\"Bull Put Spread\")\n",
    "    elif stats_90['up_bias'] <= 30 and stats_90['breaks_up'] <= stats_90['total'] * 0.15:\n",
    "        rec_parts.append(\"Bear Call Spread\")\n",
    "    \n",
    "    recommendation = \" + \".join(rec_parts) if rec_parts else \"SKIP - No Edge\"\n",
    "    \n",
    "    # Print results\n",
    "    if verbose:\n",
    "        print(f\"\\n📊 {ticker} | {avg_hvol:.1f}% HVol | {avg_tier:.1f} std (±{stats_90['avg_width']:.1f}%)\")\n",
    "        print(f\"\\n  45-Day: {stats_45['total']}/{lookback_quarters} tested\")\n",
    "        print(f\"    Containment: {stats_45['containment']:.0f}%\")\n",
    "        print(f\"    Breaks: Up {stats_45['breaks_up']}, Down {stats_45['breaks_down']}\")\n",
    "        print(f\"    Bias: {stats_45['up_bias']:.0f}% up\")\n",
    "        \n",
    "        print(f\"\\n  90-Day: {stats_90['total']}/{lookback_quarters} tested\")\n",
    "        print(f\"    Containment: {stats_90['containment']:.0f}%\")\n",
    "        print(f\"    Breaks: Up {stats_90['breaks_up']}, Down {stats_90['breaks_down']}\")\n",
    "        print(f\"    Bias: {stats_90['up_bias']:.0f}% up\")\n",
    "        \n",
    "        print(f\"\\n  💡 Strategy: {recommendation}\")\n",
    "    \n",
    "    summary = {\n",
    "        'ticker': ticker,\n",
    "        'hvol': round(avg_hvol, 1),\n",
    "        'tier': round(avg_tier, 1),\n",
    "        'strike_width': round(stats_90['avg_width'], 1),\n",
    "        '45d_contain': round(stats_45['containment'], 0),\n",
    "        '45d_breaks_up': stats_45['breaks_up'],\n",
    "        '45d_breaks_dn': stats_45['breaks_down'],\n",
    "        '45d_bias': round(stats_45['up_bias'], 0),\n",
    "        '90d_contain': round(stats_90['containment'], 0),\n",
    "        '90d_breaks_up': stats_90['breaks_up'],\n",
    "        '90d_breaks_dn': stats_90['breaks_down'],\n",
    "        '90d_bias': round(stats_90['up_bias'], 0),\n",
    "        'strategy': recommendation\n",
    "    }\n",
    "    \n",
    "    return summary, \"success\"\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def format_break_ratio(up_breaks, down_breaks):\n",
    "    \"\"\"Format break ratio with directional arrow if edge exists (2:1 threshold)\"\"\"\n",
    "    if up_breaks == 0 and down_breaks == 0:\n",
    "        return \"0:0\"\n",
    "    \n",
    "    total = up_breaks + down_breaks\n",
    "    if up_breaks >= 2 * down_breaks and up_breaks > 0:\n",
    "        return f\"{up_breaks}:{down_breaks}↑\"\n",
    "    elif down_breaks >= 2 * up_breaks and down_breaks > 0:\n",
    "        return f\"{up_breaks}:{down_breaks}↓\"\n",
    "    else:\n",
    "        return f\"{up_breaks}:{down_breaks}\"\n",
    "\n",
    "def batch_analyze(tickers, lookback_quarters=24, debug=False):\n",
    "    \"\"\"Analyze multiple tickers with progress tracking\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*75)\n",
    "    print(f\"EARNINGS CONTAINMENT ANALYZER\")\n",
    "    print(f\"Lookback: {lookback_quarters} quarters (~{lookback_quarters/4:.0f} years)\")\n",
    "    print(f\"Volatility Tiers: <25%=1.0std | 25-35%=1.2std | 35-45%=1.4std | >45%=1.5std\")\n",
    "    print(\"=\"*75)\n",
    "    \n",
    "    results = []\n",
    "    fetched_from_cache = []\n",
    "    fetched_from_api = []\n",
    "    failed_rate_limit = []\n",
    "    failed_no_data = []\n",
    "    failed_other = []\n",
    "    \n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        print(f\"\\r[{i}/{len(tickers)}] Processing {ticker}...\", end='', flush=True)\n",
    "        \n",
    "        # Check cache first to categorize\n",
    "        cache = load_cache()\n",
    "        from_cache = ticker in cache\n",
    "        \n",
    "        # Check if ticker failed before due to specific reasons\n",
    "        earnings_data, _ = get_earnings_details(ticker, debug=debug)\n",
    "        \n",
    "        summary, status = analyze_earnings_movement(ticker, lookback_quarters, verbose=False, debug=debug)\n",
    "        \n",
    "        if summary:\n",
    "            results.append(summary)\n",
    "            if from_cache:\n",
    "                fetched_from_cache.append(ticker)\n",
    "            else:\n",
    "                fetched_from_api.append(ticker)\n",
    "        else:\n",
    "            # Categorize failure\n",
    "            if not earnings_data:\n",
    "                failed_no_data.append(ticker)\n",
    "            else:\n",
    "                failed_other.append(ticker)\n",
    "        \n",
    "        time.sleep(0.5)  # Be nice to APIs\n",
    "    \n",
    "    print(\"\\r\" + \" \" * 80 + \"\\r\", end='')  # Clear progress line\n",
    "    \n",
    "    # Print fetch summary\n",
    "    print(f\"\\n📊 FETCH SUMMARY\")\n",
    "    print(f\"{'='*75}\")\n",
    "    if fetched_from_cache:\n",
    "        print(f\"✓ From Cache ({len(fetched_from_cache)}): {', '.join(fetched_from_cache)}\")\n",
    "    if fetched_from_api:\n",
    "        print(f\"✓ From API ({len(fetched_from_api)}): {', '.join(fetched_from_api)}\")\n",
    "    if failed_no_data:\n",
    "        print(f\"✗ No Earnings Data ({len(failed_no_data)}): {', '.join(failed_no_data)}\")\n",
    "    if failed_other:\n",
    "        print(f\"✗ Analysis Failed ({len(failed_other)}): {', '.join(failed_other)}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"\\n⚠️  No valid results\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate 45-day strike width for comparison\n",
    "    for idx, row in df.iterrows():\n",
    "        df.at[idx, '45d_width'] = round(row['strike_width'] * np.sqrt(45/90), 1)\n",
    "    \n",
    "    # Create formatted columns\n",
    "    df['45_break_fmt'] = df.apply(lambda x: format_break_ratio(x['45d_breaks_up'], x['45d_breaks_dn']), axis=1)\n",
    "    df['90_break_fmt'] = df.apply(lambda x: format_break_ratio(x['90d_breaks_up'], x['90d_breaks_dn']), axis=1)\n",
    "    \n",
    "    # Format strategy recommendations\n",
    "    df['strategy_fmt'] = df['strategy'].replace({\n",
    "        'IC (90 DTE)': 'IC90',\n",
    "        'IC (45 DTE)': 'IC45',\n",
    "        'IC (90 DTE) + Bull Put Spread': 'IC90+BPS',\n",
    "        'IC (90 DTE) + Bear Call Spread': 'IC90+BCS',\n",
    "        'IC (45 DTE) + Bull Put Spread': 'IC45+BPS',\n",
    "        'IC (45 DTE) + Bear Call Spread': 'IC45+BCS',\n",
    "        'Bull Put Spread': 'BPS',\n",
    "        'Bear Call Spread': 'BCS',\n",
    "        'SKIP - No Edge': 'SKIP'\n",
    "    })\n",
    "    \n",
    "    # Results table\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"TIMEFRAME COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    display_df = pd.DataFrame({\n",
    "        'Ticker': df['ticker'],\n",
    "        'HVol%': df['hvol'].astype(int),\n",
    "        'Tier': df['tier'],\n",
    "        ' ': '|',\n",
    "        '45D%': df['45d_contain'].astype(int),\n",
    "        '45Bias': df['45d_bias'].astype(int),\n",
    "        '45Break': df['45_break_fmt'],\n",
    "        '45Width': df['45d_width'].astype(str) + '%',\n",
    "        '  ': '|',\n",
    "        '90D%': df['90d_contain'].astype(int),\n",
    "        '90Bias': df['90d_bias'].astype(int),\n",
    "        '90Break': df['90_break_fmt'],\n",
    "        '90Width': df['strike_width'].astype(str) + '%',\n",
    "        '   ': '|',\n",
    "        'Strategy': df['strategy_fmt']\n",
    "    })\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# RUN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with your tickers - set debug=True to see API responses\n",
    "    tickers = [\"DAL\", \"PEP\", \"FAST\", \"BLK\", \"C\", \"DPZ\", \"GS\", \"JNJ\", \"JPM\", \n",
    "           \"WBA\", \"WFC\", \"OMC\", \"ABT\", \"BAC\", \"CFG\", \"MS\", \"PGR\", \"PLD\", \n",
    "           \"PNC\", \"SYF\", \"JBHT\", \"UAL\", \"BK\", \"KEY\", \"MMC\", \"MTB\", \"SCHW\", \n",
    "           \"SNA\", \"TRV\", \"USB\", \"CSX\", \"AXP\", \"FITB\", \"HBAN\", \"RF\", \"SLB\", \n",
    "           \"STT\", \"TFC\"]\n",
    "    \n",
    "    # Use debug=True to see actual API error messages\n",
    "    results = batch_analyze(tickers, lookback_quarters=24, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60524aa4-d67e-473b-a85d-94ab5319b36c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
